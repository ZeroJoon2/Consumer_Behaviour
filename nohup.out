[I 2025-02-04 11:39:34.656 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2025-02-04 11:39:34.661 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2025-02-04 11:39:34.665 ServerApp] jupyterlab | extension was successfully linked.
[W 2025-02-04 11:39:34.667 JupyterNotebookApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 11:39:34.670 ServerApp] ServerApp.password config is deprecated in 2.0. Use PasswordIdentityProvider.hashed_password.
[I 2025-02-04 11:39:34.670 ServerApp] notebook | extension was successfully linked.
[I 2025-02-04 11:39:34.864 ServerApp] notebook_shim | extension was successfully linked.
[I 2025-02-04 11:39:34.902 ServerApp] notebook_shim | extension was successfully loaded.
[I 2025-02-04 11:39:34.904 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2025-02-04 11:39:34.904 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2025-02-04 11:39:34.906 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyterlab
[I 2025-02-04 11:39:34.906 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/envs/Project/share/jupyter/lab
[I 2025-02-04 11:39:34.906 LabApp] Extension Manager is 'pypi'.
[I 2025-02-04 11:39:34.934 ServerApp] jupyterlab | extension was successfully loaded.
[I 2025-02-04 11:39:34.937 ServerApp] notebook | extension was successfully loaded.
[I 2025-02-04 11:39:34.938 ServerApp] Serving notebooks from local directory: /home/lab13/project
[I 2025-02-04 11:39:34.938 ServerApp] Jupyter Server 2.14.2 is running at:
[I 2025-02-04 11:39:34.938 ServerApp] http://ip-172-31-13-81:8917/tree
[I 2025-02-04 11:39:34.938 ServerApp]     http://127.0.0.1:8917/tree
[I 2025-02-04 11:39:34.938 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 2025-02-04 11:39:34.955 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[I 2025-02-04 11:39:38.008 ServerApp] Kernel started: 9a7491c4-57f1-4f1f-9d46-1b5761257db7
[I 2025-02-04 11:39:38.478 ServerApp] Connecting to kernel 9a7491c4-57f1-4f1f-9d46-1b5761257db7.
[I 2025-02-04 11:39:38.548 ServerApp] Connecting to kernel 9a7491c4-57f1-4f1f-9d46-1b5761257db7.
[I 2025-02-04 11:39:38.616 ServerApp] Connecting to kernel 9a7491c4-57f1-4f1f-9d46-1b5761257db7.
25/02/04 11:39:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-04 11:41:26.635 ServerApp] Kernel started: f628f0d5-730b-4f60-b089-8ec1507c7bd9
[I 2025-02-04 11:41:26.636 ServerApp] Kernel shutdown: 9a7491c4-57f1-4f1f-9d46-1b5761257db7
[I 2025-02-04 11:41:29.286 ServerApp] Connecting to kernel f628f0d5-730b-4f60-b089-8ec1507c7bd9.
[I 2025-02-04 11:41:32.880 ServerApp] Kernel restarted: f628f0d5-730b-4f60-b089-8ec1507c7bd9
[I 2025-02-04 11:41:32.914 ServerApp] Starting buffering for f628f0d5-730b-4f60-b089-8ec1507c7bd9:ca2456cb-05bf-4243-8bf6-ab17b59e6150
[I 2025-02-04 11:41:32.944 ServerApp] Connecting to kernel f628f0d5-730b-4f60-b089-8ec1507c7bd9.
[I 2025-02-04 11:41:32.944 ServerApp] Restoring connection for f628f0d5-730b-4f60-b089-8ec1507c7bd9:ca2456cb-05bf-4243-8bf6-ab17b59e6150
[I 2025-02-04 11:41:37.800 ServerApp] Saving file at /YJ/youtube_preprocessing.ipynb
25/02/04 11:41:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[I 2025-02-04 11:43:27.934 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2025-02-04 11:43:27.938 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2025-02-04 11:43:27.943 ServerApp] jupyterlab | extension was successfully linked.
[W 2025-02-04 11:43:27.945 JupyterNotebookApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 11:43:27.948 ServerApp] ServerApp.password config is deprecated in 2.0. Use PasswordIdentityProvider.hashed_password.
[I 2025-02-04 11:43:27.948 ServerApp] notebook | extension was successfully linked.
[I 2025-02-04 11:43:28.140 ServerApp] notebook_shim | extension was successfully linked.
[I 2025-02-04 11:43:28.177 ServerApp] notebook_shim | extension was successfully loaded.
[I 2025-02-04 11:43:28.179 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2025-02-04 11:43:28.180 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2025-02-04 11:43:28.182 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyterlab
[I 2025-02-04 11:43:28.182 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/envs/Project/share/jupyter/lab
[I 2025-02-04 11:43:28.182 LabApp] Extension Manager is 'pypi'.
[I 2025-02-04 11:43:28.209 ServerApp] jupyterlab | extension was successfully loaded.
[I 2025-02-04 11:43:28.213 ServerApp] notebook | extension was successfully loaded.
[I 2025-02-04 11:43:28.213 ServerApp] Serving notebooks from local directory: /home/lab13/project
[I 2025-02-04 11:43:28.213 ServerApp] Jupyter Server 2.14.2 is running at:
[I 2025-02-04 11:43:28.213 ServerApp] http://ip-172-31-13-81:8917/tree
[I 2025-02-04 11:43:28.213 ServerApp]     http://127.0.0.1:8917/tree
[I 2025-02-04 11:43:28.213 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 2025-02-04 11:43:28.231 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[I 2025-02-04 11:43:32.737 ServerApp] Kernel started: bf98f90a-7c57-4747-a087-5a1a5fe20ffc
[I 2025-02-04 11:43:33.178 ServerApp] Connecting to kernel bf98f90a-7c57-4747-a087-5a1a5fe20ffc.
[I 2025-02-04 11:43:33.241 ServerApp] Connecting to kernel bf98f90a-7c57-4747-a087-5a1a5fe20ffc.
[I 2025-02-04 11:43:33.309 ServerApp] Connecting to kernel bf98f90a-7c57-4747-a087-5a1a5fe20ffc.
25/02/04 11:43:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-04 11:45:33.466 ServerApp] Saving file at /YJ/youtube_preprocessing.ipynb
[I 2025-02-04 11:47:33.618 ServerApp] Saving file at /YJ/youtube_preprocessing.ipynb
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[I 2025-02-04 11:53:19.219 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2025-02-04 11:53:19.224 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2025-02-04 11:53:19.228 ServerApp] jupyterlab | extension was successfully linked.
[W 2025-02-04 11:53:19.230 JupyterNotebookApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 11:53:19.233 ServerApp] ServerApp.password config is deprecated in 2.0. Use PasswordIdentityProvider.hashed_password.
[I 2025-02-04 11:53:19.233 ServerApp] notebook | extension was successfully linked.
[I 2025-02-04 11:53:19.428 ServerApp] notebook_shim | extension was successfully linked.
[I 2025-02-04 11:53:19.466 ServerApp] notebook_shim | extension was successfully loaded.
[I 2025-02-04 11:53:19.467 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2025-02-04 11:53:19.468 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2025-02-04 11:53:19.470 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyterlab
[I 2025-02-04 11:53:19.470 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/envs/Project/share/jupyter/lab
[I 2025-02-04 11:53:19.470 LabApp] Extension Manager is 'pypi'.
[I 2025-02-04 11:53:19.497 ServerApp] jupyterlab | extension was successfully loaded.
[I 2025-02-04 11:53:19.501 ServerApp] notebook | extension was successfully loaded.
[I 2025-02-04 11:53:19.501 ServerApp] Serving notebooks from local directory: /home/lab13/project
[I 2025-02-04 11:53:19.502 ServerApp] Jupyter Server 2.14.2 is running at:
[I 2025-02-04 11:53:19.502 ServerApp] http://ip-172-31-13-81:8917/tree
[I 2025-02-04 11:53:19.502 ServerApp]     http://127.0.0.1:8917/tree
[I 2025-02-04 11:53:19.502 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 2025-02-04 11:53:19.519 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[W 2025-02-04 11:53:21.267 ServerApp] 400 GET /api/contents/YJ/df2.parquet?type=file&content=1&hash=1&format=text&1738637600440 (125.129.250.60): /home/lab13/project/YJ/df2.parquet is not UTF-8 encoded
[W 2025-02-04 11:53:21.268 ServerApp] wrote error: '/home/lab13/project/YJ/df2.parquet is not UTF-8 encoded'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/contents/fileio.py", line 536, in _read_file
        (bcontent.decode("utf8"), "text", bcontent)
    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xdc in position 7: invalid continuation byte
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/contents/handlers.py", line 154, in get
        model = await ensure_async(
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_core/utils/__init__.py", line 198, in ensure_async
        result = await obj
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/contents/filemanager.py", line 926, in get
        model = await self._file_model(
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/contents/filemanager.py", line 835, in _file_model
        content, format, bytes_content = await self._read_file(os_path, format, raw=True)  # type: ignore[misc]
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/contents/fileio.py", line 545, in _read_file
        raise HTTPError(
    tornado.web.HTTPError: HTTP 400: bad format (/home/lab13/project/YJ/df2.parquet is not UTF-8 encoded)
[W 2025-02-04 11:53:21.269 ServerApp] 400 GET /api/contents/YJ/df2.parquet?type=file&content=1&hash=1&format=text&1738637600440 (d3510c086d21437a83e97539efffe8c2@125.129.250.60) 4.71ms referer=http://15.168.221.131:8917/tree/YJ
[I 2025-02-04 11:53:24.318 ServerApp] Kernel started: c435ead3-3e73-46e7-8378-7d0876769bb8
[I 2025-02-04 11:53:24.761 ServerApp] Connecting to kernel c435ead3-3e73-46e7-8378-7d0876769bb8.
[I 2025-02-04 11:53:24.844 ServerApp] Connecting to kernel c435ead3-3e73-46e7-8378-7d0876769bb8.
[I 2025-02-04 11:53:24.913 ServerApp] Connecting to kernel c435ead3-3e73-46e7-8378-7d0876769bb8.
25/02/04 11:53:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 2025-02-04 11:53:30.465 ServerApp] Connecting to kernel c435ead3-3e73-46e7-8378-7d0876769bb8.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-04 11:55:24.031 ServerApp] Saving file at /YJ/youtube_preprocessing.ipynb
[I 2025-02-04 12:03:25.442 ServerApp] Saving file at /YJ/youtube_preprocessing.ipynb
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[W 2025-02-04 12:05:37.834 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:05:37.834 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:05:37.834 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:05:37.834 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:05:37.834 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[I 2025-02-04 12:05:37.842 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/lib/python3.7/site-packages/jupyterlab
[I 2025-02-04 12:05:37.842 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/share/jupyter/lab
[I 12:05:37.846 NotebookApp] Serving notebooks from local directory: /home/lab13/project
[I 12:05:37.846 NotebookApp] Jupyter Notebook 6.4.11 is running at:
[I 12:05:37.846 NotebookApp] http://ip-172-31-13-81:8917/
[I 12:05:37.846 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 12:05:40.533 NotebookApp] Kernel started: c67bde05-5911-496a-b0ba-85c270005d20, name: project
[I 12:05:52.320 NotebookApp] Starting buffering for c67bde05-5911-496a-b0ba-85c270005d20:25b3f6e2d3f34945b88ed75b82396729
25/02/04 12:06:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 4) / 5][Stage 3:=======================>                                   (2 + 3) / 5][Stage 3:===================================>                       (3 + 2) / 5][Stage 3:===============================================>           (4 + 1) / 5]                                                                                [Stage 4:>                                                          (0 + 4) / 5][Stage 4:===========>                                               (1 + 4) / 5][Stage 4:=======================>                                   (2 + 3) / 5][Stage 4:===================================>                       (3 + 2) / 5][Stage 4:===============================================>           (4 + 1) / 5]                                                                                [I 12:07:44.900 NotebookApp] Saving file at /YJ/youtube_preprocessing.ipynb
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[W 2025-02-04 12:08:16.909 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:08:16.909 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:08:16.909 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:08:16.909 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:08:16.909 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[I 2025-02-04 12:08:16.919 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/lib/python3.7/site-packages/jupyterlab
[I 2025-02-04 12:08:16.919 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/share/jupyter/lab
[I 12:08:16.923 NotebookApp] Serving notebooks from local directory: /home/lab13/project
[I 12:08:16.923 NotebookApp] Jupyter Notebook 6.4.11 is running at:
[I 12:08:16.923 NotebookApp] http://ip-172-31-13-81:8917/
[I 12:08:16.923 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 12:08:17.140 NotebookApp] 404 GET /api/kernels/c67bde05-5911-496a-b0ba-85c270005d20/channels?session_id=84fd3a0339c94a0e9e96c33a6f8b0f54 (125.129.250.60): Kernel does not exist: c67bde05-5911-496a-b0ba-85c270005d20
[W 12:08:17.160 NotebookApp] 404 GET /api/kernels/c67bde05-5911-496a-b0ba-85c270005d20/channels?session_id=84fd3a0339c94a0e9e96c33a6f8b0f54 (125.129.250.60) 21.320000ms referer=None
[I 12:08:19.864 NotebookApp] Kernel started: 53de2ffa-08a8-40a3-a940-718224705bf2, name: project
[I 12:08:21.014 NotebookApp] Starting buffering for 53de2ffa-08a8-40a3-a940-718224705bf2:c5bf3accbc8f4c8a89d1475a57be2dff
[W 12:08:21.034 NotebookApp] 404 GET /api/events/subscribe (125.129.250.60) 1.080000ms referer=None
[I 12:08:32.320 NotebookApp] 302 GET / (125.129.250.60) 0.490000ms
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[W 2025-02-04 12:09:47.347 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:09:47.347 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:09:47.347 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:09:47.347 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:09:47.347 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[I 2025-02-04 12:09:47.354 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/lib/python3.7/site-packages/jupyterlab
[I 2025-02-04 12:09:47.354 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/share/jupyter/lab
[I 12:09:47.358 NotebookApp] Serving notebooks from local directory: /home/lab13/project
[I 12:09:47.358 NotebookApp] Jupyter Notebook 6.4.11 is running at:
[I 12:09:47.358 NotebookApp] http://ip-172-31-13-81:8917/
[I 12:09:47.358 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 12:10:00.941 NotebookApp] Kernel started: da3eb268-8588-4652-821c-c7a95ae79814, name: project
25/02/04 12:10:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[W 2025-02-04 12:11:33.135 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:11:33.135 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:11:33.135 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:11:33.135 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 12:11:33.135 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[I 2025-02-04 12:11:33.143 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/lib/python3.7/site-packages/jupyterlab
[I 2025-02-04 12:11:33.143 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/share/jupyter/lab
[I 12:11:33.147 NotebookApp] Serving notebooks from local directory: /home/lab13/project
[I 12:11:33.147 NotebookApp] Jupyter Notebook 6.4.11 is running at:
[I 12:11:33.147 NotebookApp] http://ip-172-31-13-81:8917/
[I 12:11:33.147 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 12:11:39.008 NotebookApp] Kernel started: a776f755-8e9a-4ad4-bfa4-e1be29e1ad06, name: project
25/02/04 12:11:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 12:12:27.140 NotebookApp] Starting buffering for a776f755-8e9a-4ad4-bfa4-e1be29e1ad06:60e22c3c3dc04c4db1dc6995464ad82e
25/02/04 12:12:27 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-069a2879-68e2-4ad4-a953-ea5d798fc5be/userFiles-eca9ea6c-1f45-45af-b73f-6334adde145a. Falling back to Java IO way
java.io.IOException: Failed to delete: /tmp/spark-069a2879-68e2-4ad4-a953-ea5d798fc5be/userFiles-eca9ea6c-1f45-45af-b73f-6334adde145a
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1141)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:105)
	at org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2108)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2108)
	at org.apache.spark.SparkContext.$anonfun$new$37(SparkContext.scala:661)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[I 12:12:29.769 NotebookApp] Kernel restarted: a776f755-8e9a-4ad4-bfa4-e1be29e1ad06
[I 12:12:29.830 NotebookApp] Restoring connection for a776f755-8e9a-4ad4-bfa4-e1be29e1ad06:60e22c3c3dc04c4db1dc6995464ad82e
[I 12:12:29.831 NotebookApp] Replaying 2 buffered messages
25/02/04 12:12:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 12:13:39.259 NotebookApp] Saving file at /YJ/youtube_preprocessing.ipynb
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[W 2025-02-04 13:05:45.178 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 13:05:45.178 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 13:05:45.178 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 13:05:45.178 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 13:05:45.178 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[I 2025-02-04 13:05:45.185 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/lib/python3.7/site-packages/jupyterlab
[I 2025-02-04 13:05:45.185 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/share/jupyter/lab
[I 13:05:45.190 NotebookApp] Serving notebooks from local directory: /home/lab13/project
[I 13:05:45.190 NotebookApp] Jupyter Notebook 6.4.11 is running at:
[I 13:05:45.190 NotebookApp] http://ip-172-31-13-81:8917/
[I 13:05:45.190 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 13:05:48.761 NotebookApp] Kernel started: 2f45d459-f4ff-4e93-8944-c41d74fb2838, name: project
[I 13:05:56.104 NotebookApp] Starting buffering for 2f45d459-f4ff-4e93-8944-c41d74fb2838:6ead7414de8d4e838eb12bc151537f7a
[I 13:05:56.235 NotebookApp] Kernel restarted: 2f45d459-f4ff-4e93-8944-c41d74fb2838
[I 13:05:56.300 NotebookApp] Restoring connection for 2f45d459-f4ff-4e93-8944-c41d74fb2838:6ead7414de8d4e838eb12bc151537f7a
[I 13:05:56.697 NotebookApp] Replaying 3 buffered messages
25/02/04 13:06:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 13:07:48.773 NotebookApp] Saving file at /YJ/youtube_preprocessing.ipynb
[I 13:10:24.749 NotebookApp] Kernel started: b6a1c265-347d-4890-a040-4ce894c44456, name: project
[I 13:11:00.566 NotebookApp] Copying YJ/test/pyspark_test.ipynb to /YJ/test
[I 13:11:02.302 NotebookApp] Kernel started: 3f4d0e02-60c9-49f8-82c3-0af127a55ddc, name: project
25/02/04 13:11:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 13:11:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 13:11:28.540 NotebookApp] Starting buffering for 3f4d0e02-60c9-49f8-82c3-0af127a55ddc:aba56e25aea541808670371adbe7c116
[I 13:11:31.171 NotebookApp] Kernel restarted: 3f4d0e02-60c9-49f8-82c3-0af127a55ddc
[I 13:11:31.241 NotebookApp] Restoring connection for 3f4d0e02-60c9-49f8-82c3-0af127a55ddc:aba56e25aea541808670371adbe7c116
[I 13:11:31.241 NotebookApp] Replaying 1 buffered messages
25/02/04 13:11:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 13:11:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[I 13:11:49.383 NotebookApp] Saving file at /YJ/youtube_preprocessing.ipynb
[I 13:11:54.759 NotebookApp] Starting buffering for 3f4d0e02-60c9-49f8-82c3-0af127a55ddc:aba56e25aea541808670371adbe7c116
[I 13:11:57.389 NotebookApp] Kernel restarted: 3f4d0e02-60c9-49f8-82c3-0af127a55ddc
[I 13:11:57.458 NotebookApp] Restoring connection for 3f4d0e02-60c9-49f8-82c3-0af127a55ddc:aba56e25aea541808670371adbe7c116
[I 13:11:57.459 NotebookApp] Replaying 1 buffered messages
25/02/04 13:11:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 13:12:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 13:12:10.636 NotebookApp] Starting buffering for b6a1c265-347d-4890-a040-4ce894c44456:97463f47afee458e85116d16c5552bc5
[I 13:12:30.573 NotebookApp] Creating new notebook in /YJ
[I 13:12:31.519 NotebookApp] Kernel started: 2f9333de-d2a9-416e-9c07-2b6b6c721c4a, name: project
25/02/04 13:12:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 13:12:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/02/04 13:12:39 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 13:12:56.230 NotebookApp] Starting buffering for 3f4d0e02-60c9-49f8-82c3-0af127a55ddc:aba56e25aea541808670371adbe7c116
25/02/04 13:13:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/02/04 13:13:39 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[Stage 3:>                                                          (0 + 4) / 5][I 13:13:49.361 NotebookApp] Saving file at /YJ/youtube_preprocessing.ipynb
[Stage 3:=======================>                                   (2 + 3) / 5][Stage 3:===============================================>           (4 + 1) / 5]                                                                                [I 13:14:32.348 NotebookApp] Saving file at /YJ/Untitled.ipynb
[I 13:14:39.022 NotebookApp] Starting buffering for 2f45d459-f4ff-4e93-8944-c41d74fb2838:6ead7414de8d4e838eb12bc151537f7a
[I 13:15:27.477 NotebookApp] Saving file at /YJ/youtube_preprocessing.py
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[I 13:37:42.581 NotebookApp] Serving notebooks from local directory: /home/lab13/project
[I 13:37:42.581 NotebookApp] Jupyter Notebook 6.4.10 is running at:
[I 13:37:42.581 NotebookApp] http://ip-172-31-13-81:8917/
[I 13:37:42.581 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 2025-02-04 13:42:59.017 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 13:42:59.017 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 13:42:59.017 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 13:42:59.017 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 13:42:59.018 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[I 2025-02-04 13:42:59.025 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/lib/python3.7/site-packages/jupyterlab
[I 2025-02-04 13:42:59.025 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/share/jupyter/lab
[I 13:42:59.029 NotebookApp] Serving notebooks from local directory: /home/lab13/project
[I 13:42:59.029 NotebookApp] Jupyter Notebook 6.4.11 is running at:
[I 13:42:59.029 NotebookApp] http://ip-172-31-13-81:8917/
[I 13:42:59.029 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 13:43:23.922 NotebookApp] Notebook YJ/youtube_preprocessing2.ipynb is not trusted
[I 13:43:24.045 NotebookApp] Kernel started: db7c7dae-f0e1-4414-9780-e620b44406aa, name: project
25/02/04 13:43:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 13:43:53.611 NotebookApp] Creating new notebook in /YJ
[I 13:43:54.533 NotebookApp] Kernel started: 23bb9a89-6945-4d20-84fd-5d55b8c36e78, name: project
25/02/04 13:44:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 13:44:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[W 2025-02-04 13:45:49.768 LabApp] 'ip' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 13:45:49.768 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 13:45:49.768 LabApp] 'port' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 13:45:49.768 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 13:45:49.768 LabApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[I 2025-02-04 13:45:49.775 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/lib/python3.7/site-packages/jupyterlab
[I 2025-02-04 13:45:49.775 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/share/jupyter/lab
[I 13:45:49.779 NotebookApp] Serving notebooks from local directory: /home/lab13/project
[I 13:45:49.779 NotebookApp] Jupyter Notebook 6.4.11 is running at:
[I 13:45:49.779 NotebookApp] http://ip-172-31-13-81:8917/
[I 13:45:49.779 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 13:45:50.141 NotebookApp] 404 GET /api/kernels/23bb9a89-6945-4d20-84fd-5d55b8c36e78/channels?session_id=d76c102b9b894dea8a0a454d31a2c36e (125.129.250.60): Kernel does not exist: 23bb9a89-6945-4d20-84fd-5d55b8c36e78
[W 13:45:50.180 NotebookApp] 404 GET /api/kernels/23bb9a89-6945-4d20-84fd-5d55b8c36e78/channels?session_id=d76c102b9b894dea8a0a454d31a2c36e (125.129.250.60) 40.740000ms referer=None
[I 13:45:55.329 NotebookApp] Saving file at /YJ/Untitled.ipynb
[W 13:45:58.763 NotebookApp] Notebook YJ/Untitled.ipynb is not trusted
[I 13:45:58.876 NotebookApp] Kernel started: 0cbca0a8-0991-4e3c-a335-09622a4eab2b, name: project
25/02/04 13:46:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[W 13:46:05.302 NotebookApp] 404 GET /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/channels?session_id=2d4b3c5c9b33474dace290ab5baa82f4 (125.129.250.60): Kernel does not exist: db7c7dae-f0e1-4414-9780-e620b44406aa
[W 13:46:05.303 NotebookApp] 404 GET /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/channels?session_id=2d4b3c5c9b33474dace290ab5baa82f4 (125.129.250.60) 3.190000ms referer=None
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [W 13:46:38.308 NotebookApp] 404 GET /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/channels?session_id=2d4b3c5c9b33474dace290ab5baa82f4 (125.129.250.60): Kernel does not exist: db7c7dae-f0e1-4414-9780-e620b44406aa
[W 13:46:38.308 NotebookApp] 404 GET /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/channels?session_id=2d4b3c5c9b33474dace290ab5baa82f4 (125.129.250.60) 2.180000ms referer=None
[I 13:47:24.315 NotebookApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[W 13:47:43.303 NotebookApp] 404 GET /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/channels?session_id=2d4b3c5c9b33474dace290ab5baa82f4 (125.129.250.60): Kernel does not exist: db7c7dae-f0e1-4414-9780-e620b44406aa
[W 13:47:43.304 NotebookApp] 404 GET /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/channels?session_id=2d4b3c5c9b33474dace290ab5baa82f4 (125.129.250.60) 2.190000ms referer=None
[I 13:47:58.899 NotebookApp] Saving file at /YJ/Untitled.ipynb
[I 13:49:29.487 NotebookApp] Kernel started: 6e3b5913-beb0-4a96-8b44-05eee7a95ccb, name: project
25/02/04 13:49:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 13:49:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 13:49:59.328 NotebookApp] Saving file at /YJ/Untitled.ipynb
[W 13:50:08.320 NotebookApp] 404 GET /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/channels?session_id=2d4b3c5c9b33474dace290ab5baa82f4 (125.129.250.60): Kernel does not exist: db7c7dae-f0e1-4414-9780-e620b44406aa
[W 13:50:08.321 NotebookApp] 404 GET /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/channels?session_id=2d4b3c5c9b33474dace290ab5baa82f4 (125.129.250.60) 2.330000ms referer=None
[W 13:50:17.936 NotebookApp] 404 POST /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/interrupt (125.129.250.60): Kernel does not exist: db7c7dae-f0e1-4414-9780-e620b44406aa
[W 13:50:17.936 NotebookApp] Kernel does not exist: db7c7dae-f0e1-4414-9780-e620b44406aa
[W 13:50:17.937 NotebookApp] 404 POST /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/interrupt (125.129.250.60) 1.050000ms referer=http://15.168.221.131:8917/notebooks/YJ/youtube_preprocessing2.ipynb
[W 13:50:18.731 NotebookApp] 404 POST /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/interrupt (125.129.250.60): Kernel does not exist: db7c7dae-f0e1-4414-9780-e620b44406aa
[W 13:50:18.731 NotebookApp] Kernel does not exist: db7c7dae-f0e1-4414-9780-e620b44406aa
[W 13:50:18.732 NotebookApp] 404 POST /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/interrupt (125.129.250.60) 0.910000ms referer=http://15.168.221.131:8917/notebooks/YJ/youtube_preprocessing2.ipynb
[W 13:50:18.889 NotebookApp] 404 POST /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/interrupt (125.129.250.60): Kernel does not exist: db7c7dae-f0e1-4414-9780-e620b44406aa
[W 13:50:18.889 NotebookApp] Kernel does not exist: db7c7dae-f0e1-4414-9780-e620b44406aa
[W 13:50:18.889 NotebookApp] 404 POST /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/interrupt (125.129.250.60) 0.750000ms referer=http://15.168.221.131:8917/notebooks/YJ/youtube_preprocessing2.ipynb
[E 13:50:22.582 NotebookApp] Exception restarting kernel
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/notebook/services/kernels/handlers.py", line 89, in post
        yield maybe_future(km.restart_kernel(kernel_id))
      File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 762, in run
        value = future.result()
      File "/home/ubuntu/anaconda3/lib/python3.7/asyncio/futures.py", line 181, in result
        raise self._exception
      File "/home/ubuntu/anaconda3/lib/python3.7/asyncio/tasks.py", line 249, in __step
        result = coro.send(None)
      File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/notebook/services/kernels/kernelmanager.py", line 313, in restart_kernel
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/notebook/services/kernels/kernelmanager.py", line 394, in _check_kernel_id
        raise web.HTTPError(404, f'Kernel does not exist: {kernel_id}')
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: db7c7dae-f0e1-4414-9780-e620b44406aa)
[E 13:50:22.588 NotebookApp] {
      "Host": "15.168.221.131:8917",
      "Accept": "application/json, text/javascript, */*; q=0.01",
      "Referer": "http://15.168.221.131:8917/notebooks/YJ/youtube_preprocessing2.ipynb",
      "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36"
    }
[E 13:50:22.588 NotebookApp] 500 POST /api/kernels/db7c7dae-f0e1-4414-9780-e620b44406aa/restart (125.129.250.60) 6.980000ms referer=http://15.168.221.131:8917/notebooks/YJ/youtube_preprocessing2.ipynb
[W 13:50:22.615 NotebookApp] 404 DELETE /api/sessions/04ba3548-69ee-41bb-bacb-71c7924790b0 (125.129.250.60): Session not found: session_id='04ba3548-69ee-41bb-bacb-71c7924790b0'
[W 13:50:22.615 NotebookApp] Session not found: session_id='04ba3548-69ee-41bb-bacb-71c7924790b0'
[W 13:50:22.615 NotebookApp] 404 DELETE /api/sessions/04ba3548-69ee-41bb-bacb-71c7924790b0 (125.129.250.60) 0.930000ms referer=http://15.168.221.131:8917/notebooks/YJ/youtube_preprocessing2.ipynb
[I 13:50:22.662 NotebookApp] Kernel started: dbf36b43-e2f8-4f16-9df7-f30561c83e0d, name: project
25/02/04 13:50:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 13:50:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/02/04 13:50:25 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 13:50:49.078 NotebookApp] Starting buffering for dbf36b43-e2f8-4f16-9df7-f30561c83e0d:2d4b3c5c9b33474dace290ab5baa82f4
[I 13:50:51.706 NotebookApp] Kernel restarted: dbf36b43-e2f8-4f16-9df7-f30561c83e0d
[I 13:50:51.769 NotebookApp] Restoring connection for dbf36b43-e2f8-4f16-9df7-f30561c83e0d:2d4b3c5c9b33474dace290ab5baa82f4
[I 13:50:51.769 NotebookApp] Replaying 1 buffered messages
25/02/04 13:50:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 13:50:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/02/04 13:50:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 13:51:24.041 NotebookApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 13:51:30.306 NotebookApp] Saving file at /YJ/test/pyspark_test-Copy1.ipynb
[I 13:51:41.742 NotebookApp] Starting buffering for 6e3b5913-beb0-4a96-8b44-05eee7a95ccb:980da34273b94dac8bce2fb57cdeed11
[I 13:51:44.374 NotebookApp] Kernel restarted: 6e3b5913-beb0-4a96-8b44-05eee7a95ccb
[I 13:51:44.431 NotebookApp] Restoring connection for 6e3b5913-beb0-4a96-8b44-05eee7a95ccb:980da34273b94dac8bce2fb57cdeed11
[I 13:51:44.431 NotebookApp] Replaying 1 buffered messages
25/02/04 13:51:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 13:51:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 13:52:17.646 NotebookApp] Starting buffering for 6e3b5913-beb0-4a96-8b44-05eee7a95ccb:980da34273b94dac8bce2fb57cdeed11
[I 13:52:20.278 NotebookApp] Kernel restarted: 6e3b5913-beb0-4a96-8b44-05eee7a95ccb
[I 13:52:20.340 NotebookApp] Restoring connection for 6e3b5913-beb0-4a96-8b44-05eee7a95ccb:980da34273b94dac8bce2fb57cdeed11
[I 13:52:20.340 NotebookApp] Replaying 1 buffered messages
25/02/04 13:52:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 13:52:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[I 13:52:33.295 NotebookApp] Starting buffering for 6e3b5913-beb0-4a96-8b44-05eee7a95ccb:980da34273b94dac8bce2fb57cdeed11
[I 13:52:35.925 NotebookApp] Kernel restarted: 6e3b5913-beb0-4a96-8b44-05eee7a95ccb
[I 13:52:35.989 NotebookApp] Restoring connection for 6e3b5913-beb0-4a96-8b44-05eee7a95ccb:980da34273b94dac8bce2fb57cdeed11
[I 13:52:35.989 NotebookApp] Replaying 1 buffered messages
25/02/04 13:52:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 13:52:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 13:52:55.259 NotebookApp] Starting buffering for 0cbca0a8-0991-4e3c-a335-09622a4eab2b:3f1313c19ba14a1eb7759d09cd0ce170
[I 13:52:57.890 NotebookApp] Kernel restarted: 0cbca0a8-0991-4e3c-a335-09622a4eab2b
[I 13:52:57.956 NotebookApp] Restoring connection for 0cbca0a8-0991-4e3c-a335-09622a4eab2b:3f1313c19ba14a1eb7759d09cd0ce170
[I 13:52:57.956 NotebookApp] Replaying 1 buffered messages
25/02/04 13:53:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 13:53:24.295 NotebookApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 13:53:30.156 NotebookApp] Saving file at /YJ/test/pyspark_test-Copy1.ipynb
[I 13:53:45.268 NotebookApp] Starting buffering for dbf36b43-e2f8-4f16-9df7-f30561c83e0d:2d4b3c5c9b33474dace290ab5baa82f4
[I 13:53:47.898 NotebookApp] Kernel restarted: dbf36b43-e2f8-4f16-9df7-f30561c83e0d
[I 13:53:47.955 NotebookApp] Restoring connection for dbf36b43-e2f8-4f16-9df7-f30561c83e0d:2d4b3c5c9b33474dace290ab5baa82f4
[I 13:53:47.955 NotebookApp] Replaying 1 buffered messages
25/02/04 13:53:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 13:53:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/02/04 13:53:52 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 13:53:59.288 NotebookApp] Saving file at /YJ/Untitled.ipynb
25/02/04 13:54:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/02/04 13:54:10 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/02/04 13:54:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/02/04 13:54:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[Stage 3:>                                                          (0 + 4) / 5][Stage 3:=======================>                                   (2 + 3) / 5][Stage 3:===================================>                       (3 + 2) / 5][Stage 3:===============================================>           (4 + 1) / 5]                                                                                [I 13:55:13.541 NotebookApp] Starting buffering for 6e3b5913-beb0-4a96-8b44-05eee7a95ccb:980da34273b94dac8bce2fb57cdeed11
[I 13:55:14.665 NotebookApp] Starting buffering for 0cbca0a8-0991-4e3c-a335-09622a4eab2b:3f1313c19ba14a1eb7759d09cd0ce170
[I 13:55:20.324 NotebookApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 13:55:34.552 NotebookApp] Saving file at /YJ/youtube_preprocessing.py
[I 13:55:37.630 NotebookApp] Saving file at /YJ/youtube_preprocessing.py
25/02/04 13:57:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/02/04 13:57:01 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[Stage 3:>                                                          (0 + 4) / 5][Stage 3:===========>                                               (1 + 4) / 5][Stage 3:=======================>                                   (2 + 3) / 5][Stage 3:===================================>                       (3 + 2) / 5][Stage 3:===============================================>           (4 + 1) / 5]                                                                                [I 13:57:24.296 NotebookApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 13:57:44.357 NotebookApp] Saving file at /YJ/youtube_preprocessing.py
[I 13:59:24.300 NotebookApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[W 14:03:12.368 NotebookApp] Notebook YJ/youtube_preprocessing.ipynb is not trusted
[I 14:03:12.541 NotebookApp] Kernel started: 9a14f787-5900-454e-a3ed-325c9d60fce0, name: project
[I 14:03:24.078 NotebookApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 14:03:46.049 NotebookApp] Starting buffering for dbf36b43-e2f8-4f16-9df7-f30561c83e0d:2d4b3c5c9b33474dace290ab5baa82f4
[I 14:03:48.710 NotebookApp] Kernel restarted: dbf36b43-e2f8-4f16-9df7-f30561c83e0d
[I 14:03:48.778 NotebookApp] Restoring connection for dbf36b43-e2f8-4f16-9df7-f30561c83e0d:2d4b3c5c9b33474dace290ab5baa82f4
[I 14:03:48.778 NotebookApp] Replaying 1 buffered messages
25/02/04 14:03:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[I 2025-02-04 14:06:17.494 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2025-02-04 14:06:17.498 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2025-02-04 14:06:17.503 ServerApp] jupyterlab | extension was successfully linked.
[W 2025-02-04 14:06:17.504 JupyterNotebookApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-04 14:06:17.507 ServerApp] ServerApp.password config is deprecated in 2.0. Use PasswordIdentityProvider.hashed_password.
[I 2025-02-04 14:06:17.507 ServerApp] notebook | extension was successfully linked.
[I 2025-02-04 14:06:17.702 ServerApp] notebook_shim | extension was successfully linked.
[I 2025-02-04 14:06:17.740 ServerApp] notebook_shim | extension was successfully loaded.
[I 2025-02-04 14:06:17.742 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2025-02-04 14:06:17.743 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2025-02-04 14:06:17.745 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyterlab
[I 2025-02-04 14:06:17.745 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/envs/Project/share/jupyter/lab
[I 2025-02-04 14:06:17.745 LabApp] Extension Manager is 'pypi'.
[I 2025-02-04 14:06:17.772 ServerApp] jupyterlab | extension was successfully loaded.
[I 2025-02-04 14:06:17.775 ServerApp] notebook | extension was successfully loaded.
[I 2025-02-04 14:06:17.776 ServerApp] Serving notebooks from local directory: /home/lab13/project
[I 2025-02-04 14:06:17.776 ServerApp] Jupyter Server 2.14.2 is running at:
[I 2025-02-04 14:06:17.776 ServerApp] http://ip-172-31-13-81:8917/tree
[I 2025-02-04 14:06:17.776 ServerApp]     http://127.0.0.1:8917/tree
[I 2025-02-04 14:06:17.776 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 2025-02-04 14:06:17.793 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[W 2025-02-04 14:06:22.014 ServerApp] Notebook YJ/youtube_preprocessing2.ipynb is not trusted
[I 2025-02-04 14:06:22.378 ServerApp] Kernel started: 5a29a183-e776-4ce1-bffb-da5553afc7ec
[I 2025-02-04 14:06:22.885 ServerApp] Connecting to kernel 5a29a183-e776-4ce1-bffb-da5553afc7ec.
[I 2025-02-04 14:06:22.966 ServerApp] Connecting to kernel 5a29a183-e776-4ce1-bffb-da5553afc7ec.
[I 2025-02-04 14:06:23.037 ServerApp] Connecting to kernel 5a29a183-e776-4ce1-bffb-da5553afc7ec.
25/02/04 14:06:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-04 14:08:22.230 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 2025-02-04 14:08:41.532 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 2025-02-04 14:08:47.919 ServerApp] Kernel restarted: 5a29a183-e776-4ce1-bffb-da5553afc7ec
[I 2025-02-04 14:08:47.947 ServerApp] Starting buffering for 5a29a183-e776-4ce1-bffb-da5553afc7ec:53c63dd5-b387-422a-acbb-79bf05f1012e
[I 2025-02-04 14:08:47.975 ServerApp] Connecting to kernel 5a29a183-e776-4ce1-bffb-da5553afc7ec.
[I 2025-02-04 14:08:47.975 ServerApp] Restoring connection for 5a29a183-e776-4ce1-bffb-da5553afc7ec:53c63dd5-b387-422a-acbb-79bf05f1012e
[I 2025-02-04 14:08:56.566 ServerApp] Kernel started: c22487bc-16a6-4bfb-abd3-9da453b88190
[I 2025-02-04 14:08:56.567 ServerApp] Kernel shutdown: 5a29a183-e776-4ce1-bffb-da5553afc7ec
[I 2025-02-04 14:08:57.030 ServerApp] Connecting to kernel c22487bc-16a6-4bfb-abd3-9da453b88190.
25/02/04 14:09:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-04 14:09:57.666 ServerApp] Connecting to kernel c22487bc-16a6-4bfb-abd3-9da453b88190.
[I 2025-02-04 14:10:00.853 ServerApp] Connecting to kernel c22487bc-16a6-4bfb-abd3-9da453b88190.
[I 2025-02-04 14:10:06.126 ServerApp] Connecting to kernel c22487bc-16a6-4bfb-abd3-9da453b88190.
[I 2025-02-04 14:10:06.521 ServerApp] Kernel started: db722577-241e-4d99-9e7e-a995cd7cb2b6
[I 2025-02-04 14:10:06.968 ServerApp] Connecting to kernel db722577-241e-4d99-9e7e-a995cd7cb2b6.
25/02/04 14:10:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 14:10:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-04 14:10:41.694 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 2025-02-04 14:11:01.113 ServerApp] Creating new notebook in /YJ
[I 2025-02-04 14:11:01.283 ServerApp] Saving file at /YJ/Untitled1.ipynb
[I 2025-02-04 14:11:01.352 ServerApp] Kernel started: 83f76563-f9e5-406c-bffa-4be165b772bb
[I 2025-02-04 14:11:01.818 ServerApp] Connecting to kernel 83f76563-f9e5-406c-bffa-4be165b772bb.
[I 2025-02-04 14:11:01.895 ServerApp] Connecting to kernel 83f76563-f9e5-406c-bffa-4be165b772bb.
[I 2025-02-04 14:11:02.724 ServerApp] Connecting to kernel c22487bc-16a6-4bfb-abd3-9da453b88190.
[I 2025-02-04 14:11:02.785 ServerApp] Connecting to kernel db722577-241e-4d99-9e7e-a995cd7cb2b6.
[I 2025-02-04 14:11:02.885 ServerApp] Connecting to kernel 83f76563-f9e5-406c-bffa-4be165b772bb.
[I 2025-02-04 14:11:03.114 ServerApp] Connecting to kernel 83f76563-f9e5-406c-bffa-4be165b772bb.
[I 2025-02-04 14:11:10.519 ServerApp] Kernel started: 98b723d9-cc74-4ff7-a126-c3d12fd657dc
[I 2025-02-04 14:11:10.520 ServerApp] Kernel shutdown: 83f76563-f9e5-406c-bffa-4be165b772bb
[I 2025-02-04 14:11:10.970 ServerApp] Connecting to kernel 98b723d9-cc74-4ff7-a126-c3d12fd657dc.
25/02/04 14:11:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/02/04 14:11:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-04 14:11:29.947 ServerApp] Starting buffering for db722577-241e-4d99-9e7e-a995cd7cb2b6:433373f9-74b6-402e-847c-867d6a54b498
25/02/04 14:11:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[I 2025-02-04 14:12:42.329 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 2025-02-04 14:13:02.944 ServerApp] Saving file at /YJ/Untitled1.ipynb
25/02/04 14:13:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/02/04 14:13:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 3:>                                                          (0 + 4) / 5][Stage 3:=======================>                                   (2 + 3) / 5][Stage 3:===================================>                       (3 + 2) / 5][Stage 3:===============================================>           (4 + 1) / 5]                                                                                [I 2025-02-04 14:14:23.001 ServerApp] Saving file at /YJ/youtube_preprocessing.py
[I 2025-02-04 14:14:34.293 ServerApp] Connecting to kernel 98b723d9-cc74-4ff7-a126-c3d12fd657dc.
[I 2025-02-04 14:14:43.272 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 2025-02-04 14:15:03.271 ServerApp] Saving file at /YJ/Untitled1.ipynb
[I 2025-02-04 14:16:59.590 ServerApp] Connecting to kernel c22487bc-16a6-4bfb-abd3-9da453b88190.
[I 2025-02-04 14:16:59.675 ServerApp] Connecting to kernel db722577-241e-4d99-9e7e-a995cd7cb2b6.
[I 2025-02-04 14:16:59.753 ServerApp] Connecting to kernel 98b723d9-cc74-4ff7-a126-c3d12fd657dc.
[I 2025-02-04 14:16:59.841 ServerApp] Starting buffering for db722577-241e-4d99-9e7e-a995cd7cb2b6:d627419a-5ca7-44cd-bae8-23e5b59f8b89
[I 2025-02-04 14:18:21.195 ServerApp] Kernel restarted: c22487bc-16a6-4bfb-abd3-9da453b88190
[I 2025-02-04 14:18:21.228 ServerApp] Starting buffering for c22487bc-16a6-4bfb-abd3-9da453b88190:53c63dd5-b387-422a-acbb-79bf05f1012e
[I 2025-02-04 14:18:21.258 ServerApp] Connecting to kernel c22487bc-16a6-4bfb-abd3-9da453b88190.
[I 2025-02-04 14:18:21.258 ServerApp] Restoring connection for c22487bc-16a6-4bfb-abd3-9da453b88190:53c63dd5-b387-422a-acbb-79bf05f1012e
25/02/04 14:18:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-04 14:18:45.265 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 2025-02-04 14:18:53.092 ServerApp] Kernel restarted: 98b723d9-cc74-4ff7-a126-c3d12fd657dc
[I 2025-02-04 14:18:53.155 ServerApp] Connecting to kernel 98b723d9-cc74-4ff7-a126-c3d12fd657dc.
25/02/04 14:18:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-04 14:19:04.276 ServerApp] Saving file at /YJ/Untitled1.ipynb
[I 2025-02-04 14:20:37.679 ServerApp] Connecting to kernel c22487bc-16a6-4bfb-abd3-9da453b88190.
[I 2025-02-04 14:20:37.772 ServerApp] Connecting to kernel db722577-241e-4d99-9e7e-a995cd7cb2b6.
[I 2025-02-04 14:20:37.866 ServerApp] Connecting to kernel 98b723d9-cc74-4ff7-a126-c3d12fd657dc.
[I 2025-02-04 14:20:38.051 ServerApp] Starting buffering for db722577-241e-4d99-9e7e-a995cd7cb2b6:c13ef8bb-2e70-47dd-8dd0-96d6d0a0cd04
[I 2025-02-04 14:20:38.197 ServerApp] Kernel started: be463339-194a-4453-818c-81b854117143
[I 2025-02-04 14:20:38.650 ServerApp] Connecting to kernel be463339-194a-4453-818c-81b854117143.
[I 2025-02-04 14:20:42.190 ServerApp] Starting buffering for be463339-194a-4453-818c-81b854117143:18038214-6f88-49a2-9db4-74ce12bbbee1
25/02/04 14:20:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[I 2025-02-04 14:21:04.399 ServerApp] Saving file at /YJ/Untitled1.ipynb
[I 2025-02-04 14:21:08.692 ServerApp] Connecting to kernel c22487bc-16a6-4bfb-abd3-9da453b88190.
[I 2025-02-04 14:21:08.804 ServerApp] Connecting to kernel db722577-241e-4d99-9e7e-a995cd7cb2b6.
[I 2025-02-04 14:21:08.879 ServerApp] Connecting to kernel 98b723d9-cc74-4ff7-a126-c3d12fd657dc.
[I 2025-02-04 14:21:08.959 ServerApp] Connecting to kernel be463339-194a-4453-818c-81b854117143.
[I 2025-02-04 14:21:08.980 ServerApp] Starting buffering for db722577-241e-4d99-9e7e-a995cd7cb2b6:35163078-621f-444a-b432-495e0211b6c5
[I 2025-02-04 14:21:09.081 ServerApp] Starting buffering for be463339-194a-4453-818c-81b854117143:6207a854-e36b-4242-ad6b-b5ec3685a446
[I 2025-02-04 14:21:09.095 ServerApp] Connecting to kernel db722577-241e-4d99-9e7e-a995cd7cb2b6.
[I 2025-02-04 14:22:15.968 ServerApp] Kernel restarted: c22487bc-16a6-4bfb-abd3-9da453b88190
[I 2025-02-04 14:22:16.002 ServerApp] Starting buffering for c22487bc-16a6-4bfb-abd3-9da453b88190:53c63dd5-b387-422a-acbb-79bf05f1012e
[I 2025-02-04 14:22:16.043 ServerApp] Connecting to kernel c22487bc-16a6-4bfb-abd3-9da453b88190.
[I 2025-02-04 14:22:16.044 ServerApp] Restoring connection for c22487bc-16a6-4bfb-abd3-9da453b88190:53c63dd5-b387-422a-acbb-79bf05f1012e
[I 2025-02-04 14:22:23.796 ServerApp] Kernel started: 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77
[I 2025-02-04 14:22:23.797 ServerApp] Kernel shutdown: c22487bc-16a6-4bfb-abd3-9da453b88190
[I 2025-02-04 14:22:24.298 ServerApp] Connecting to kernel 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77.
25/02/04 14:22:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 4) / 5][I 2025-02-04 14:22:45.762 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[Stage 3:===========>                                               (1 + 4) / 5][Stage 3:=======================>                                   (2 + 3) / 5][Stage 3:===================================>                       (3 + 2) / 5][Stage 3:===============================================>           (4 + 1) / 5]                                                                                [I 2025-02-04 14:23:04.702 ServerApp] Saving file at /YJ/youtube_preprocessing.py
[I 2025-02-04 14:23:05.277 ServerApp] Saving file at /YJ/Untitled1.ipynb
[I 2025-02-04 14:23:29.250 ServerApp] Saving file at /YJ/youtube_preprocessing.py
[I 2025-02-04 14:23:30.436 ServerApp] Saving file at /YJ/youtube_preprocessing.py
[I 2025-02-04 14:24:45.889 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 2025-02-04 14:24:55.389 ServerApp] Kernel restarted: 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77
[I 2025-02-04 14:24:55.423 ServerApp] Starting buffering for 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77:53c63dd5-b387-422a-acbb-79bf05f1012e
[I 2025-02-04 14:24:55.456 ServerApp] Connecting to kernel 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77.
[I 2025-02-04 14:24:55.456 ServerApp] Restoring connection for 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77:53c63dd5-b387-422a-acbb-79bf05f1012e
25/02/04 14:24:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 3:>                                                          (0 + 4) / 5][Stage 3:===========>                                               (1 + 4) / 5][Stage 3:=======================>                                   (2 + 3) / 5][Stage 3:===============================================>           (4 + 1) / 5]                                                                                [I 2025-02-04 14:26:46.254 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[Stage 3:>                                                          (0 + 4) / 5][Stage 3:===========>                                               (1 + 4) / 5][Stage 3:=======================>                                   (2 + 3) / 5][Stage 3:===================================>                       (3 + 2) / 5][Stage 3:===============================================>           (4 + 1) / 5]                                                                                [I 2025-02-04 14:28:47.266 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 2025-02-04 14:29:32.229 ServerApp] Kernel restarted: 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77
[I 2025-02-04 14:29:32.264 ServerApp] Starting buffering for 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77:53c63dd5-b387-422a-acbb-79bf05f1012e
[I 2025-02-04 14:29:32.300 ServerApp] Connecting to kernel 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77.
[I 2025-02-04 14:29:32.300 ServerApp] Restoring connection for 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77:53c63dd5-b387-422a-acbb-79bf05f1012e
[I 2025-02-04 14:30:47.389 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
25/02/04 14:48:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 3:>                                                          (0 + 4) / 5][Stage 3:=======================>                                   (2 + 3) / 5][Stage 3:===================================>                       (3 + 2) / 5][Stage 3:===============================================>           (4 + 1) / 5]                                                                                [I 2025-02-04 14:49:25.236 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 2025-02-04 14:55:09.351 ServerApp] Saving file at /YJ/youtube_preprocessing.py
[I 2025-02-04 14:57:03.914 ServerApp] Saving file at /YJ/youtube_preprocessing.py
[I 2025-02-04 14:58:02.203 ServerApp] Saving file at /YJ/youtube_preprocessing.py
[Stage 3:>                                                          (0 + 4) / 5][Stage 3:=======================>                                   (2 + 3) / 5][Stage 3:===================================>                       (3 + 2) / 5][Stage 3:===============================================>           (4 + 1) / 5]                                                                                [I 2025-02-04 14:59:30.222 ServerApp] Saving file at /YJ/youtube_preprocessing2.ipynb
[I 2025-02-04 15:07:20.407 ServerApp] Kernel restarted: 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77
[I 2025-02-04 15:07:20.436 ServerApp] Starting buffering for 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77:53c63dd5-b387-422a-acbb-79bf05f1012e
[I 2025-02-04 15:07:20.469 ServerApp] Connecting to kernel 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77.
[I 2025-02-04 15:07:20.469 ServerApp] Restoring connection for 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77:53c63dd5-b387-422a-acbb-79bf05f1012e
[W 2025-02-04 15:09:30.045 ServerApp] delete /YJ/YJ
[I 2025-02-04 15:09:34.166 ServerApp] Creating new file in /YJ
[I 2025-02-04 15:09:40.234 ServerApp] Connecting to kernel 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77.
[I 2025-02-04 15:09:40.303 ServerApp] Connecting to kernel db722577-241e-4d99-9e7e-a995cd7cb2b6.
[I 2025-02-04 15:09:40.375 ServerApp] Connecting to kernel 98b723d9-cc74-4ff7-a126-c3d12fd657dc.
[I 2025-02-04 15:09:40.442 ServerApp] Connecting to kernel be463339-194a-4453-818c-81b854117143.
[I 2025-02-04 15:09:40.539 ServerApp] Starting buffering for be463339-194a-4453-818c-81b854117143:c025e6d3-0395-4af6-9941-0e6f748e5bf5
[I 2025-02-04 15:09:41.483 ServerApp] Saving file at /YJ/test.oy
[I 2025-02-04 15:09:43.955 ServerApp] Saving file at /YJ/test.oy
[I 2025-02-04 15:09:57.155 ServerApp] Saving file at /YJ/test.oy
[I 2025-02-04 15:10:22.394 ServerApp] Connecting to kernel 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77.
[I 2025-02-04 15:10:22.458 ServerApp] Connecting to kernel db722577-241e-4d99-9e7e-a995cd7cb2b6.
[I 2025-02-04 15:10:22.531 ServerApp] Connecting to kernel 98b723d9-cc74-4ff7-a126-c3d12fd657dc.
[I 2025-02-04 15:10:22.611 ServerApp] Connecting to kernel be463339-194a-4453-818c-81b854117143.
[I 2025-02-04 15:10:22.726 ServerApp] Starting buffering for be463339-194a-4453-818c-81b854117143:9b78a688-3ec5-4e8c-a10c-a64092ede77c
[I 2025-02-04 15:10:26.737 ServerApp] Saving file at /YJ/test.py
[I 2025-02-04 15:10:43.483 ServerApp] Connecting to kernel 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77.
[I 2025-02-04 15:10:43.545 ServerApp] Connecting to kernel db722577-241e-4d99-9e7e-a995cd7cb2b6.
[I 2025-02-04 15:10:43.614 ServerApp] Connecting to kernel 98b723d9-cc74-4ff7-a126-c3d12fd657dc.
[I 2025-02-04 15:10:43.693 ServerApp] Connecting to kernel be463339-194a-4453-818c-81b854117143.
[I 2025-02-04 15:10:43.804 ServerApp] Starting buffering for be463339-194a-4453-818c-81b854117143:05843cf4-563a-4b29-9924-b6f517d94a27
[I 2025-02-04 15:10:49.892 ServerApp] Creating new file in /YJ
[I 2025-02-04 15:10:55.099 ServerApp] Connecting to kernel 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77.
[I 2025-02-04 15:10:55.197 ServerApp] Connecting to kernel db722577-241e-4d99-9e7e-a995cd7cb2b6.
[I 2025-02-04 15:10:55.266 ServerApp] Connecting to kernel 98b723d9-cc74-4ff7-a126-c3d12fd657dc.
[I 2025-02-04 15:10:55.339 ServerApp] Connecting to kernel be463339-194a-4453-818c-81b854117143.
[I 2025-02-04 15:10:55.433 ServerApp] Starting buffering for be463339-194a-4453-818c-81b854117143:13d3f925-0c96-448d-aa34-884f618c1ae7
[I 2025-02-04 15:10:56.423 ServerApp] Saving file at /YJ/test2.py
[I 2025-02-04 15:11:18.071 ServerApp] Connecting to kernel 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77.
[I 2025-02-04 15:11:18.149 ServerApp] Connecting to kernel db722577-241e-4d99-9e7e-a995cd7cb2b6.
[I 2025-02-04 15:11:18.217 ServerApp] Connecting to kernel 98b723d9-cc74-4ff7-a126-c3d12fd657dc.
[I 2025-02-04 15:11:18.287 ServerApp] Connecting to kernel be463339-194a-4453-818c-81b854117143.
[I 2025-02-04 15:11:18.382 ServerApp] Starting buffering for be463339-194a-4453-818c-81b854117143:dec36219-353b-4e25-b4cd-eb1d0d56a508
[I 2025-02-04 15:11:33.155 ServerApp] Saving file at /YJ/test2.py
[I 2025-02-04 15:13:29.737 ServerApp] Saving file at /YJ/test2.py
[I 2025-02-04 15:14:42.065 ServerApp] Saving file at /YJ/test2.py
[I 2025-02-04 15:15:08.910 ServerApp] Saving file at /YJ/test2.py
[I 2025-02-04 15:17:09.182 ServerApp] Saving file at /YJ/test2.py
[I 2025-02-04 15:30:14.619 ServerApp] Saving file at /YJ/test2.py
[I 2025-02-04 16:06:35.594 ServerApp] Starting buffering for 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77:53c63dd5-b387-422a-acbb-79bf05f1012e
[I 2025-02-04 16:06:35.606 ServerApp] Starting buffering for db722577-241e-4d99-9e7e-a995cd7cb2b6:a5014fc8-5c16-4a5c-b9eb-4bff61fed096
[I 2025-02-04 16:06:35.617 ServerApp] Starting buffering for 98b723d9-cc74-4ff7-a126-c3d12fd657dc:147d7787-af5c-4106-9e1a-b269f5494e52
[C 2025-02-04 18:54:49.774 ServerApp] received signal 15, stopping
[I 2025-02-04 18:54:49.793 ServerApp] Shutting down 5 extensions
[I 2025-02-04 18:54:49.794 ServerApp] Shutting down 4 kernels
[I 2025-02-04 18:54:49.794 ServerApp] Kernel shutdown: 98b723d9-cc74-4ff7-a126-c3d12fd657dc
[I 2025-02-04 18:54:49.795 ServerApp] Kernel shutdown: db722577-241e-4d99-9e7e-a995cd7cb2b6
[I 2025-02-04 18:54:49.843 ServerApp] Kernel shutdown: 2f11043f-dcd9-4b2d-9f80-fd8dce8b8d77
[I 2025-02-04 18:54:49.844 ServerApp] Kernel shutdown: be463339-194a-4453-818c-81b854117143
[I 2025-02-05 09:23:16.319 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2025-02-05 09:23:16.325 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2025-02-05 09:23:16.329 ServerApp] jupyterlab | extension was successfully linked.
[W 2025-02-05 09:23:16.332 JupyterNotebookApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-05 09:23:16.335 ServerApp] ServerApp.password config is deprecated in 2.0. Use PasswordIdentityProvider.hashed_password.
[I 2025-02-05 09:23:16.335 ServerApp] notebook | extension was successfully linked.
[I 2025-02-05 09:23:16.858 ServerApp] notebook_shim | extension was successfully linked.
[I 2025-02-05 09:23:16.956 ServerApp] notebook_shim | extension was successfully loaded.
[I 2025-02-05 09:23:16.958 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2025-02-05 09:23:16.959 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2025-02-05 09:23:16.973 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyterlab
[I 2025-02-05 09:23:16.973 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/envs/Project/share/jupyter/lab
[I 2025-02-05 09:23:16.973 LabApp] Extension Manager is 'pypi'.
[I 2025-02-05 09:23:17.031 ServerApp] jupyterlab | extension was successfully loaded.
[I 2025-02-05 09:23:17.035 ServerApp] notebook | extension was successfully loaded.
[I 2025-02-05 09:23:17.035 ServerApp] Serving notebooks from local directory: /home/lab13/project
[I 2025-02-05 09:23:17.035 ServerApp] Jupyter Server 2.14.2 is running at:
[I 2025-02-05 09:23:17.035 ServerApp] http://ip-172-31-13-81:8917/tree
[I 2025-02-05 09:23:17.035 ServerApp]     http://127.0.0.1:8917/tree
[I 2025-02-05 09:23:17.035 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 2025-02-05 09:23:17.055 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[I 2025-02-05 09:23:37.091 ServerApp] 302 GET / (@125.129.250.60) 0.50ms
[I 2025-02-05 09:23:37.124 JupyterNotebookApp] 302 GET /tree? (@125.129.250.60) 0.62ms
[I 2025-02-05 09:23:42.992 ServerApp] User d5a2526841e2479cbfd1197f97234a6f logged in.
[I 2025-02-05 09:23:42.992 ServerApp] 302 POST /login?next=%2Ftree%3F (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 55.53ms
[W 2025-02-05 09:23:48.286 ServerApp] Notebook YJ/youtube_preprocessing.ipynb is not trusted
[I 2025-02-05 09:25:09.057 ServerApp] Kernel started: 1f7d14fb-69ae-4638-884e-321cec2f975d
[I 2025-02-05 09:25:09.607 ServerApp] Connecting to kernel 1f7d14fb-69ae-4638-884e-321cec2f975d.
[I 2025-02-05 09:25:09.696 ServerApp] Connecting to kernel 1f7d14fb-69ae-4638-884e-321cec2f975d.
[I 2025-02-05 09:25:09.769 ServerApp] Connecting to kernel 1f7d14fb-69ae-4638-884e-321cec2f975d.
[I 2025-02-05 09:25:12.288 ServerApp] Starting buffering for 1f7d14fb-69ae-4638-884e-321cec2f975d:4c1a4aee-b37e-4491-99cd-b5b95c831be0
[I 2025-02-05 09:25:17.076 ServerApp] Connecting to kernel 1f7d14fb-69ae-4638-884e-321cec2f975d.
[I 2025-02-05 09:25:17.302 ServerApp] Starting buffering for 1f7d14fb-69ae-4638-884e-321cec2f975d:31473ffc-d9e7-4d83-8a02-c8e7e39b1926
[I 2025-02-05 09:25:21.833 ServerApp] Connecting to kernel 1f7d14fb-69ae-4638-884e-321cec2f975d.
[I 2025-02-05 09:25:22.010 ServerApp] Starting buffering for 1f7d14fb-69ae-4638-884e-321cec2f975d:ef99a05b-e944-4b15-8bdd-ceee24515a67
[I 2025-02-05 09:25:29.467 ServerApp] Connecting to kernel 1f7d14fb-69ae-4638-884e-321cec2f975d.
[I 2025-02-05 09:25:29.562 ServerApp] Starting buffering for 1f7d14fb-69ae-4638-884e-321cec2f975d:465d451c-b6f7-4924-8fb6-166e93e50692
[W 2025-02-05 09:25:34.137 ServerApp] delete /YJ/youtube_preprocessing.py
[I 2025-02-05 09:30:56.385 ServerApp] Connecting to kernel 1f7d14fb-69ae-4638-884e-321cec2f975d.
[I 2025-02-05 09:30:56.571 ServerApp] Starting buffering for 1f7d14fb-69ae-4638-884e-321cec2f975d:20bb1378-8391-4d95-babf-9bcf51ea0e96
[I 2025-02-05 09:32:35.413 ServerApp] Connecting to kernel 1f7d14fb-69ae-4638-884e-321cec2f975d.
[I 2025-02-05 09:32:35.594 ServerApp] Starting buffering for 1f7d14fb-69ae-4638-884e-321cec2f975d:6e52e78d-6d1d-4bba-90fc-96ad764bc571
[I 2025-02-05 09:32:39.415 ServerApp] Saving file at /YJ/test.py
[I 2025-02-05 09:32:40.694 ServerApp] Saving file at /YJ/test.py
[I 2025-02-05 09:42:04.749 ServerApp] Saving file at /YJ/youtube_preprocessing.py
[I 2025-02-05 09:43:03.231 ServerApp] Saving file at /YJ/youtube_preprocessing.py
[I 2025-02-05 11:41:35.322 ServerApp] Connecting to kernel 1f7d14fb-69ae-4638-884e-321cec2f975d.
[I 2025-02-05 11:41:35.575 ServerApp] Starting buffering for 1f7d14fb-69ae-4638-884e-321cec2f975d:55a023a6-e895-4e45-aa94-1501cbffcc74
[I 2025-02-05 11:41:35.758 ServerApp] Kernel started: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[I 2025-02-05 11:41:36.216 ServerApp] Connecting to kernel b5908e22-fed1-45b7-9b39-3bdc0cb34699.
[I 2025-02-05 11:41:36.810 ServerApp] Starting buffering for b5908e22-fed1-45b7-9b39-3bdc0cb34699:f952f840-7f50-462b-a688-4b92ce34a31d
[W 2025-02-05 11:41:38.030 ServerApp] Notebook YJ/Untitled.ipynb is not trusted
[I 2025-02-05 11:41:39.556 ServerApp] Connecting to kernel 1f7d14fb-69ae-4638-884e-321cec2f975d.
[I 2025-02-05 11:41:39.653 ServerApp] Connecting to kernel b5908e22-fed1-45b7-9b39-3bdc0cb34699.
[W 2025-02-05 11:41:39.661 ServerApp] Notebook YJ/Untitled.ipynb is not trusted
[I 2025-02-05 11:41:39.842 ServerApp] Starting buffering for b5908e22-fed1-45b7-9b39-3bdc0cb34699:9a5c3dfc-7cea-45f8-9007-f878dae1004a
[I 2025-02-05 11:41:39.845 ServerApp] Starting buffering for 1f7d14fb-69ae-4638-884e-321cec2f975d:1629dab6-8937-40d1-848c-b8413624a62d
[I 2025-02-05 11:41:39.959 ServerApp] Kernel started: 153bcb25-8880-4d43-bcd2-f03f3bd7138b
[I 2025-02-05 11:41:40.428 ServerApp] Connecting to kernel 153bcb25-8880-4d43-bcd2-f03f3bd7138b.
[I 2025-02-05 11:41:43.068 ServerApp] Starting buffering for 153bcb25-8880-4d43-bcd2-f03f3bd7138b:a4ebfb80-86ba-438a-9ed0-5c33a24deaea
[I 2025-02-05 11:41:58.183 ServerApp] Connecting to kernel 1f7d14fb-69ae-4638-884e-321cec2f975d.
[I 2025-02-05 11:41:58.272 ServerApp] Connecting to kernel b5908e22-fed1-45b7-9b39-3bdc0cb34699.
[I 2025-02-05 11:41:58.349 ServerApp] Connecting to kernel 153bcb25-8880-4d43-bcd2-f03f3bd7138b.
[I 2025-02-05 11:41:58.433 ServerApp] Starting buffering for 1f7d14fb-69ae-4638-884e-321cec2f975d:778c69c8-5e20-401a-ae0e-22b0da2245bd
[I 2025-02-05 11:41:58.434 ServerApp] Starting buffering for b5908e22-fed1-45b7-9b39-3bdc0cb34699:112f2a08-2975-4277-bfe1-99e964b05255
[I 2025-02-05 11:41:58.525 ServerApp] Starting buffering for 153bcb25-8880-4d43-bcd2-f03f3bd7138b:2a634770-efbc-453c-8db1-907e2be2d80a
[I 2025-02-05 11:41:58.564 ServerApp] Connecting to kernel b5908e22-fed1-45b7-9b39-3bdc0cb34699.
[I 2025-02-05 11:43:58.987 ServerApp] Saving file at /YJ/Untitled1.ipynb
[C 2025-02-05 11:50:59.613 ServerApp] received signal 15, stopping
[I 2025-02-05 11:50:59.614 ServerApp] Shutting down 5 extensions
[I 2025-02-05 11:50:59.615 ServerApp] Shutting down 3 kernels
[I 2025-02-05 11:50:59.616 ServerApp] Kernel shutdown: 1f7d14fb-69ae-4638-884e-321cec2f975d
[I 2025-02-05 11:50:59.616 ServerApp] Kernel shutdown: 153bcb25-8880-4d43-bcd2-f03f3bd7138b
[I 2025-02-05 11:50:59.616 ServerApp] Kernel shutdown: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[I 2025-02-05 13:37:05.650 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2025-02-05 13:37:05.656 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2025-02-05 13:37:05.661 ServerApp] jupyterlab | extension was successfully linked.
[W 2025-02-05 13:37:05.663 JupyterNotebookApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-05 13:37:05.666 ServerApp] ServerApp.password config is deprecated in 2.0. Use PasswordIdentityProvider.hashed_password.
[I 2025-02-05 13:37:05.666 ServerApp] notebook | extension was successfully linked.
[I 2025-02-05 13:37:06.203 ServerApp] notebook_shim | extension was successfully linked.
[I 2025-02-05 13:37:06.302 ServerApp] notebook_shim | extension was successfully loaded.
[I 2025-02-05 13:37:06.304 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2025-02-05 13:37:06.305 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2025-02-05 13:37:06.321 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyterlab
[I 2025-02-05 13:37:06.321 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/envs/Project/share/jupyter/lab
[I 2025-02-05 13:37:06.321 LabApp] Extension Manager is 'pypi'.
[I 2025-02-05 13:37:06.381 ServerApp] jupyterlab | extension was successfully loaded.
[I 2025-02-05 13:37:06.385 ServerApp] notebook | extension was successfully loaded.
[I 2025-02-05 13:37:06.385 ServerApp] Serving notebooks from local directory: /home/lab13/project
[I 2025-02-05 13:37:06.385 ServerApp] Jupyter Server 2.14.2 is running at:
[I 2025-02-05 13:37:06.385 ServerApp] http://ip-172-31-13-81:8917/tree
[I 2025-02-05 13:37:06.385 ServerApp]     http://127.0.0.1:8917/tree
[I 2025-02-05 13:37:06.385 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 2025-02-05 13:37:06.405 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[I 2025-02-05 13:37:10.652 ServerApp] Creating new notebook in /YJ
[I 2025-02-05 13:37:11.111 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 13:37:11.217 ServerApp] Kernel started: a584f394-569e-42d6-9a06-852cf79a6f65
[I 2025-02-05 13:37:11.806 ServerApp] Connecting to kernel a584f394-569e-42d6-9a06-852cf79a6f65.
[I 2025-02-05 13:37:11.899 ServerApp] Connecting to kernel a584f394-569e-42d6-9a06-852cf79a6f65.
[I 2025-02-05 13:37:12.606 ServerApp] Connecting to kernel a584f394-569e-42d6-9a06-852cf79a6f65.
[I 2025-02-05 13:37:12.952 ServerApp] Connecting to kernel a584f394-569e-42d6-9a06-852cf79a6f65.
[W 2025-02-05 13:37:19.818 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730238520 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:37:19.819 ServerApp] wrote error: 'Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699)
[W 2025-02-05 13:37:19.825 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730238520 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 7.27ms referer=http://15.168.221.131:8917/notebooks/YJ/Untitled1.ipynb
[W 2025-02-05 13:37:20.844 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699/channels?session_id=db246c77-6b84-4299-b258-84001c56cf90 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:37:20.907 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699/channels?session_id=db246c77-6b84-4299-b258-84001c56cf90 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 63.28ms referer=None
[W 2025-02-05 13:37:20.934 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730239637 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:37:20.934 ServerApp] wrote error: 'Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699)
[W 2025-02-05 13:37:20.935 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730239637 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 0.98ms referer=http://15.168.221.131:8917/notebooks/YJ/Untitled1.ipynb
[W 2025-02-05 13:37:21.851 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699/channels?session_id=db246c77-6b84-4299-b258-84001c56cf90 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:37:21.851 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699/channels?session_id=db246c77-6b84-4299-b258-84001c56cf90 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 1.13ms referer=None
[W 2025-02-05 13:37:21.884 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730240587 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:37:21.884 ServerApp] wrote error: 'Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699)
[W 2025-02-05 13:37:21.885 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730240587 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 0.90ms referer=http://15.168.221.131:8917/notebooks/YJ/Untitled1.ipynb
[W 2025-02-05 13:37:24.849 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699/channels?session_id=db246c77-6b84-4299-b258-84001c56cf90 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:37:24.849 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699/channels?session_id=db246c77-6b84-4299-b258-84001c56cf90 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 1.38ms referer=None
[W 2025-02-05 13:37:24.882 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730243585 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:37:24.882 ServerApp] wrote error: 'Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699)
[W 2025-02-05 13:37:24.882 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730243585 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 1.06ms referer=http://15.168.221.131:8917/notebooks/YJ/Untitled1.ipynb
[W 2025-02-05 13:37:33.854 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699/channels?session_id=db246c77-6b84-4299-b258-84001c56cf90 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:37:33.855 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699/channels?session_id=db246c77-6b84-4299-b258-84001c56cf90 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 1.31ms referer=None
[W 2025-02-05 13:37:33.887 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730252591 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:37:33.887 ServerApp] wrote error: 'Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699)
[W 2025-02-05 13:37:33.888 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730252591 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 1.13ms referer=http://15.168.221.131:8917/notebooks/YJ/Untitled1.ipynb
[W 2025-02-05 13:37:43.840 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699/channels?session_id=db246c77-6b84-4299-b258-84001c56cf90 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:37:43.841 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699/channels?session_id=db246c77-6b84-4299-b258-84001c56cf90 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 1.32ms referer=None
[W 2025-02-05 13:37:43.873 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730262577 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:37:43.873 ServerApp] wrote error: 'Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699)
[W 2025-02-05 13:37:43.874 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730262577 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 1.03ms referer=http://15.168.221.131:8917/notebooks/YJ/Untitled1.ipynb
[W 2025-02-05 13:38:44.850 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699/channels?session_id=db246c77-6b84-4299-b258-84001c56cf90 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:38:44.851 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699/channels?session_id=db246c77-6b84-4299-b258-84001c56cf90 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 1.30ms referer=None
[W 2025-02-05 13:38:44.883 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730323588 (125.129.250.60): Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699
[W 2025-02-05 13:38:44.884 ServerApp] wrote error: 'Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: b5908e22-fed1-45b7-9b39-3bdc0cb34699)
[W 2025-02-05 13:38:44.884 ServerApp] 404 GET /api/kernels/b5908e22-fed1-45b7-9b39-3bdc0cb34699?1738730323588 (d5a2526841e2479cbfd1197f97234a6f@125.129.250.60) 1.02ms referer=http://15.168.221.131:8917/notebooks/YJ/Untitled1.ipynb
[I 2025-02-05 13:39:12.802 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 13:41:44.012 ServerApp] Connecting to kernel a584f394-569e-42d6-9a06-852cf79a6f65.
[I 2025-02-05 13:41:47.908 ServerApp] Connecting to kernel a584f394-569e-42d6-9a06-852cf79a6f65.
[I 2025-02-05 13:44:42.243 ServerApp] Connecting to kernel a584f394-569e-42d6-9a06-852cf79a6f65.
[I 2025-02-05 13:44:42.583 ServerApp] Connecting to kernel a584f394-569e-42d6-9a06-852cf79a6f65.
[I 2025-02-05 13:44:47.029 ServerApp] Kernel started: 4c92c4dd-75fc-4889-9cba-54c431e1a36c
[I 2025-02-05 13:44:47.030 ServerApp] Kernel shutdown: a584f394-569e-42d6-9a06-852cf79a6f65
[I 2025-02-05 13:44:47.622 ServerApp] Connecting to kernel 4c92c4dd-75fc-4889-9cba-54c431e1a36c.
[I 2025-02-05 13:44:47.683 ServerApp] Connecting to kernel 4c92c4dd-75fc-4889-9cba-54c431e1a36c.
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: a6fbb616-d85f-4001-87d2-5adb2349c1a8
[I 2025-02-05 13:46:42.836 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 13:48:42.955 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 13:50:43.829 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 13:54:47.240 ServerApp] Starting buffering for 4c92c4dd-75fc-4889-9cba-54c431e1a36c:688ce441-ed97-44a7-a45c-bcf29c3802a9
[I 2025-02-05 13:54:48.602 ServerApp] Connecting to kernel 4c92c4dd-75fc-4889-9cba-54c431e1a36c.
[I 2025-02-05 13:54:48.859 ServerApp] Starting buffering for 4c92c4dd-75fc-4889-9cba-54c431e1a36c:de880a46-1801-4bec-9d6a-c2f7634bc548
[I 2025-02-05 13:54:48.977 ServerApp] Connecting to kernel 4c92c4dd-75fc-4889-9cba-54c431e1a36c.
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: 9d50e5ee-5f28-49c4-a5a5-6438b5c15b6d
[I 2025-02-05 13:54:52.127 ServerApp] Kernel started: d7a62dfa-cae8-4506-b71e-c49c72c856e4
[I 2025-02-05 13:54:52.128 ServerApp] Kernel shutdown: 4c92c4dd-75fc-4889-9cba-54c431e1a36c
[I 2025-02-05 13:54:53.109 ServerApp] Connecting to kernel d7a62dfa-cae8-4506-b71e-c49c72c856e4.
[I 2025-02-05 13:54:53.177 ServerApp] Connecting to kernel d7a62dfa-cae8-4506-b71e-c49c72c856e4.
[I 2025-02-05 13:56:48.831 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 13:58:49.819 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 14:02:50.827 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 14:06:50.945 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 14:08:51.279 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 14:10:51.817 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 14:12:51.943 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 14:16:52.055 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 14:18:52.169 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 14:21:25.218 ServerApp] Saving file at /YJ/Untitled2.ipynb
[I 2025-02-05 14:40:28.779 ServerApp] Connecting to kernel d7a62dfa-cae8-4506-b71e-c49c72c856e4.
[I 2025-02-05 14:40:35.924 ServerApp] Saving file at /YJ/youtube_preprocessing.py
[I 2025-02-05 14:40:38.924 ServerApp] Connecting to kernel d7a62dfa-cae8-4506-b71e-c49c72c856e4.
[I 2025-02-05 14:40:38.992 ServerApp] Connecting to kernel d7a62dfa-cae8-4506-b71e-c49c72c856e4.
[I 2025-02-05 15:45:50.423 ServerApp] Connecting to kernel d7a62dfa-cae8-4506-b71e-c49c72c856e4.
[I 2025-02-05 17:18:46.018 ServerApp] Creating new directory in /YJ
[I 2025-02-05 17:18:58.664 ServerApp] Creating new notebook in /YJ/analytics
[I 2025-02-05 17:18:58.855 ServerApp] Saving file at /YJ/analytics/Untitled.ipynb
[I 2025-02-05 17:18:58.918 ServerApp] Kernel started: 87df68ce-2e98-4a7a-8de2-d302ecc3212b
[I 2025-02-05 17:18:59.549 ServerApp] Connecting to kernel 87df68ce-2e98-4a7a-8de2-d302ecc3212b.
[I 2025-02-05 17:19:00.371 ServerApp] Connecting to kernel d7a62dfa-cae8-4506-b71e-c49c72c856e4.
[I 2025-02-05 17:19:00.434 ServerApp] Connecting to kernel 87df68ce-2e98-4a7a-8de2-d302ecc3212b.
[I 2025-02-05 17:19:00.714 ServerApp] Connecting to kernel 87df68ce-2e98-4a7a-8de2-d302ecc3212b.
[W 2025-02-05 17:19:09.121 ServerApp] Notebook YJ/youtube_preprocessing.ipynb is not trusted
[I 2025-02-05 17:19:10.430 ServerApp] Connecting to kernel d7a62dfa-cae8-4506-b71e-c49c72c856e4.
[I 2025-02-05 17:19:10.532 ServerApp] Connecting to kernel 87df68ce-2e98-4a7a-8de2-d302ecc3212b.
[W 2025-02-05 17:19:10.570 ServerApp] Notebook YJ/youtube_preprocessing.ipynb is not trusted
[I 2025-02-05 17:19:10.892 ServerApp] Kernel started: d3200e36-9336-41f6-857a-e94c1a673a51
[I 2025-02-05 17:19:11.382 ServerApp] Connecting to kernel d3200e36-9336-41f6-857a-e94c1a673a51.
[I 2025-02-05 17:19:43.282 ServerApp] Connecting to kernel d7a62dfa-cae8-4506-b71e-c49c72c856e4.
[I 2025-02-05 17:19:43.375 ServerApp] Connecting to kernel 87df68ce-2e98-4a7a-8de2-d302ecc3212b.
[I 2025-02-05 17:19:43.448 ServerApp] Connecting to kernel d3200e36-9336-41f6-857a-e94c1a673a51.
[I 2025-02-05 17:19:43.655 ServerApp] Kernel started: 67306476-de9d-472c-b0c4-a7f47624df98
[I 2025-02-05 17:19:44.139 ServerApp] Connecting to kernel 67306476-de9d-472c-b0c4-a7f47624df98.
[I 2025-02-05 17:19:53.510 ServerApp] Starting buffering for d3200e36-9336-41f6-857a-e94c1a673a51:161eba07-8bf8-4993-9611-7fa62db8ebe9
25/02/05 17:20:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 2025-02-05 17:21:00.570 ServerApp] Saving file at /YJ/analytics/Untitled.ipynb
[I 2025-02-05 17:23:00.678 ServerApp] Saving file at /YJ/analytics/Untitled.ipynb
[I 2025-02-05 17:31:12.471 ServerApp] Connecting to kernel 67306476-de9d-472c-b0c4-a7f47624df98.
[C 2025-02-05 19:00:01.280 ServerApp] received signal 15, stopping
[I 2025-02-05 19:00:01.280 ServerApp] Shutting down 5 extensions
[I 2025-02-05 19:00:01.281 ServerApp] Shutting down 4 kernels
[I 2025-02-05 19:00:01.282 ServerApp] Kernel shutdown: 87df68ce-2e98-4a7a-8de2-d302ecc3212b
[I 2025-02-05 19:00:01.282 ServerApp] Kernel shutdown: d7a62dfa-cae8-4506-b71e-c49c72c856e4
[I 2025-02-05 19:00:01.282 ServerApp] Kernel shutdown: d3200e36-9336-41f6-857a-e94c1a673a51
[I 2025-02-05 19:00:01.283 ServerApp] Kernel shutdown: 67306476-de9d-472c-b0c4-a7f47624df98
[I 2025-02-06 15:09:12.294 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2025-02-06 15:09:12.298 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2025-02-06 15:09:12.303 ServerApp] jupyterlab | extension was successfully linked.
[W 2025-02-06 15:09:12.305 JupyterNotebookApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2025-02-06 15:09:12.308 ServerApp] ServerApp.password config is deprecated in 2.0. Use PasswordIdentityProvider.hashed_password.
[I 2025-02-06 15:09:12.308 ServerApp] notebook | extension was successfully linked.
[I 2025-02-06 15:09:12.507 ServerApp] notebook_shim | extension was successfully linked.
[I 2025-02-06 15:09:12.545 ServerApp] notebook_shim | extension was successfully loaded.
[I 2025-02-06 15:09:12.547 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2025-02-06 15:09:12.548 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2025-02-06 15:09:12.550 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/envs/Project/lib/python3.8/site-packages/jupyterlab
[I 2025-02-06 15:09:12.550 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/envs/Project/share/jupyter/lab
[I 2025-02-06 15:09:12.550 LabApp] Extension Manager is 'pypi'.
[I 2025-02-06 15:09:12.577 ServerApp] jupyterlab | extension was successfully loaded.
[I 2025-02-06 15:09:12.581 ServerApp] notebook | extension was successfully loaded.
[I 2025-02-06 15:09:12.581 ServerApp] Serving notebooks from local directory: /home/lab13/project
[I 2025-02-06 15:09:12.581 ServerApp] Jupyter Server 2.14.2 is running at:
[I 2025-02-06 15:09:12.581 ServerApp] http://ip-172-31-13-81:8917/tree
[I 2025-02-06 15:09:12.581 ServerApp]     http://127.0.0.1:8917/tree
[I 2025-02-06 15:09:12.581 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 2025-02-06 15:09:12.599 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[I 2025-02-06 15:10:09.443 ServerApp] 302 GET / (@125.129.250.60) 0.47ms
[I 2025-02-06 15:10:09.475 JupyterNotebookApp] 302 GET /tree? (@125.129.250.60) 0.52ms
[I 2025-02-06 15:10:13.391 ServerApp] User 8c1b3224a2404a088fde4e8bb099b22e logged in.
[I 2025-02-06 15:10:13.392 ServerApp] 302 POST /login?next=%2Ftree%3F (8c1b3224a2404a088fde4e8bb099b22e@125.129.250.60) 37.70ms
[W 2025-02-06 15:10:17.704 ServerApp] Notebook YJ/Untitled.ipynb is not trusted
[W 2025-02-06 15:10:20.541 ServerApp] Notebook YJ/youtube_preprocessing.ipynb is not trusted
[W 2025-02-06 15:10:22.092 ServerApp] Notebook YJ/youtube_preprocessing.ipynb is not trusted
[I 2025-02-06 15:10:22.501 ServerApp] Kernel started: 54599486-28f9-4652-bdb2-a979ce71c285
[I 2025-02-06 15:10:23.009 ServerApp] Connecting to kernel 54599486-28f9-4652-bdb2-a979ce71c285.
[I 2025-02-06 15:10:23.081 ServerApp] Connecting to kernel 54599486-28f9-4652-bdb2-a979ce71c285.
[I 2025-02-06 15:10:23.142 ServerApp] Connecting to kernel 54599486-28f9-4652-bdb2-a979ce71c285.
[I 2025-02-06 15:10:25.711 ServerApp] Connecting to kernel 54599486-28f9-4652-bdb2-a979ce71c285.
25/02/06 15:10:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 4:>                                                          (0 + 4) / 5][Stage 4:===========>                                               (1 + 4) / 5][Stage 4:===============================================>           (4 + 1) / 5]                                                                                [Stage 6:>                                                          (0 + 4) / 5][Stage 6:===========>                                               (1 + 4) / 5][Stage 7:=======================>                                (84 + 4) / 200][Stage 7:==================================>                    (125 + 4) / 200][Stage 7:==============================================>        (168 + 4) / 200]                                                                                [I 2025-02-06 15:12:22.316 ServerApp] Saving file at /YJ/youtube_preprocessing.ipynb
[I 2025-02-06 15:12:51.340 ServerApp] Saving file at /YJ/youtube_preprocessing.ipynb
[I 2025-02-06 15:19:46.042 ServerApp] Saving file at /YJ/youtube_preprocessing.ipynb
[I 2025-02-06 15:19:46.976 ServerApp] Starting buffering for 54599486-28f9-4652-bdb2-a979ce71c285:6cbab958-4ef6-4449-843e-67ad561929de
[I 2025-02-06 15:19:54.023 ServerApp] Connecting to kernel 54599486-28f9-4652-bdb2-a979ce71c285.
[I 2025-02-06 15:19:54.264 ServerApp] Starting buffering for 54599486-28f9-4652-bdb2-a979ce71c285:e3b165eb-f44e-4162-a5bb-8b57b1338af7
[I 2025-02-06 15:19:54.424 ServerApp] Kernel started: 077eb05d-22c8-4133-ab21-92b4631a5e18
[I 2025-02-06 15:19:54.886 ServerApp] Connecting to kernel 077eb05d-22c8-4133-ab21-92b4631a5e18.
[I 2025-02-06 15:20:10.555 ServerApp] Connecting to kernel 54599486-28f9-4652-bdb2-a979ce71c285.
[I 2025-02-06 15:20:10.660 ServerApp] Connecting to kernel 077eb05d-22c8-4133-ab21-92b4631a5e18.
[I 2025-02-06 15:20:10.816 ServerApp] Starting buffering for 54599486-28f9-4652-bdb2-a979ce71c285:3ecdbeea-13f2-4675-8936-633a0dbdf916
[I 2025-02-06 15:20:11.093 ServerApp] Connecting to kernel 54599486-28f9-4652-bdb2-a979ce71c285.
[I 2025-02-06 15:20:20.325 ServerApp] Saving file at /YJ/youtube_preprocessing.ipynb
25/02/06 15:20:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 2025-02-06 15:21:54.322 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[I 2025-02-06 15:22:11.669 ServerApp] Saving file at /YJ/youtube_preprocessing.ipynb
[I 2025-02-06 15:23:54.424 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[I 2025-02-06 15:29:42.026 ServerApp] Connecting to kernel 54599486-28f9-4652-bdb2-a979ce71c285.
[I 2025-02-06 15:29:42.115 ServerApp] Connecting to kernel 077eb05d-22c8-4133-ab21-92b4631a5e18.
[I 2025-02-06 15:29:54.556 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[I 2025-02-06 15:30:19.054 ServerApp] Kernel restarted: 077eb05d-22c8-4133-ab21-92b4631a5e18
[I 2025-02-06 15:30:19.090 ServerApp] Starting buffering for 077eb05d-22c8-4133-ab21-92b4631a5e18:5cd12b90-1af4-4e2c-a34e-269db9eea263
[I 2025-02-06 15:30:19.124 ServerApp] Connecting to kernel 077eb05d-22c8-4133-ab21-92b4631a5e18.
[I 2025-02-06 15:30:19.124 ServerApp] Restoring connection for 077eb05d-22c8-4133-ab21-92b4631a5e18:5cd12b90-1af4-4e2c-a34e-269db9eea263
25/02/06 15:30:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-06 15:31:54.676 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[I 2025-02-06 15:33:54.783 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[I 2025-02-06 15:35:54.883 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[Stage 10:>                                                         (0 + 1) / 1][Stage 11:>                                                         (0 + 1) / 1]                                                                                [Stage 13:>                                                         (0 + 1) / 1]                                                                                25/02/06 15:36:33 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB
25/02/06 15:36:48 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB
25/02/06 15:36:53 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB
[Stage 17:>                                                         (0 + 1) / 1][I 2025-02-06 15:37:54.997 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
                                                                                25/02/06 15:37:59 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB
[Stage 21:>                                                         (0 + 1) / 1]                                                                                25/02/06 15:38:06 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:38:12 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:38:40 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 24:>                                                         (0 + 1) / 1]                                                                                25/02/06 15:38:47 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:38:47 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 26:>                                                         (0 + 1) / 1]                                                                                25/02/06 15:38:48 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:38:48 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:38:49 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:38:49 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:38:49 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
25/02/06 15:38:49 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
25/02/06 15:38:50 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:38:50 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:38:51 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[Stage 38:>                                                         (0 + 1) / 1]25/02/06 15:38:54 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[Stage 39:>                                                       (0 + 4) / 200][Stage 39:=>                                                      (5 + 5) / 200][Stage 39:===>                                                   (11 + 4) / 200][Stage 39:===>                                                   (13 + 4) / 200][Stage 39:====>                                                  (17 + 4) / 200][Stage 39:=====>                                                 (21 + 4) / 200][Stage 39:======>                                                (25 + 4) / 200][Stage 39:=======>                                               (29 + 4) / 200][Stage 39:=========>                                             (33 + 4) / 200][Stage 39:==========>                                            (38 + 4) / 200][Stage 39:===========>                                           (42 + 4) / 200][Stage 39:============>                                          (46 + 4) / 200][Stage 39:=============>                                         (50 + 4) / 200][Stage 39:==============>                                        (54 + 4) / 200][Stage 39:===============>                                       (58 + 4) / 200][Stage 39:=================>                                     (63 + 4) / 200][Stage 39:=================>                                     (65 + 4) / 200][Stage 39:==================>                                    (69 + 4) / 200][Stage 39:====================>                                  (74 + 4) / 200][Stage 39:======================>                                (80 + 4) / 200][Stage 39:=======================>                               (85 + 4) / 200][Stage 39:========================>                              (90 + 4) / 200][Stage 39:=========================>                             (93 + 4) / 200][Stage 39:==========================>                            (98 + 4) / 200][Stage 39:===========================>                          (102 + 4) / 200][Stage 39:=============================>                        (108 + 5) / 200][Stage 39:==============================>                       (113 + 4) / 200][Stage 39:===============================>                      (118 + 4) / 200][Stage 39:================================>                     (122 + 4) / 200][Stage 39:==================================>                   (126 + 4) / 200][Stage 39:===================================>                  (130 + 4) / 200][Stage 39:====================================>                 (134 + 4) / 200][Stage 39:=====================================>                (140 + 4) / 200][Stage 39:=======================================>              (146 + 4) / 200][Stage 39:=======================================>              (147 + 4) / 200][Stage 39:=========================================>            (154 + 4) / 200][Stage 39:==========================================>           (158 + 4) / 200][Stage 39:===========================================>          (162 + 4) / 200][Stage 39:============================================>         (165 + 4) / 200][Stage 39:=============================================>        (169 + 4) / 200][Stage 39:==============================================>       (171 + 4) / 200][Stage 39:===============================================>      (175 + 5) / 200][Stage 39:================================================>     (180 + 4) / 200][Stage 39:=================================================>    (184 + 4) / 200][Stage 39:===================================================>  (189 + 5) / 200][Stage 39:====================================================> (194 + 4) / 200]                                                                                25/02/06 15:39:09 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
25/02/06 15:39:37 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[Stage 41:>                                                         (0 + 1) / 1]25/02/06 15:39:41 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
                                                                                25/02/06 15:39:41 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
25/02/06 15:39:41 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[Stage 46:===========>                                             (4 + 4) / 20][Stage 46:======================>                                  (8 + 4) / 20][Stage 46:=================================>                      (12 + 4) / 20][Stage 46:===============================================>        (17 + 3) / 20]                                                                                25/02/06 15:39:43 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[Stage 48:==>                                                     (4 + 4) / 100][Stage 48:====>                                                   (8 + 4) / 100][Stage 48:=======>                                               (13 + 4) / 100][Stage 48:=========>                                             (17 + 4) / 100][Stage 48:===========>                                           (21 + 4) / 100][Stage 48:=============>                                         (25 + 4) / 100][Stage 48:===============>                                       (29 + 4) / 100][Stage 48:==================>                                    (33 + 4) / 100][Stage 48:====================>                                  (37 + 5) / 100][Stage 48:=======================>                               (43 + 4) / 100][Stage 48:==========================>                            (48 + 4) / 100][Stage 48:============================>                          (52 + 4) / 100][Stage 48:==============================>                        (56 + 4) / 100][Stage 48:=================================>                     (60 + 4) / 100][Stage 48:===================================>                   (64 + 4) / 100][Stage 48:=====================================>                 (69 + 4) / 100][Stage 48:========================================>              (73 + 4) / 100][Stage 48:=========================================>             (75 + 4) / 100][Stage 48:============================================>          (81 + 4) / 100][Stage 48:==============================================>        (85 + 4) / 100][Stage 48:=================================================>     (90 + 4) / 100][Stage 48:====================================================>  (95 + 4) / 100]                                                                                25/02/06 15:39:50 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[Stage 50:======>                                                  (8 + 4) / 75][Stage 50:==========>                                             (14 + 4) / 75][Stage 50:=============>                                          (18 + 4) / 75][Stage 50:================>                                       (22 + 4) / 75][Stage 50:====================>                                   (27 + 4) / 75][Stage 50:=======================>                                (32 + 4) / 75][Stage 50:========================>                               (33 + 4) / 75][Stage 50:============================>                           (38 + 4) / 75][Stage 50:=================================>                      (45 + 4) / 75][Stage 50:====================================>                   (49 + 4) / 75][Stage 50:======================================>                 (51 + 4) / 75][Stage 50:=========================================>              (55 + 4) / 75][I 2025-02-06 15:39:55.150 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[Stage 50:==========================================>             (57 + 4) / 75][Stage 50:===============================================>        (63 + 4) / 75][Stage 50:==================================================>     (67 + 4) / 75][Stage 50:=====================================================>  (72 + 3) / 75]                                                                                25/02/06 15:39:56 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
25/02/06 15:39:57 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[Stage 52:>                                                         (0 + 1) / 1]                                                                                25/02/06 15:40:02 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[Stage 53:>                                                         (0 + 1) / 1]                                                                                25/02/06 15:40:54 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
25/02/06 15:41:12 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
25/02/06 15:41:27 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[I 2025-02-06 15:41:55.413 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
25/02/06 15:42:05 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[Stage 57:>                                                         (0 + 1) / 1]                                                                                25/02/06 15:42:15 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[Stage 58:>                                                         (0 + 1) / 1]                                                                                25/02/06 15:43:04 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
25/02/06 15:43:40 ERROR Instrumentation: java.lang.IllegalArgumentException: tfidf_features does not exist. Available: id, video_id, title, publish_date, channel_name, comment, like_count, comment_publish_date, inserted_at
	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)
	at scala.collection.MapLike.getOrElse(MapLike.scala:131)
	at scala.collection.MapLike.getOrElse$(MapLike.scala:129)
	at scala.collection.AbstractMap.getOrElse(Map.scala:63)
	at org.apache.spark.sql.types.StructType.apply(StructType.scala:277)
	at org.apache.spark.ml.util.SchemaUtils$.checkColumnTypes(SchemaUtils.scala:59)
	at org.apache.spark.ml.util.SchemaUtils$.validateVectorCompatibleColumn(SchemaUtils.scala:205)
	at org.apache.spark.ml.clustering.KMeansParams.validateAndTransformSchema(KMeans.scala:98)
	at org.apache.spark.ml.clustering.KMeansParams.validateAndTransformSchema$(KMeans.scala:97)
	at org.apache.spark.ml.clustering.KMeans.validateAndTransformSchema(KMeans.scala:272)
	at org.apache.spark.ml.clustering.KMeans.transformSchema(KMeans.scala:372)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:330)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:329)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)

25/02/06 15:43:54 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: Column cluster already exists.
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.ml.util.SchemaUtils$.appendColumn(SchemaUtils.scala:106)
	at org.apache.spark.ml.util.SchemaUtils$.appendColumn(SchemaUtils.scala:96)
	at org.apache.spark.ml.clustering.KMeansParams.validateAndTransformSchema(KMeans.scala:99)
	at org.apache.spark.ml.clustering.KMeansParams.validateAndTransformSchema$(KMeans.scala:97)
	at org.apache.spark.ml.clustering.KMeans.validateAndTransformSchema(KMeans.scala:272)
	at org.apache.spark.ml.clustering.KMeans.transformSchema(KMeans.scala:372)
	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
	at org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:330)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:329)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)

[I 2025-02-06 15:43:55.663 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
25/02/06 15:45:06 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 60:>                                                         (0 + 1) / 1]                                                                                25/02/06 15:45:11 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:12 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:12 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:12 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:12 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:13 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:13 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:13 WARN BlockManager: Asked to remove block broadcast_74, which does not exist
25/02/06 15:45:13 WARN BlockManager: Asked to remove block broadcast_74_piece0, which does not exist
25/02/06 15:45:13 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:13 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:14 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 74:>                                                         (0 + 1) / 1]25/02/06 15:45:17 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 75:=>                                                      (4 + 4) / 200][Stage 75:==>                                                     (8 + 4) / 200][Stage 75:====>                                                  (16 + 4) / 200][Stage 75:=====>                                                 (20 + 4) / 200][Stage 75:======>                                                (25 + 4) / 200][Stage 75:========>                                              (30 + 4) / 200][Stage 75:=========>                                             (34 + 4) / 200][Stage 75:==========>                                            (38 + 4) / 200][Stage 75:===========>                                           (43 + 4) / 200][Stage 75:============>                                          (45 + 4) / 200][Stage 75:=============>                                         (49 + 4) / 200][Stage 75:===============>                                       (55 + 4) / 200][Stage 75:================>                                      (60 + 4) / 200][Stage 75:=================>                                     (64 + 4) / 200][Stage 75:==================>                                    (69 + 4) / 200][Stage 75:====================>                                  (75 + 4) / 200][Stage 75:======================>                                (80 + 4) / 200][Stage 75:======================>                                (83 + 4) / 200][Stage 75:========================>                              (88 + 4) / 200][Stage 75:=========================>                             (93 + 4) / 200][Stage 75:===========================>                           (99 + 4) / 200][Stage 75:============================>                         (104 + 4) / 200][Stage 75:=============================>                        (110 + 4) / 200][Stage 75:===============================>                      (116 + 4) / 200][Stage 75:================================>                     (119 + 4) / 200][Stage 75:=================================>                    (124 + 4) / 200][Stage 75:==================================>                   (129 + 4) / 200][Stage 75:====================================>                 (135 + 4) / 200][Stage 75:====================================>                 (137 + 4) / 200][Stage 75:======================================>               (141 + 4) / 200][Stage 75:=======================================>              (146 + 4) / 200][Stage 75:=========================================>            (152 + 4) / 200][Stage 75:==========================================>           (157 + 4) / 200][Stage 75:===========================================>          (162 + 4) / 200][Stage 75:=============================================>        (167 + 4) / 200][Stage 75:=============================================>        (170 + 4) / 200][Stage 75:===============================================>      (175 + 4) / 200][Stage 75:================================================>     (179 + 4) / 200][Stage 75:=================================================>    (184 + 4) / 200][Stage 75:===================================================>  (189 + 4) / 200][Stage 75:====================================================> (195 + 4) / 200]                                                                                25/02/06 15:45:29 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 76:>                                                         (0 + 1) / 1]                                                                                25/02/06 15:45:34 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:34 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:35 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:35 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:35 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:35 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:36 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:36 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:36 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:36 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:37 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:37 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:37 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:38 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:38 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:38 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:38 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:39 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:39 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:39 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:39 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:40 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:40 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:40 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:45:41 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[Stage 138:>                                                        (0 + 1) / 1]25/02/06 15:45:43 WARN DAGScheduler: Broadcasting large task binary with size 11.4 MiB
[Stage 139:=>                                                     (7 + 4) / 200][Stage 139:==>                                                   (11 + 4) / 200][Stage 139:===>                                                  (14 + 4) / 200][Stage 139:====>                                                 (18 + 4) / 200][Stage 139:=====>                                                (22 + 4) / 200][Stage 139:=======>                                              (26 + 4) / 200][Stage 139:========>                                             (31 + 4) / 200][Stage 139:=========>                                            (36 + 4) / 200][Stage 139:==========>                                           (40 + 4) / 200][Stage 139:===========>                                          (44 + 4) / 200][Stage 139:============>                                         (48 + 4) / 200][Stage 139:==============>                                       (53 + 4) / 200][Stage 139:===============>                                      (57 + 4) / 200][Stage 139:================>                                     (62 + 4) / 200][Stage 139:=================>                                    (66 + 4) / 200][Stage 139:==================>                                   (67 + 4) / 200][Stage 139:===================>                                  (71 + 4) / 200][Stage 139:=====================>                                (78 + 4) / 200][Stage 139:=====================>                                (81 + 4) / 200][Stage 139:======================>                               (85 + 4) / 200][Stage 139:========================>                             (91 + 4) / 200][Stage 139:=========================>                            (96 + 4) / 200][Stage 139:==========================>                          (100 + 4) / 200][Stage 139:===========================>                         (104 + 4) / 200][Stage 139:=============================>                       (110 + 4) / 200][Stage 139:==============================>                      (114 + 4) / 200][Stage 139:==============================>                      (115 + 4) / 200][Stage 139:================================>                    (121 + 4) / 200][Stage 139:=================================>                   (126 + 4) / 200][Stage 139:==================================>                  (130 + 4) / 200][Stage 139:====================================>                (136 + 4) / 200][Stage 139:====================================>                (139 + 4) / 200][Stage 139:=====================================>               (140 + 4) / 200][Stage 139:======================================>              (144 + 4) / 200][Stage 139:=======================================>             (149 + 4) / 200][Stage 139:=========================================>           (156 + 4) / 200][Stage 139:==========================================>          (160 + 4) / 200][I 2025-02-06 15:45:55.859 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[Stage 139:===========================================>         (164 + 4) / 200][Stage 139:============================================>        (167 + 4) / 200][Stage 139:=============================================>       (172 + 4) / 200][Stage 139:==============================================>      (177 + 4) / 200][Stage 139:================================================>    (182 + 4) / 200][Stage 139:=================================================>   (186 + 4) / 200][Stage 139:==================================================>  (191 + 4) / 200][Stage 139:===================================================> (194 + 4) / 200]                                                                                25/02/06 15:45:58 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 140:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:46:03 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:03 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:04 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:04 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:04 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:05 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:05 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:05 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:06 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:06 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:06 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:06 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:07 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:07 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:07 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:08 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:08 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:08 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:08 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:09 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:09 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:09 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:10 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:10 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:10 WARN DAGScheduler: Broadcasting large task binary with size 13.0 MiB
[Stage 202:>                                                        (0 + 1) / 1]25/02/06 15:46:13 WARN DAGScheduler: Broadcasting large task binary with size 13.0 MiB
[Stage 203:=>                                                     (6 + 4) / 200][Stage 203:==>                                                    (8 + 4) / 200][Stage 203:===>                                                  (12 + 4) / 200][Stage 203:====>                                                 (16 + 4) / 200][Stage 203:=====>                                                (20 + 4) / 200][Stage 203:======>                                               (24 + 4) / 200][Stage 203:=======>                                              (28 + 4) / 200][Stage 203:========>                                             (32 + 4) / 200][Stage 203:==========>                                           (38 + 4) / 200][Stage 203:===========>                                          (41 + 5) / 200][Stage 203:============>                                         (46 + 4) / 200][Stage 203:=============>                                        (49 + 4) / 200][Stage 203:==============>                                       (54 + 4) / 200][Stage 203:===============>                                      (59 + 4) / 200][Stage 203:=================>                                    (64 + 4) / 200][Stage 203:==================>                                   (67 + 4) / 200][Stage 203:==================>                                   (68 + 5) / 200][Stage 203:===================>                                  (72 + 4) / 200][Stage 203:====================>                                 (76 + 4) / 200][Stage 203:=====================>                                (81 + 4) / 200][Stage 203:======================>                               (84 + 4) / 200][Stage 203:=======================>                              (86 + 4) / 200][Stage 203:========================>                             (90 + 4) / 200][Stage 203:=========================>                            (93 + 4) / 200][Stage 203:=========================>                            (96 + 4) / 200][Stage 203:===========================>                         (102 + 4) / 200][Stage 203:============================>                        (106 + 4) / 200][Stage 203:============================>                        (107 + 4) / 200][Stage 203:=============================>                       (111 + 4) / 200][Stage 203:===============================>                     (117 + 4) / 200][Stage 203:================================>                    (122 + 4) / 200][Stage 203:=================================>                   (126 + 4) / 200][Stage 203:=================================>                   (128 + 4) / 200][Stage 203:===================================>                 (133 + 4) / 200][Stage 203:====================================>                (137 + 4) / 200][Stage 203:=====================================>               (142 + 4) / 200][Stage 203:======================================>              (147 + 4) / 200][Stage 203:=======================================>             (148 + 4) / 200][Stage 203:=========================================>           (155 + 4) / 200][Stage 203:==========================================>          (159 + 4) / 200][Stage 203:===========================================>         (163 + 4) / 200][Stage 203:============================================>        (167 + 4) / 200][Stage 203:=============================================>       (171 + 4) / 200][Stage 203:==============================================>      (175 + 4) / 200][Stage 203:===============================================>     (180 + 4) / 200][Stage 203:================================================>    (184 + 4) / 200][Stage 203:=================================================>   (188 + 4) / 200][Stage 203:===================================================> (193 + 4) / 200]                                                                                25/02/06 15:46:31 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 204:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:46:36 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:36 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:37 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:37 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:37 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:37 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:38 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:38 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:39 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:39 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:39 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:40 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:40 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:40 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:40 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:41 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:41 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:41 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:42 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:42 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:42 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:43 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:43 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:43 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:46:44 WARN DAGScheduler: Broadcasting large task binary with size 14.6 MiB
[Stage 266:>                                                        (0 + 1) / 1]25/02/06 15:46:47 WARN DAGScheduler: Broadcasting large task binary with size 14.6 MiB
[Stage 267:=>                                                     (4 + 4) / 200][Stage 267:==>                                                    (9 + 4) / 200][Stage 267:===>                                                  (13 + 4) / 200][Stage 267:====>                                                 (17 + 4) / 200][Stage 267:=====>                                                (21 + 4) / 200][Stage 267:======>                                               (25 + 4) / 200][Stage 267:=======>                                              (29 + 4) / 200][Stage 267:========>                                             (33 + 4) / 200][Stage 267:=========>                                            (37 + 4) / 200][Stage 267:===========>                                          (42 + 4) / 200][Stage 267:============>                                         (46 + 4) / 200][Stage 267:=============>                                        (51 + 4) / 200][Stage 267:==============>                                       (52 + 4) / 200][Stage 267:===============>                                      (58 + 4) / 200][Stage 267:================>                                     (62 + 4) / 200][Stage 267:==================>                                   (67 + 4) / 200][Stage 267:==================>                                   (69 + 4) / 200][Stage 267:===================>                                  (72 + 4) / 200][Stage 267:=====================>                                (78 + 4) / 200][Stage 267:======================>                               (82 + 4) / 200][Stage 267:=======================>                              (88 + 4) / 200][Stage 267:========================>                             (92 + 4) / 200][Stage 267:==========================>                           (99 + 4) / 200][Stage 267:===========================>                         (103 + 4) / 200][Stage 267:============================>                        (107 + 4) / 200][Stage 267:=============================>                       (111 + 4) / 200][Stage 267:=============================>                       (112 + 4) / 200][Stage 267:===============================>                     (117 + 4) / 200][Stage 267:===============================>                     (120 + 4) / 200][Stage 267:================================>                    (124 + 4) / 200][Stage 267:=================================>                   (127 + 4) / 200][Stage 267:==================================>                  (131 + 4) / 200][Stage 267:===================================>                 (135 + 4) / 200][Stage 267:====================================>                (136 + 4) / 200][Stage 267:====================================>                (139 + 4) / 200][Stage 267:======================================>              (144 + 4) / 200][Stage 267:=======================================>             (150 + 4) / 200][Stage 267:========================================>            (154 + 4) / 200][Stage 267:==========================================>          (159 + 4) / 200][Stage 267:===========================================>         (163 + 4) / 200][Stage 267:============================================>        (167 + 4) / 200][Stage 267:=============================================>       (172 + 4) / 200][Stage 267:=============================================>       (173 + 4) / 200][Stage 267:==============================================>      (177 + 4) / 200][Stage 267:================================================>    (184 + 4) / 200][Stage 267:=================================================>   (188 + 5) / 200][Stage 267:==================================================>  (192 + 4) / 200][Stage 267:===================================================> (194 + 4) / 200]                                                                                25/02/06 15:47:05 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 268:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:47:10 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:10 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:11 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:11 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:11 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:12 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:12 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:12 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:13 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:13 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:13 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:14 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:14 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:14 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:15 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:15 WARN DAGScheduler: Broadcasting large task binary with size 16.2 MiB
[Stage 303:>                                                        (0 + 1) / 1]25/02/06 15:47:18 WARN DAGScheduler: Broadcasting large task binary with size 16.2 MiB
[Stage 304:=>                                                     (4 + 4) / 200][Stage 304:==>                                                    (8 + 4) / 200][Stage 304:===>                                                  (12 + 4) / 200][Stage 304:====>                                                 (16 + 4) / 200][Stage 304:=====>                                                (20 + 4) / 200][Stage 304:======>                                               (24 + 4) / 200][Stage 304:=======>                                              (28 + 4) / 200][Stage 304:========>                                             (31 + 5) / 200][Stage 304:=========>                                            (35 + 4) / 200][Stage 304:==========>                                           (40 + 4) / 200][Stage 304:===========>                                          (44 + 4) / 200][Stage 304:============>                                         (48 + 4) / 200][Stage 304:==============>                                       (52 + 4) / 200][Stage 304:===============>                                      (57 + 4) / 200][Stage 304:================>                                     (61 + 4) / 200][Stage 304:=================>                                    (65 + 4) / 200][Stage 304:==================>                                   (69 + 4) / 200][Stage 304:====================>                                 (75 + 4) / 200][Stage 304:=====================>                                (80 + 4) / 200][Stage 304:======================>                               (84 + 4) / 200][Stage 304:=======================>                              (88 + 4) / 200][Stage 304:========================>                             (91 + 5) / 200][Stage 304:=========================>                            (96 + 4) / 200][Stage 304:==========================>                           (97 + 4) / 200][Stage 304:==========================>                          (101 + 5) / 200][Stage 304:============================>                        (106 + 4) / 200][Stage 304:=============================>                       (110 + 4) / 200][Stage 304:=============================>                       (112 + 4) / 200][Stage 304:==============================>                      (116 + 4) / 200][Stage 304:===============================>                     (120 + 4) / 200][Stage 304:=================================>                   (125 + 4) / 200][Stage 304:==================================>                  (131 + 4) / 200][Stage 304:====================================>                (136 + 4) / 200][Stage 304:=====================================>               (140 + 4) / 200][Stage 304:======================================>              (144 + 4) / 200][Stage 304:=======================================>             (148 + 4) / 200][Stage 304:========================================>            (152 + 4) / 200][Stage 304:=========================================>           (156 + 4) / 200][Stage 304:==========================================>          (159 + 4) / 200][Stage 304:===========================================>         (164 + 4) / 200][Stage 304:============================================>        (168 + 4) / 200][Stage 304:=============================================>       (172 + 4) / 200][Stage 304:==============================================>      (176 + 4) / 200][Stage 304:==============================================>      (177 + 4) / 200][Stage 304:================================================>    (182 + 4) / 200][Stage 304:=================================================>   (186 + 4) / 200][Stage 304:==================================================>  (191 + 4) / 200][Stage 304:==================================================>  (192 + 4) / 200][Stage 304:===================================================> (196 + 4) / 200]                                                                                25/02/06 15:47:37 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 305:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:47:42 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:42 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:43 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:43 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:43 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:43 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:44 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:45 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:45 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:45 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:46 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:46 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:46 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:47 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:47:47 WARN DAGScheduler: Broadcasting large task binary with size 17.8 MiB
[Stage 337:>                                                        (0 + 1) / 1]25/02/06 15:47:50 WARN DAGScheduler: Broadcasting large task binary with size 17.8 MiB
[Stage 338:>                                                      (0 + 4) / 200][Stage 338:=>                                                     (4 + 4) / 200][Stage 338:==>                                                    (8 + 4) / 200][Stage 338:==>                                                   (10 + 4) / 200][Stage 338:===>                                                  (13 + 4) / 200][Stage 338:====>                                                 (18 + 4) / 200][Stage 338:=====>                                                (20 + 4) / 200][Stage 338:======>                                               (23 + 4) / 200][Stage 338:=======>                                              (27 + 4) / 200][Stage 338:========>                                             (31 + 4) / 200][Stage 338:=========>                                            (35 + 4) / 200][Stage 338:==========>                                           (39 + 4) / 200][Stage 338:===========>                                          (43 + 4) / 200][Stage 338:============>                                         (47 + 4) / 200][Stage 338:=============>                                        (51 + 4) / 200][Stage 338:==============>                                       (54 + 4) / 200][Stage 338:===============>                                      (57 + 4) / 200][Stage 338:================>                                     (61 + 4) / 200][Stage 338:=================>                                    (65 + 4) / 200][Stage 338:==================>                                   (69 + 4) / 200][Stage 338:===================>                                  (72 + 4) / 200][Stage 338:===================>                                  (73 + 4) / 200][Stage 338:====================>                                 (77 + 4) / 200][Stage 338:=====================>                                (80 + 4) / 200][Stage 338:======================>                               (83 + 4) / 200][Stage 338:=======================>                              (86 + 4) / 200][Stage 338:========================>                             (90 + 4) / 200][Stage 338:=========================>                            (93 + 4) / 200][Stage 338:=========================>                            (95 + 4) / 200][Stage 338:==========================>                           (99 + 4) / 200][Stage 338:===========================>                         (103 + 4) / 200][Stage 338:============================>                        (107 + 4) / 200][Stage 338:=============================>                       (110 + 4) / 200][Stage 338:==============================>                      (114 + 4) / 200][Stage 338:==============================>                      (115 + 4) / 200][Stage 338:===============================>                     (119 + 4) / 200][Stage 338:================================>                    (122 + 4) / 200][Stage 338:=================================>                   (125 + 4) / 200][Stage 338:=================================>                   (128 + 4) / 200][Stage 338:==================================>                  (132 + 4) / 200][Stage 338:====================================>                (137 + 4) / 200][Stage 338:=====================================>               (141 + 4) / 200][Stage 338:======================================>              (144 + 4) / 200][Stage 338:=======================================>             (148 + 4) / 200][Stage 338:========================================>            (152 + 4) / 200][Stage 338:=========================================>           (155 + 4) / 200][Stage 338:=========================================>           (157 + 4) / 200][Stage 338:==========================================>          (161 + 4) / 200][Stage 338:===========================================>         (164 + 4) / 200][Stage 338:============================================>        (168 + 4) / 200][Stage 338:=============================================>       (172 + 4) / 200][Stage 338:==============================================>      (176 + 4) / 200][Stage 338:==============================================>      (177 + 4) / 200][Stage 338:===============================================>     (178 + 4) / 200][Stage 338:================================================>    (183 + 4) / 200][Stage 338:=================================================>   (186 + 4) / 200][Stage 338:=================================================>   (188 + 4) / 200][Stage 338:==================================================>  (191 + 4) / 200][Stage 338:===================================================> (194 + 4) / 200][Stage 338:====================================================>(198 + 2) / 200]                                                                                25/02/06 15:48:13 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 339:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:48:18 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:18 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:19 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:19 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:19 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:20 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:21 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:21 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:21 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:22 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:22 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:23 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:23 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:23 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:24 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:24 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:24 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:25 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:25 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:25 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:26 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:26 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:27 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:27 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:28 WARN DAGScheduler: Broadcasting large task binary with size 19.4 MiB
[Stage 401:>                                                        (0 + 1) / 1]25/02/06 15:48:31 WARN DAGScheduler: Broadcasting large task binary with size 19.4 MiB
[Stage 402:=>                                                     (4 + 4) / 200][Stage 402:=>                                                     (7 + 4) / 200][Stage 402:==>                                                    (8 + 4) / 200][Stage 402:===>                                                  (13 + 4) / 200][Stage 402:===>                                                  (14 + 4) / 200][Stage 402:====>                                                 (17 + 4) / 200][Stage 402:=====>                                                (21 + 4) / 200][Stage 402:======>                                               (25 + 4) / 200][Stage 402:=======>                                              (27 + 4) / 200][Stage 402:=======>                                              (28 + 4) / 200][Stage 402:=======>                                              (29 + 4) / 200][Stage 402:========>                                             (33 + 4) / 200][Stage 402:=========>                                            (37 + 4) / 200][Stage 402:===========>                                          (42 + 4) / 200][Stage 402:===========>                                          (44 + 4) / 200][Stage 402:============>                                         (45 + 4) / 200][Stage 402:=============>                                        (49 + 4) / 200][Stage 402:==============>                                       (52 + 5) / 200][Stage 402:===============>                                      (57 + 4) / 200][Stage 402:===============>                                      (58 + 4) / 200][Stage 402:================>                                     (61 + 4) / 200][Stage 402:=================>                                    (65 + 4) / 200][Stage 402:==================>                                   (69 + 4) / 200][Stage 402:===================>                                  (73 + 4) / 200][Stage 402:=====================>                                (78 + 4) / 200][Stage 402:======================>                               (82 + 4) / 200][Stage 402:=======================>                              (86 + 4) / 200][Stage 402:========================>                             (90 + 4) / 200][Stage 402:=========================>                            (94 + 4) / 200][Stage 402:=========================>                            (95 + 4) / 200][Stage 402:==========================>                           (99 + 4) / 200][Stage 402:===========================>                         (103 + 4) / 200][Stage 402:============================>                        (107 + 4) / 200][Stage 402:=============================>                       (111 + 4) / 200][Stage 402:==============================>                      (115 + 4) / 200][Stage 402:===============================>                     (119 + 4) / 200][Stage 402:================================>                    (122 + 4) / 200][Stage 402:=================================>                   (127 + 4) / 200][Stage 402:==================================>                  (131 + 4) / 200][Stage 402:===================================>                 (135 + 4) / 200][Stage 402:====================================>                (139 + 4) / 200][Stage 402:======================================>              (144 + 4) / 200][Stage 402:=======================================>             (148 + 4) / 200][Stage 402:========================================>            (152 + 4) / 200][Stage 402:========================================>            (153 + 5) / 200][Stage 402:=========================================>           (158 + 4) / 200][Stage 402:===========================================>         (163 + 4) / 200][Stage 402:============================================>        (167 + 4) / 200][Stage 402:============================================>        (168 + 4) / 200][Stage 402:=============================================>       (172 + 4) / 200][Stage 402:==============================================>      (176 + 4) / 200][Stage 402:===============================================>     (180 + 4) / 200][Stage 402:================================================>    (184 + 4) / 200][Stage 402:=================================================>   (187 + 4) / 200][Stage 402:==================================================>  (192 + 4) / 200][Stage 402:===================================================> (195 + 4) / 200]                                                                                25/02/06 15:48:53 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 403:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:48:57 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:57 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:58 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:58 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:59 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:48:59 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:00 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:00 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:01 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:01 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:01 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:02 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:02 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:03 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:03 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:03 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:04 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:04 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:04 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:05 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:05 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:06 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:06 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:06 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:07 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:07 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:49:08 WARN DAGScheduler: Broadcasting large task binary with size 21.0 MiB
[Stage 471:>                                                        (0 + 1) / 1]25/02/06 15:49:11 WARN DAGScheduler: Broadcasting large task binary with size 21.0 MiB
[Stage 472:=>                                                     (4 + 4) / 200][Stage 472:==>                                                    (8 + 4) / 200][Stage 472:===>                                                  (12 + 4) / 200][Stage 472:====>                                                 (16 + 4) / 200][Stage 472:====>                                                 (18 + 4) / 200][Stage 472:=====>                                                (19 + 4) / 200][Stage 472:=====>                                                (20 + 4) / 200][Stage 472:======>                                               (24 + 4) / 200][Stage 472:=======>                                              (28 + 4) / 200][Stage 472:========>                                             (32 + 4) / 200][Stage 472:=========>                                            (36 + 4) / 200][Stage 472:==========>                                           (39 + 4) / 200][Stage 472:===========>                                          (41 + 4) / 200][Stage 472:============>                                         (45 + 4) / 200][Stage 472:=============>                                        (49 + 4) / 200][Stage 472:==============>                                       (52 + 4) / 200][Stage 472:===============>                                      (56 + 5) / 200][Stage 472:================>                                     (60 + 4) / 200][Stage 472:=================>                                    (64 + 4) / 200][Stage 472:==================>                                   (68 + 4) / 200][Stage 472:==================>                                   (69 + 4) / 200][Stage 472:===================>                                  (73 + 4) / 200][Stage 472:====================>                                 (77 + 4) / 200][Stage 472:=====================>                                (79 + 4) / 200][Stage 472:======================>                               (83 + 4) / 200][Stage 472:======================>                               (85 + 4) / 200][Stage 472:========================>                             (89 + 4) / 200][Stage 472:=========================>                            (93 + 4) / 200][Stage 472:==========================>                           (97 + 4) / 200][Stage 472:==========================>                          (100 + 4) / 200][Stage 472:===========================>                         (104 + 4) / 200][Stage 472:============================>                        (109 + 4) / 200][Stage 472:=============================>                       (111 + 4) / 200][Stage 472:==============================>                      (114 + 4) / 200][Stage 472:===============================>                     (118 + 4) / 200][Stage 472:================================>                    (121 + 4) / 200][Stage 472:=================================>                   (125 + 4) / 200][Stage 472:==================================>                  (129 + 4) / 200][Stage 472:===================================>                 (133 + 4) / 200][Stage 472:====================================>                (137 + 4) / 200][Stage 472:=====================================>               (141 + 4) / 200][Stage 472:======================================>              (145 + 4) / 200][Stage 472:=======================================>             (148 + 4) / 200][Stage 472:========================================>            (152 + 4) / 200][Stage 472:=========================================>           (156 + 4) / 200][Stage 472:=========================================>           (157 + 4) / 200][Stage 472:==========================================>          (161 + 4) / 200][Stage 472:===========================================>         (165 + 4) / 200][Stage 472:============================================>        (169 + 4) / 200][Stage 472:=============================================>       (173 + 4) / 200][Stage 472:==============================================>      (177 + 4) / 200][Stage 472:===============================================>     (181 + 4) / 200][Stage 472:=================================================>   (185 + 4) / 200][Stage 472:==================================================>  (189 + 4) / 200][Stage 472:===================================================> (193 + 4) / 200][Stage 472:===================================================> (196 + 4) / 200]                                                                                [I 2025-02-06 15:49:56.087 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[Stage 473:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:50:26 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB
[Stage 477:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:50:32 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:50:36 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 479:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:50:41 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:50:41 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:50:41 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:50:41 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:50:42 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:50:42 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:50:42 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:50:43 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:50:43 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 15:50:43 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 493:>                                                        (0 + 1) / 1]25/02/06 15:50:46 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 494:>                                                      (0 + 4) / 200][Stage 494:==>                                                    (8 + 4) / 200][Stage 494:==>                                                   (10 + 5) / 200][Stage 494:====>                                                 (15 + 4) / 200][Stage 494:=====>                                                (19 + 4) / 200][Stage 494:======>                                               (23 + 4) / 200][Stage 494:=======>                                              (27 + 4) / 200][Stage 494:========>                                             (31 + 4) / 200][Stage 494:=========>                                            (35 + 4) / 200][Stage 494:==========>                                           (39 + 4) / 200][Stage 494:===========>                                          (44 + 4) / 200][Stage 494:============>                                         (48 + 4) / 200][Stage 494:==============>                                       (53 + 4) / 200][Stage 494:===============>                                      (58 + 4) / 200][Stage 494:=================>                                    (63 + 4) / 200][Stage 494:==================>                                   (67 + 4) / 200][Stage 494:===================>                                  (71 + 4) / 200][Stage 494:====================>                                 (76 + 4) / 200][Stage 494:=====================>                                (78 + 4) / 200][Stage 494:======================>                               (82 + 4) / 200][Stage 494:=======================>                              (86 + 4) / 200][Stage 494:========================>                             (91 + 4) / 200][Stage 494:=========================>                            (95 + 4) / 200][Stage 494:==========================>                           (98 + 4) / 200][Stage 494:===========================>                         (102 + 4) / 200][Stage 494:============================>                        (106 + 4) / 200][Stage 494:=============================>                       (112 + 4) / 200][Stage 494:===============================>                     (117 + 4) / 200][Stage 494:================================>                    (121 + 4) / 200][Stage 494:=================================>                   (125 + 4) / 200][Stage 494:==================================>                  (129 + 4) / 200][Stage 494:====================================>                (136 + 4) / 200][Stage 494:=====================================>               (141 + 4) / 200][Stage 494:======================================>              (145 + 4) / 200][Stage 494:=======================================>             (150 + 4) / 200][Stage 494:=========================================>           (156 + 4) / 200][Stage 494:==========================================>          (161 + 4) / 200][Stage 494:===========================================>         (165 + 4) / 200][Stage 494:============================================>        (169 + 4) / 200][Stage 494:==============================================>      (176 + 5) / 200][Stage 494:===============================================>     (181 + 4) / 200][Stage 494:=================================================>   (185 + 4) / 200][Stage 494:==================================================>  (189 + 4) / 200][Stage 494:===================================================> (193 + 4) / 200][Stage 494:====================================================>(197 + 3) / 200]                                                                                25/02/06 15:51:02 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
25/02/06 15:51:05 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 496:>                                                        (0 + 1) / 1]25/02/06 15:51:08 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
                                                                                25/02/06 15:51:08 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
25/02/06 15:51:09 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 501:===========>                                            (4 + 4) / 20][Stage 501:======================>                                 (8 + 4) / 20][Stage 501:=================================>                     (12 + 4) / 20][Stage 501:=========================================>             (15 + 4) / 20]                                                                                25/02/06 15:51:11 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 503:>                                                      (0 + 4) / 100][Stage 503:==>                                                    (5 + 4) / 100][Stage 503:=====>                                                (10 + 4) / 100][Stage 503:========>                                             (15 + 4) / 100][Stage 503:==========>                                           (19 + 4) / 100][Stage 503:============>                                         (24 + 4) / 100][Stage 503:===============>                                      (28 + 4) / 100][Stage 503:=================>                                    (32 + 4) / 100][Stage 503:===================>                                  (36 + 4) / 100][Stage 503:=====================>                                (40 + 4) / 100][Stage 503:=========================>                            (47 + 4) / 100][Stage 503:===========================>                          (50 + 4) / 100][Stage 503:==============================>                       (56 + 4) / 100][Stage 503:===============================>                      (58 + 4) / 100][Stage 503:================================>                     (61 + 4) / 100][Stage 503:==================================>                   (64 + 4) / 100][Stage 503:====================================>                 (68 + 4) / 100][Stage 503:=======================================>              (74 + 4) / 100][Stage 503:===========================================>          (80 + 4) / 100][Stage 503:============================================>         (83 + 4) / 100][Stage 503:================================================>     (89 + 4) / 100][Stage 503:==================================================>   (93 + 4) / 100]                                                                                25/02/06 15:51:18 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 505:=====>                                                  (8 + 4) / 75][Stage 505:=========>                                             (13 + 4) / 75][Stage 505:=============>                                         (18 + 4) / 75][Stage 505:=================>                                     (24 + 4) / 75][Stage 505:====================>                                  (28 + 4) / 75][Stage 505:=========================>                             (35 + 4) / 75][Stage 505:============================>                          (39 + 4) / 75][Stage 505:===============================>                       (43 + 4) / 75][Stage 505:==================================>                    (47 + 4) / 75][Stage 505:======================================>                (53 + 4) / 75][Stage 505:==========================================>            (58 + 4) / 75][Stage 505:==============================================>        (63 + 4) / 75][Stage 505:=================================================>     (68 + 4) / 75][Stage 505:====================================================>  (72 + 3) / 75]                                                                                25/02/06 15:51:23 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 506:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:51:24 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 507:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:51:27 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 508:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:51:46 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
25/02/06 15:51:53 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 510:>                                                        (0 + 1) / 1]                                                                                [I 2025-02-06 15:51:56.235 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
25/02/06 15:52:22 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
25/02/06 15:52:29 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 512:>                                                        (0 + 1) / 1]                                                                                25/02/06 15:52:43 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB
[Stage 513:>                                                        (0 + 1) / 1]                                                                                [I 2025-02-06 15:53:56.380 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[I 2025-02-06 15:55:56.534 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[I 2025-02-06 16:01:10.789 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[I 2025-02-06 16:01:21.061 ServerApp] Creating new notebook in /YJ
[I 2025-02-06 16:01:21.255 ServerApp] Saving file at /YJ/Untitled3.ipynb
[I 2025-02-06 16:01:21.283 ServerApp] Kernel started: a9205a99-d47d-4d7f-b954-eb91ebd81499
[I 2025-02-06 16:01:21.761 ServerApp] Connecting to kernel a9205a99-d47d-4d7f-b954-eb91ebd81499.
[I 2025-02-06 16:01:22.693 ServerApp] Connecting to kernel 54599486-28f9-4652-bdb2-a979ce71c285.
[I 2025-02-06 16:01:22.794 ServerApp] Connecting to kernel 077eb05d-22c8-4133-ab21-92b4631a5e18.
[I 2025-02-06 16:01:22.860 ServerApp] Connecting to kernel a9205a99-d47d-4d7f-b954-eb91ebd81499.
[I 2025-02-06 16:01:23.049 ServerApp] Connecting to kernel a9205a99-d47d-4d7f-b954-eb91ebd81499.
[I 2025-02-06 16:03:22.908 ServerApp] Saving file at /YJ/Untitled3.ipynb
[I 2025-02-06 16:05:23.618 ServerApp] Saving file at /YJ/Untitled3.ipynb
[I 2025-02-06 16:27:53.636 ServerApp] Kernel started: 40f07ade-5d1a-4a52-b472-4bd4bbd6abaa
[I 2025-02-06 16:27:53.637 ServerApp] Kernel shutdown: 077eb05d-22c8-4133-ab21-92b4631a5e18
[I 2025-02-06 16:27:55.466 ServerApp] Starting buffering for 077eb05d-22c8-4133-ab21-92b4631a5e18:5cd12b90-1af4-4e2c-a34e-269db9eea263
[I 2025-02-06 16:27:55.504 ServerApp] Connecting to kernel 40f07ade-5d1a-4a52-b472-4bd4bbd6abaa.
[I 2025-02-06 16:29:11.675 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
25/02/06 16:30:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 3:>                                                          (0 + 1) / 1]                                                                                25/02/06 16:30:51 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB
[Stage 4:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-06 16:31:11.855 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
25/02/06 16:31:25 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 5:>                                                          (0 + 1) / 1]25/02/06 16:31:41 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 4)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 473, in main
    raise Exception(("Python in worker has different version %s than that in " +
Exception: Python in worker has different version 3.8 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
25/02/06 16:31:41 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 4) (ip-172-31-13-81.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 473, in main
    raise Exception(("Python in worker has different version %s than that in " +
Exception: Python in worker has different version 3.8 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/02/06 16:31:41 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job
25/02/06 16:32:18 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 16:32:18 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 5)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 473, in main
    raise Exception(("Python in worker has different version %s than that in " +
Exception: Python in worker has different version 3.8 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
25/02/06 16:32:18 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 5) (ip-172-31-13-81.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 473, in main
    raise Exception(("Python in worker has different version %s than that in " +
Exception: Python in worker has different version 3.8 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/02/06 16:32:18 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job
25/02/06 16:32:36 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
25/02/06 16:32:36 ERROR Executor: Exception in task 0.0 in stage 7.0 (TID 6)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 473, in main
    raise Exception(("Python in worker has different version %s than that in " +
Exception: Python in worker has different version 3.8 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
25/02/06 16:32:36 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 6) (ip-172-31-13-81.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 473, in main
    raise Exception(("Python in worker has different version %s than that in " +
Exception: Python in worker has different version 3.8 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/02/06 16:32:36 ERROR TaskSetManager: Task 0 in stage 7.0 failed 1 times; aborting job
[I 2025-02-06 16:33:12.056 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
25/02/06 16:33:47 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-51ac7da5-205f-4497-9a5e-21e936ff9b72. Falling back to Java IO way
java.io.IOException: Failed to delete: /tmp/spark-51ac7da5-205f-4497-9a5e-21e936ff9b72
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1141)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
25/02/06 16:33:47 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-51ac7da5-205f-4497-9a5e-21e936ff9b72/pyspark-555718cc-11a5-4102-b8ee-1546fffe11d2. Falling back to Java IO way
java.io.IOException: Failed to delete: /tmp/spark-51ac7da5-205f-4497-9a5e-21e936ff9b72/pyspark-555718cc-11a5-4102-b8ee-1546fffe11d2
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1141)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[I 2025-02-06 16:33:50.049 ServerApp] Kernel restarted: 40f07ade-5d1a-4a52-b472-4bd4bbd6abaa
[I 2025-02-06 16:33:50.076 ServerApp] Starting buffering for 40f07ade-5d1a-4a52-b472-4bd4bbd6abaa:5cd12b90-1af4-4e2c-a34e-269db9eea263
[I 2025-02-06 16:33:50.111 ServerApp] Connecting to kernel 40f07ade-5d1a-4a52-b472-4bd4bbd6abaa.
[I 2025-02-06 16:33:50.111 ServerApp] Restoring connection for 40f07ade-5d1a-4a52-b472-4bd4bbd6abaa:5cd12b90-1af4-4e2c-a34e-269db9eea263
[I 2025-02-06 16:33:57.167 ServerApp] Kernel started: ad968cad-5d07-4b93-8567-fd574ee12f8f
[I 2025-02-06 16:33:57.168 ServerApp] Kernel shutdown: 40f07ade-5d1a-4a52-b472-4bd4bbd6abaa
[I 2025-02-06 16:33:57.632 ServerApp] Connecting to kernel ad968cad-5d07-4b93-8567-fd574ee12f8f.
[I 2025-02-06 16:34:08.653 ServerApp] Kernel started: 5099063b-fc86-4c42-865b-2a19dacce825
[I 2025-02-06 16:34:08.654 ServerApp] Kernel shutdown: ad968cad-5d07-4b93-8567-fd574ee12f8f
[I 2025-02-06 16:34:09.080 ServerApp] Connecting to kernel 5099063b-fc86-4c42-865b-2a19dacce825.
[I 2025-02-06 16:34:31.758 ServerApp] Kernel started: b8a5d084-ae63-42e0-830a-31b265730581
[I 2025-02-06 16:34:31.759 ServerApp] Kernel shutdown: 5099063b-fc86-4c42-865b-2a19dacce825
[I 2025-02-06 16:34:32.194 ServerApp] Connecting to kernel b8a5d084-ae63-42e0-830a-31b265730581.
[I 2025-02-06 16:35:12.194 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[I 2025-02-06 16:36:28.915 ServerApp] Kernel restarted: b8a5d084-ae63-42e0-830a-31b265730581
[I 2025-02-06 16:36:28.949 ServerApp] Starting buffering for b8a5d084-ae63-42e0-830a-31b265730581:5cd12b90-1af4-4e2c-a34e-269db9eea263
[I 2025-02-06 16:36:28.983 ServerApp] Connecting to kernel b8a5d084-ae63-42e0-830a-31b265730581.
[I 2025-02-06 16:36:28.983 ServerApp] Restoring connection for b8a5d084-ae63-42e0-830a-31b265730581:5cd12b90-1af4-4e2c-a34e-269db9eea263
25/02/06 16:36:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 1) / 1][Stage 1:>                                                          (0 + 1) / 1]                                                                                25/02/06 16:36:50 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB
[Stage 4:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-06 16:37:12.355 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
25/02/06 16:37:24 WARN DAGScheduler: Broadcasting large task binary with size 6.6 MiB
[Stage 5:>                                                          (0 + 1) / 1][Stage 5:>                                                          (0 + 1) / 1][I 2025-02-06 16:39:12.518 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[Stage 5:>                                                          (0 + 1) / 1][Stage 5:>                                                          (0 + 1) / 1][Stage 5:>                                                          (0 + 1) / 1][Stage 5:>                                                          (0 + 1) / 1][Stage 5:>                                                          (0 + 1) / 1][Stage 5:>                                                          (0 + 1) / 1][Stage 5:>                                                          (0 + 1) / 1][Stage 5:>                                                          (0 + 1) / 1]                                                                                [I 2025-02-06 16:47:12.716 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[Stage 6:>                                                          (0 + 1) / 1][I 2025-02-06 16:49:12.901 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][Stage 6:>                                                          (0 + 1) / 1][I 2025-02-06 17:02:30.901 ServerApp] Saving file at /YJ/analytics/read_mysql.ipynb
[Stage 6:>                                                          (0 + 1) / 1]