{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74490cfa-a70a-4be8-9360-0a0a1e7230fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU 개수: 1\n",
      "GPU 이름: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU 개수:\", torch.cuda.device_count())\n",
    "print(\"GPU 이름:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d00e68-c1cf-4bb9-a36e-3dca9bf6f119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4353\n",
      "Epoch 2, Loss: 1.4196\n",
      "Epoch 3, Loss: 1.3952\n",
      "Epoch 4, Loss: 1.3788\n",
      "Epoch 5, Loss: 1.3646\n",
      "Epoch 6, Loss: 1.3416\n",
      "Epoch 7, Loss: 1.3175\n",
      "Epoch 8, Loss: 1.2907\n",
      "Epoch 9, Loss: 1.2570\n",
      "Epoch 10, Loss: 1.2168\n",
      "리뷰: 이 제품 사용감이 별로네요 -> 부정\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 샘플 데이터 (긍정 1, 부정 0)\n",
    "data = [\n",
    "    (\"이 제품 사용감이 별로네요\", 0),\n",
    "    (\"정말 좋아요! 만족합니다.\", 1),\n",
    "    (\"별로예요. 다시는 안 살 듯\", 0),\n",
    "    (\"매우 편리하고 좋아요!\", 1),\n",
    "    (\"퀄리티가 너무 낮아요.\", 0)\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['review', 'label'])\n",
    "\n",
    "# 형태소 분석 및 토큰화\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize(text):\n",
    "    return okt.morphs(text, stem=True)  # 원형 복원\n",
    "\n",
    "df['tokenized'] = df['review'].apply(tokenize)\n",
    "\n",
    "# 단어 사전 생성\n",
    "vocab = {word: idx + 2 for idx, word in enumerate(set(sum(df['tokenized'].tolist(), [])))}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "\n",
    "# 텍스트를 인덱스로 변환\n",
    "def encode_text(tokenized_sentence):\n",
    "    return [vocab.get(word, 1) for word in tokenized_sentence]\n",
    "\n",
    "df['encoded'] = df['tokenized'].apply(encode_text)\n",
    "\n",
    "# 패딩 적용 (최대 길이 10)\n",
    "MAX_LEN = 10\n",
    "\n",
    "def pad_sequence(seq):\n",
    "    return seq[:MAX_LEN] + [0] * (MAX_LEN - len(seq))\n",
    "\n",
    "df['padded'] = df['encoded'].apply(pad_sequence)\n",
    "\n",
    "# 데이터 분할\n",
    "X = torch.tensor(df['padded'].tolist(), dtype=torch.long)\n",
    "y = torch.tensor(df['label'].tolist(), dtype=torch.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32, hidden_dim=64, output_dim=1):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        x = self.fc(hidden[-1])\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# 모델 초기화\n",
    "vocab_size = len(vocab)\n",
    "model = SentimentLSTM(vocab_size).cuda()\n",
    "\n",
    "# 손실 함수 & 옵티마이저\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 10\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = tokenize(sentence)\n",
    "    encoded = encode_text(tokenized)\n",
    "    padded = pad_sequence(encoded)\n",
    "    input_tensor = torch.tensor([padded], dtype=torch.long).cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor).item()\n",
    "    \n",
    "    return \"긍정\" if output > 0.5 else \"부정\"\n",
    "\n",
    "# 예제 리뷰 평가\n",
    "test_sentence = \"이 제품 사용감이 별로네요\"\n",
    "print(f\"리뷰: {test_sentence} -> {predict_sentiment(model, test_sentence)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9e58c9e-6536-4c7d-b321-b55e3c907dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰: 이 제품 사용감이 별로네요 -> 클러스터 0\n",
      "리뷰: 정말 좋아요! 만족합니다. -> 클러스터 2\n",
      "리뷰: 별로예요. 다시는 안 살 듯 -> 클러스터 0\n",
      "리뷰: 매우 편리하고 좋아요! -> 클러스터 1\n",
      "리뷰: 퀄리티가 너무 낮아요. -> 클러스터 0\n",
      "리뷰: 배송이 빠르고 좋아요! -> 클러스터 1\n",
      "리뷰: 디자인이 멋있고 성능이 뛰어나요! -> 클러스터 0\n",
      "리뷰: 가성비가 별로인 것 같아요. -> 클러스터 0\n",
      "토픽 0:  ['좋다', '별로', '정말', '만족하다', '배송', '빠르다', '가성', '같다', '다시다', '예요']\n",
      "토픽 1:  ['성능', '뛰어나다', '디자인', '멋있다', '퀄리티', '너무', '낮다', '매우', '편리하다', '좋다']\n",
      "토픽 2:  ['별로', '용감', '제품', '좋다', '예요', '다시다', '가성', '같다', '배송', '빠르다']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 샘플 데이터 (비지도 학습이므로 라벨 없음)\n",
    "reviews = [\n",
    "    \"이 제품 사용감이 별로네요\",\n",
    "    \"정말 좋아요! 만족합니다.\",\n",
    "    \"별로예요. 다시는 안 살 듯\",\n",
    "    \"매우 편리하고 좋아요!\",\n",
    "    \"퀄리티가 너무 낮아요.\",\n",
    "    \"배송이 빠르고 좋아요!\",\n",
    "    \"디자인이 멋있고 성능이 뛰어나요!\",\n",
    "    \"가성비가 별로인 것 같아요.\"\n",
    "]\n",
    "\n",
    "# 형태소 분석 및 전처리\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize(text):\n",
    "    return \" \".join(okt.morphs(text, stem=True))\n",
    "\n",
    "processed_reviews = [tokenize(review) for review in reviews]\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(processed_reviews)\n",
    "\n",
    "# K-Means 클러스터링 (k=2로 긍/부정 예상)\n",
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# 클러스터 할당 결과 출력\n",
    "for i, review in enumerate(reviews):\n",
    "    print(f\"리뷰: {review} -> 클러스터 {kmeans.labels_[i]}\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# LDA를 위한 벡터화\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(processed_reviews)\n",
    "\n",
    "# LDA 모델 (2개의 주요 토픽 찾기)\n",
    "lda = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# 주요 단어 출력\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"토픽 {topic_idx}: \", [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49b8e2-454d-4724-9c08-cd8615e74569",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://wonhwa.tistory.com/35"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p39",
   "language": "python",
   "name": "pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
