{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c4db42-2464-424b-a0d6-bed118f00ded",
   "metadata": {},
   "outputs": [
    {
     "ename": "SessionNotCreatedException",
     "evalue": "Message: session not created: DevToolsActivePort file doesn't exist\nStacktrace:\n#0 0x557d508aa53a <unknown>\n#1 0x557d503a5f00 <unknown>\n#2 0x557d503e2b24 <unknown>\n#3 0x557d503dd589 <unknown>\n#4 0x557d503d94b6 <unknown>\n#5 0x557d504267b6 <unknown>\n#6 0x557d50425e06 <unknown>\n#7 0x557d5041a343 <unknown>\n#8 0x557d503e778a <unknown>\n#9 0x557d503e89de <unknown>\n#10 0x557d508742cb <unknown>\n#11 0x557d50878242 <unknown>\n#12 0x557d508617ac <unknown>\n#13 0x557d50878df7 <unknown>\n#14 0x557d50845b2f <unknown>\n#15 0x557d508991a8 <unknown>\n#16 0x557d50899370 <unknown>\n#17 0x557d508a93b6 <unknown>\n#18 0x7f612e4426db start_thread\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSessionNotCreatedException\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m chrome_option \u001b[38;5;241m=\u001b[39m Options()\n\u001b[1;32m     22\u001b[0m chrome_option\u001b[38;5;241m.\u001b[39madd_experimental_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetach\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 23\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchrome_option\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m wait \u001b[38;5;241m=\u001b[39m WebDriverWait(driver,\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     26\u001b[0m urls \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS24\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://search.danawa.com/dsearch.php?query=\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m%82%\u001b[39;00m\u001b[38;5;124mBC\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m%84%\u001b[39;00m\u001b[38;5;124mB1\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA0\u001b[39m\u001b[38;5;132;01m%84%\u001b[39;00m\u001b[38;5;124mEC\u001b[39m\u001b[38;5;132;01m%9E\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m90+\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mB\u001b[39m\u001b[38;5;132;01m%9F\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mAD\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m8B\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m9Cs24+256gb\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2C+\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m%9E\u001b[39;00m\u001b[38;5;132;01m%90%\u001b[39;00m\u001b[38;5;124mEA\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB8\u001b[39m\u001b[38;5;132;01m%89%\u001b[39;00m\u001b[38;5;124mEC\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m9C&originalQuery=\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m%82%\u001b[39;00m\u001b[38;5;124mBC\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m%84%\u001b[39;00m\u001b[38;5;124mB1\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA0\u001b[39m\u001b[38;5;132;01m%84%\u001b[39;00m\u001b[38;5;124mEC\u001b[39m\u001b[38;5;132;01m%9E\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m90+\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mB\u001b[39m\u001b[38;5;132;01m%9F\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mAD\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m8B\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m9Cs24+256gb\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2C+\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m%9E\u001b[39;00m\u001b[38;5;132;01m%90%\u001b[39;00m\u001b[38;5;124mEA\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB8\u001b[39m\u001b[38;5;132;01m%89%\u001b[39;00m\u001b[38;5;124mEC\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m9C&checkedInfo=N&volumeType=allvs&page=1&limit=40&sort=opinionDESC&list=list&boost=true&tab=main&addDelivery=N&coupangMemberSort=N&simpleDescOpen=Y&mode=simple&isInitTireSmartFinder=N&recommendedSort=N&defaultUICategoryCode=122515&defaultPhysicsCategoryCode=224\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m7C48419\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m7C48829\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m7C0&defaultVmTab=8&defaultVaTab=2041&isZeroPrice=Y&quickProductYN=N&priceUnitSort=N&priceUnitSortOrder=A\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m        , [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m아이폰16\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://search.danawa.com/dsearch.php?query=\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m%95%\u001b[39;00m\u001b[38;5;124m84\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m9D\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB4\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mD\u001b[39m\u001b[38;5;132;01m%8F\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB016+\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m%9E\u001b[39;00m\u001b[38;5;132;01m%90%\u001b[39;00m\u001b[38;5;124mEA\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB8\u001b[39m\u001b[38;5;132;01m%89%\u001b[39;00m\u001b[38;5;124mEC\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m9C\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     28\u001b[0m        , [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m플립6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://prod.danawa.com/list/?cate=12345013&searchOption=/searchAttributeValue=991897&shortcutKeyword=\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mB\u001b[39m\u001b[38;5;132;01m%9F\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mAD\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m8B\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m9CZ\u001b[39m\u001b[38;5;132;01m%20%\u001b[39;00m\u001b[38;5;124mED\u001b[39m\u001b[38;5;132;01m%94%\u001b[39;00m\u001b[38;5;124m8C\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA6\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBD6\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m        , [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m폴드6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://search.danawa.com/dsearch.php?query=\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mB\u001b[39m\u001b[38;5;132;01m%9F\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mAD\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m8B\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m9Cz+\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mD\u001b[39m\u001b[38;5;132;01m%8F\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB4\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124mB\u001b[39m\u001b[38;5;132;01m%93%\u001b[39;00m\u001b[38;5;124m9C6\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/crawling/lib/python3.11/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[1;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/crawling/lib/python3.11/site-packages/selenium/webdriver/chromium/webdriver.py:66\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     57\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[1;32m     58\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[1;32m     59\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquit()\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/crawling/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:238\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, command_executor, keep_alive, file_detector, options, locator_converter, web_element_cls, client_config)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authenticator_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_client()\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcapabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_websocket_connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_script \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/crawling/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:325\u001b[0m, in \u001b[0;36mWebDriver.start_session\u001b[0;34m(self, capabilities)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new session with the desired capabilities.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m:Args:\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m - capabilities - a capabilities dict to start the session with.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    324\u001b[0m caps \u001b[38;5;241m=\u001b[39m _create_caps(capabilities)\n\u001b[0;32m--> 325\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEW_SESSION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaps\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaps \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/crawling/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:380\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    378\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/crawling/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mSessionNotCreatedException\u001b[0m: Message: session not created: DevToolsActivePort file doesn't exist\nStacktrace:\n#0 0x557d508aa53a <unknown>\n#1 0x557d503a5f00 <unknown>\n#2 0x557d503e2b24 <unknown>\n#3 0x557d503dd589 <unknown>\n#4 0x557d503d94b6 <unknown>\n#5 0x557d504267b6 <unknown>\n#6 0x557d50425e06 <unknown>\n#7 0x557d5041a343 <unknown>\n#8 0x557d503e778a <unknown>\n#9 0x557d503e89de <unknown>\n#10 0x557d508742cb <unknown>\n#11 0x557d50878242 <unknown>\n#12 0x557d508617ac <unknown>\n#13 0x557d50878df7 <unknown>\n#14 0x557d50845b2f <unknown>\n#15 0x557d508991a8 <unknown>\n#16 0x557d50899370 <unknown>\n#17 0x557d508a93b6 <unknown>\n#18 0x7f612e4426db start_thread\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "import time\n",
    "\n",
    "chrome_option = Options()\n",
    "chrome_option.add_experimental_option('detach', True)\n",
    "driver = webdriver.Chrome(options=chrome_option)\n",
    "wait = WebDriverWait(driver,20)\n",
    "\n",
    "urls = [['S24', 'https://search.danawa.com/dsearch.php?query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90+%EA%B0%A4%EB%9F%AD%EC%8B%9Cs24+256gb%2C+%EC%9E%90%EA%B8%89%EC%A0%9C&originalQuery=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90+%EA%B0%A4%EB%9F%AD%EC%8B%9Cs24+256gb%2C+%EC%9E%90%EA%B8%89%EC%A0%9C&checkedInfo=N&volumeType=allvs&page=1&limit=40&sort=opinionDESC&list=list&boost=true&tab=main&addDelivery=N&coupangMemberSort=N&simpleDescOpen=Y&mode=simple&isInitTireSmartFinder=N&recommendedSort=N&defaultUICategoryCode=122515&defaultPhysicsCategoryCode=224%7C48419%7C48829%7C0&defaultVmTab=8&defaultVaTab=2041&isZeroPrice=Y&quickProductYN=N&priceUnitSort=N&priceUnitSortOrder=A']\n",
    "       , ['아이폰16', 'https://search.danawa.com/dsearch.php?query=%EC%95%84%EC%9D%B4%ED%8F%B016+%EC%9E%90%EA%B8%89%EC%A0%9C']\n",
    "       , ['플립6', 'https://prod.danawa.com/list/?cate=12345013&searchOption=/searchAttributeValue=991897&shortcutKeyword=%EA%B0%A4%EB%9F%AD%EC%8B%9CZ%20%ED%94%8C%EB%A6%BD6']\n",
    "       , ['폴드6', 'https://search.danawa.com/dsearch.php?query=%EA%B0%A4%EB%9F%AD%EC%8B%9Cz+%ED%8F%B4%EB%93%9C6']]\n",
    "\n",
    "target_item = [urls[0][0], urls[1][0]]\n",
    "\n",
    "def save_to_df(tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content):\n",
    "    global df\n",
    "    tmp_list = []\n",
    "    for s, m, d, t, c in zip(tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content):\n",
    "        tmp_list.append([s, m, d, t, c])\n",
    "        \n",
    "    df = pd.concat([df, pd.DataFrame(data = tmp_list, columns = ['scoring', 'market', 'purchasing_date', 'review_title', 'review_content'])])\n",
    "\n",
    "    print('df에 저장완료!')\n",
    "    return df\n",
    "\n",
    "def click_link(link, idx):\n",
    "    idx = int(idx)\n",
    "    global target_item\n",
    "    driver.get(link)\n",
    "    tmp = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    tmp_title = tmp.select('#blog_content > div.summary_info > div.top_summary > h3 > span')[0]\n",
    "    if target_item[idx] in tmp_title.text:\n",
    "        return tmp_title.text, 1\n",
    "    \n",
    "    else:\n",
    "        print('hmm this is error')\n",
    "        return tmp_title.text, 0\n",
    "\n",
    "\n",
    "\n",
    "# 페이지 리스트 계산\n",
    "def calc_page_list(review_soup):\n",
    "    global wait\n",
    "    page_list = []\n",
    "    wait.until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, '#danawa-prodBlog-companyReview-content-list > div > div > div span'))\n",
    "    )\n",
    "    \n",
    "    page_list.append(review_soup.select('#danawa-prodBlog-companyReview-content-list > div > div > div span')[0].text)\n",
    "    for i in review_soup.select('#danawa-prodBlog-companyReview-content-list > div > div > div a'):\n",
    "        page_list.append(i['data-pagenumber'])\n",
    "    return page_list\n",
    "\n",
    "\n",
    "# 다음 버튼 유무\n",
    "def is_click_next_button(page_list) :\n",
    "    if int(page_list[0]) + 9 == int(page_list[-1]):\n",
    "        return 1\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# 해당 페이지 크롤링\n",
    "def crawling(driver):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    review_soup = soup.select('#danawa-prodBlog-productOpinion-list-self > div.mall_review > div.area_right')\n",
    "    scoring = [i.text for i in review_soup[0].select('#danawa-prodBlog-companyReview-content-list div.top_info span.point_type_s span')]\n",
    "    market = [i['alt'] for i in review_soup[0].select('#danawa-prodBlog-companyReview-content-list div.top_info span.mall img')]\n",
    "    purchasing_date = [i.text for i in review_soup[0].select('#danawa-prodBlog-companyReview-content-list span.date')]\n",
    "    review_title = [i.text for i in review_soup[0].select('[id^=\"danawa-prodBlog-companyReview-content-wrap-\"] > div.atc_cont > div.tit_W')]\n",
    "    review_content = [i.text for i in review_soup[0].select('div.atc')]\n",
    "\n",
    "    return scoring, market, purchasing_date, review_title, review_content\n",
    "\n",
    "\n",
    "def repit_page(isTarget): \n",
    "\n",
    "    if isTarget == 1:\n",
    "        try :\n",
    "            while True:\n",
    "                time.sleep(2)\n",
    "                wait.until(\n",
    "                            EC.presence_of_element_located((By.CSS_SELECTOR, '#danawa-prodBlog-productOpinion-list-self > div.mall_review > div.area_right'))\n",
    "                )\n",
    "                review_soup = BeautifulSoup(driver.page_source, 'html.parser').select('#danawa-prodBlog-productOpinion-list-self > div.mall_review > div.area_right')[0]\n",
    "                \n",
    "                page_list = calc_page_list(review_soup)\n",
    "                print(page_list)\n",
    "                if len(page_list) == 1:\n",
    "                    print(f'{page_list[0]}페이지 시작합니다~~')\n",
    "                    tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content = crawling(driver)\n",
    "                    save_to_df(tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content)\n",
    "                    break\n",
    "                else:\n",
    "                    for i in page_list:\n",
    "                        print(f'{i}페이지 시작합니다.')\n",
    "                        i = int(i)\n",
    "                        if i % 10 == 1:\n",
    "                            tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content = crawling(driver)\n",
    "                            save_to_df(tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content)\n",
    "                            #df_to_file(df)\n",
    "\n",
    "                        elif i % 10 >= 2:\n",
    "                            \n",
    "                            wait.until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, f'a[data-pagenumber=\"{i}\"]'))\n",
    "                            )\n",
    "                            time.sleep(1)\n",
    "                            driver.find_element(By.CSS_SELECTOR, f'a[data-pagenumber=\"{i}\"]').click()\n",
    "                            tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content = crawling(driver)\n",
    "                            save_to_df(tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content)\n",
    "                            #df_to_file(df)\n",
    "\n",
    "                        elif i % 10 == 0:\n",
    "                            wait.until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, f'a[data-pagenumber=\"{i}\"]'))\n",
    "                            )\n",
    "                            time.sleep(1)\n",
    "                            driver.find_element(By.CSS_SELECTOR, f'a[data-pagenumber=\"{i}\"]').click()\n",
    "\n",
    "                            tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content = crawling(driver)\n",
    "                            save_to_df(tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content)\n",
    "                            #df_to_file(df)\n",
    "                            \n",
    "                            wait.until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, '[id^=\"danawa-pagination-button-next-\"] > span'))\n",
    "                            )\n",
    "                            driver.find_element(By.CSS_SELECTOR, '[id^=\"danawa-pagination-button-next-\"] > span').click()\n",
    "\n",
    "                            print('클릭함')\n",
    "                    \n",
    "                            print(df.tail(5))\n",
    "                    \n",
    "                    if not driver.find_elements(By.CSS_SELECTOR, '[id^=\"danawa-pagination-button-next-\"] > span'):        \n",
    "                        for i in page_list:\n",
    "                            print(f'{i}페이지 시작합니다~~')\n",
    "                            i = int(i)\n",
    "                            if i % 10 == 1:\n",
    "                                tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content = crawling(driver)\n",
    "                                save_to_df(tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content)\n",
    "                                #df_to_file(df)\n",
    "\n",
    "                            elif i % 10 >= 2:\n",
    "                                \n",
    "                                wait.until(\n",
    "                                    EC.presence_of_element_located((By.CSS_SELECTOR, f'a[data-pagenumber=\"{i}\"]'))\n",
    "                                )\n",
    "                                time.sleep(1)\n",
    "                                driver.find_element(By.CSS_SELECTOR, f'a[data-pagenumber=\"{i}\"]').click()\n",
    "                                tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content = crawling(driver)\n",
    "                                save_to_df(tmp_scoring, tmp_market, tmp_purchasing_date, tmp_review_title, tmp_review_content)\n",
    "                                #df_to_file(df)\n",
    "                            print(\"마지막 페이지 이제 시작합니다.\")\n",
    "                            break           \n",
    "\n",
    "        except TimeoutException as e:\n",
    "            print(f'{e}모든 페이지가 끝났습니다.')\n",
    "    else:\n",
    "        print('target이 아니라서 크롤링하지 않습니다.')\n",
    "\n",
    "try:\n",
    "    for url in urls:\n",
    "        tmp = url\n",
    "        url = url[-1]\n",
    "        driver.get(url)\n",
    "        print(url)\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located((By.ID, 'paginationArea'))\n",
    "        )\n",
    "        \n",
    "        # 120개 보기로 바꿈\n",
    "        Select(driver.find_element(by = By.CSS_SELECTOR, value = '#DetailSearch_Wrapper > div.view_opt > div > select')).select_by_value('120')\n",
    "\n",
    "        header = {'User-Agent': 'Mozila/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko'}\n",
    "        res = requests.get(url, headers= header)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        soup2 = soup.select('#productListArea > div.main_prodlist.main_prodlist_list > ul')\n",
    "        link_list = []\n",
    "\n",
    "        print('세부 link_list를 추출합니다. 좀 오래 걸리네요(갤럭시 1~2분, 애플 3~5분)')\n",
    "        for i in range(0, len(soup2[0].select('a', class_ = 'click_log_prod_review_count'))):\n",
    "            if soup2[0].select('a', class_ = 'click_log_prod_review_count')[i]['href'] != '#' or soup2[0].select('a', class_ = 'click_log_prod_review_count')[i]['href'] != '':\n",
    "                link_list.append(soup2[0].select('a', class_ = 'click_log_prod_review_count')[i]['href'])\n",
    "\n",
    "        # 최종 리스트\n",
    "        link_list = [link for link in link_list if 'companyReviewYN=Y' in link]\n",
    "\n",
    "        print(f'''\n",
    "            ▶ {url}의 세부 link_list는 다음과 같고 하나씩 추출합니다.\n",
    "            ▶ {len(link_list)}개를 추출합니다.\n",
    "            ▶ {link_list}\n",
    "            ''')\n",
    "        \n",
    "\n",
    "        for idx, link in enumerate(link_list):\n",
    "            for i in range(0, 2):\n",
    "                df = pd.DataFrame(columns = ['scoring', 'market', 'purchasing_date', 'review_title', 'review_content'])\n",
    "                item, isTarget = click_link(link, i)\n",
    "                repit_page(isTarget)\n",
    "                \n",
    "                if isTarget == 1:\n",
    "                    df['item'] = item\n",
    "                    df.to_parquet(f'danawa_review_{tmp[0]}+{idx}.parquet', index = False)\n",
    "                    #df.to_csv(f'danawa_review_{tmp[0]}+{idx}.csv', encoding='utf-8 sig', mode = 'w', index = False, header=True)\n",
    "\n",
    "                else:\n",
    "                    print('target item이 아니라, parquet 저장도 하지 않습니다.')\n",
    "    print('★★★★★★★★★★★★★★★★★')\n",
    "    print('★★★추출 끝!★★★★★★★★')\n",
    "    print('★★★★★★★★★★★★★★★★★')\n",
    "    driver.close()\n",
    "\n",
    "except ElementClickInterceptedException as e:\n",
    "    print('음 ElementClickInterceptedException 에러 발생했네요 크롬창을 건들이지 마세요')\n",
    "    for url in urls:\n",
    "        tmp = url\n",
    "        url = url[-1]\n",
    "        driver.get(url)\n",
    "\n",
    "        wait.until(\n",
    "            EC.presence_of_element_located((By.ID, 'paginationArea'))\n",
    "        )\n",
    "        \n",
    "        # 120개 보기로 바꿈\n",
    "        Select(driver.find_element(by = By.CSS_SELECTOR, value = '#DetailSearch_Wrapper > div.view_opt > div > select')).select_by_value('120')\n",
    "\n",
    "        header = {'User-Agent': 'Mozila/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko'}\n",
    "        res = requests.get(url, headers= header)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        soup2 = soup.select('#productListArea > div.main_prodlist.main_prodlist_list > ul')\n",
    "        link_list = []\n",
    "\n",
    "        print('세부 link_list를 추출합니다. 좀 오래 걸리네요(갤럭시 1~2분, 애플 3~5분)')\n",
    "        for i in range(0, len(soup2[0].select('a', class_ = 'click_log_prod_review_count'))):\n",
    "            if soup2[0].select('a', class_ = 'click_log_prod_review_count')[i]['href'] != '#' or soup2[0].select('a', class_ = 'click_log_prod_review_count')[i]['href'] != '':\n",
    "                link_list.append(soup2[0].select('a', class_ = 'click_log_prod_review_count')[i]['href'])\n",
    "\n",
    "        # 최종 리스트\n",
    "        link_list = [link for link in link_list if 'companyReviewYN=Y' in link]\n",
    "\n",
    "        print(f'''\n",
    "            ▶ {url}의 세부 link_list는 다음과 같고 하나씩 추출합니다.\n",
    "            ▶ {len(link_list)}개를 추출합니다.\n",
    "            ▶ {link_list}\n",
    "            ''')\n",
    "        \n",
    "\n",
    "        for idx, link in enumerate(link_list):\n",
    "            for i in range(0, 2):\n",
    "                df = pd.DataFrame(columns = ['scoring', 'market', 'purchasing_date', 'review_title', 'review_content'])\n",
    "                item, isTarget = click_link(link, i)\n",
    "                repit_page(isTarget)\n",
    "                \n",
    "                if isTarget == 1:\n",
    "                    df['item'] = item\n",
    "                    df.to_parquet(f'danawa_review_{tmp[0]}+{idx}.parquet', index = False)\n",
    "                    #df.to_csv(f'danawa_review_{tmp[0]}+{idx}.csv', encoding='utf-8 sig', mode = 'w', index = False, header=True)\n",
    "\n",
    "                else:\n",
    "                    print('target item이 아니라, parquet 저장도 하지 않습니다.')\n",
    "    print('★★★★★★★★★★★★★★★★★')\n",
    "    print('★★★추출 끝!★★★★★★★★')\n",
    "    print('★★★★★★★★★★★★★★★★★')\n",
    "    driver.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "crawling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
