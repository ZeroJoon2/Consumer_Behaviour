#!/usr/bin/env python
# coding: utf-8

# # 1. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬

import os
import pandas as pd
pd.set_option('display.max_colwidth', None)
import matplotlib.pyplot as plt
import re

from pyspark.sql.functions import col, to_timestamp
import pymysql
import mysql.connector
import pandas as pd
from kiwi import kiwi
from kiwipiepy import Kiwi
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
from konlpy.tag import Okt
from collections import Counter
from wordcloud import WordCloud
from konlpy.tag import Kkma
from umap.umap_ import UMAP
import hdbscan
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from pyspark.sql.functions import col, to_timestamp
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, pipeline
import torch
import emoji
from soynlp.normalizer import repeat_normalize

# ë‚´ ë…¸íŠ¸ë¶ì— ì„¤ì¹˜ë˜ì–´ìˆëŠ” ê¸€ê¼´ list í™•ì¸
import matplotlib.font_manager as fm
font_list = fm.findSystemFonts(fontpaths = None, fontext = 'ttf')
font_list [:]

# font ê²½ë¡œ ì„¤ì •
font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'

# ê³„ì†ë˜ëŠ” ê¸€ê¼´ ê¹¨ì§ìœ¼ë¡œ Matplotlibì˜ í°íŠ¸ ì„¤ì •ì„ ê°•ì œë¡œ ì ìš© 
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rc('font', family='NanumGothic')

# ë§ˆì´ë„ˆìŠ¤(-) ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False

# í°íŠ¸ ì ìš© í™•ì¸
print(fm.findSystemFonts(fontpaths=None, fontext='ttf'))
print(plt.rcParams['font.family'])

# # 2. í™˜ê²½ë³€ìˆ˜ ì„¤ì •
os.environ['PYSPARK_PYTHON'] = '/home/ubuntu/anaconda3/envs/Project/bin/python'
os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/ubuntu/anaconda3/envs/Project/bin/python'
os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /usr/local/lib/mysql-connector-java-5.1.49-bin.jar pyspark-shell'


# # 3. MySQLì—ì„œ í…Œì´ë¸” ë¡œë“œ
host_ip = "15.168.221.131"  
DATABASE = "SNS_DB"
user_id = "lab13"
user_password = "lab13"
mysql_url = f"jdbc:mysql://{host_ip}:3306/{DATABASE}"

connection = mysql.connector.connect(
    host='localhost',
    user='lab13',
    password='lab13',
    database='SNS_DB',
    charset='utf8mb4'
)

cursor = connection.cursor()
cursor.execute("SHOW TABLES;")  
for table in cursor.fetchall():
    print(table[0])

table_name = "tbCrawled_Danawa"
cursor.execute(f"SELECT * FROM {table_name} LIMIT 10;") 
rows = cursor.fetchall()
for row in rows:
    print(row)

# # 4. íŒë‹¤ìŠ¤ dfë¡œ ë³€í™˜

# SQL ì‹¤í–‰ í›„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸°
cursor.execute(f"SELECT * FROM {table_name};")
data = cursor.fetchall()

# ë°ì´í„° ì»¬ëŸ¼ëª… ìë™ ì„¤ì • (ì´ì „ ì½”ë“œì—ì„œ ì˜¤ë¥˜ ì›ì¸ í•´ê²°)
columns = [desc[0] for desc in cursor.description]  # SQLì—ì„œ ì‹¤ì œ ì»¬ëŸ¼ëª… ê°€ì ¸ì˜¤ê¸°

# pandas DataFrame ë³€í™˜
df_danawa = pd.DataFrame(data, columns=columns)  # ğŸ”¥ ì»¬ëŸ¼ ê°œìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ë§ì¶¤

# ë°ì´í„° í™•ì¸
df_danawa.head()

# # 5. ë°ì´í„° ì „ì²˜ë¦¬

# ## 5-3 í…ìŠ¤íŠ¸ ì •ì œ 

# ë‚˜ì¤‘ì— ë¬¸ì¥ì„ ìª¼ê°œê¸° í¸ë¦¬í•˜ê²Œ ì˜¨ì , ë¬¼ìŒí‘œ, ëŠë‚Œí‘œì˜ íŠ¹ìˆ˜ë¬¸ìëŠ” ìœ ì§€
import re
import pandas as pd

# í•œêµ­ì–´, ì˜ì–´, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ì •ì œí•˜ëŠ” í•¨ìˆ˜
def clean_text(text):
    if isinstance(text, str):  # ë¬¸ìì—´ì¸ì§€ í™•ì¸
        text = text.lower()  # ì˜ì–´ ì†Œë¬¸ìë¡œ ë³€í™˜
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s.,!?]', '', text) # ì˜¨ì , ì½¤ë§ˆ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ìœ ì§€
        text = re.sub(r'\s+', ' ', text).strip()  # ì—°ì†ëœ ê³µë°± ì œê±°
        return text
    return ""

# ë°ì´í„°í”„ë ˆì„ì˜ ë¦¬ë·° ì»¬ëŸ¼ ì •ì œ
df_danawa['clean_review'] = df_danawa['review_content'].apply(clean_text)

# ì •ì œëœ ë°ì´í„° í™•ì¸ (ì›ë³¸ vs ì •ì œëœ ë¦¬ë·° ë¹„êµ)
df_danawa[['review_content', 'clean_review']].head(3)

# ### 5-3-2 ë¶ˆìš©ì–´ ì œê±°

import pandas as pd
import re

# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
#stopwords = {'ì˜', 'ì¢‹ì•„ìš”', 'ë„ˆë¬´', 'ë¹ ë¥´ê³ ', 'ë°°ì†¡', 'ì •ë§', 'ë§¤ìš°'}
stopwords = {'ì˜', 'ë„ˆë¬´', 'ì •ë§', 'ë§¤ìš°'}

# ë¶ˆìš©ì–´ ì œê±° í•¨ìˆ˜
def remove_stopwords(text):
    words = text.split()
    return ' '.join([word for word in words if word not in stopwords])

# ë¶ˆìš©ì–´ ì œê±° ì ìš©
df_danawa['clean_review'] = df_danawa['review_content'].astype(str).apply(remove_stopwords)

# ì›ë³¸ë¦¬ë·° vs ë¶ˆìš©ì–´ ì œê±°ëœ ë¦¬ë·° ë¹„êµ
df_danawa[['review_content', 'clean_review']].head(3)

# ### 5-3-3 ì›Œë“œí´ë¼ìš°ë“œ í™•ì¸: ë°ì´í„°ì „ì²˜ë¦¬(ì •ì œ), ë¶ˆìš©ì–´ ì œê±° í›„

import matplotlib.pyplot as plt
from wordcloud import WordCloud
import pandas as pd
import re

df_danawa[['clean_review']].head(3)

## all_word ê¼­ ì‹¤í–‰!!-> ë°‘ì— ì›Œë“œ í´ë¼ìš°ë“œ ì‹¤í–‰ ì‹œí‚¬ ìˆ˜ ìˆìŒ.
# 'clean_review' ì»¬ëŸ¼ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
all_words = ' '.join(df_danawa['clean_review'])

# ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
wordcloud = WordCloud(
    font_path="/Library/Fonts/NanumBarunGothicBold.ttf", 
    width=800,
    height=400,
    background_color='white'
).generate(all_words)

# ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # ì¶• ì œê±°
plt.title("ë‹¤ë‚˜ì™€ ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ (ë¶ˆìš©ì–´ ì œê±° í›„)", fontsize=15)
plt.show()

# Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ê°ì²´ ìƒì„± 
okt = Okt()

# ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜
def extract_nouns(text):
    return okt.nouns(text)  # ëª…ì‚¬ë§Œ ì¶”ì¶œ

# 'clean_review' ì»¬ëŸ¼ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì—¬ ìƒˆë¡œìš´ 'nouns' ì»¬ëŸ¼ ìƒì„±
df_danawa['nouns'] = df_danawa['clean_review'].apply(extract_nouns)

# 'nouns' ì»¬ëŸ¼ í™•ì¸
df_danawa[['clean_review', 'nouns']].head(3)

# # 6. ë°ì´í„° ë¶„ì„

# clean_reviewì—ì„œ ëª…ì‚¬ ì¶”ì¶œ -> ì—¬ê¸°ì„œ ì¶”ì¶œëœ ëª…ì‚¬ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê°ì„±ë¶„ì„ ì§„í–‰ ì˜ˆì • (clean_review ëª…ì‚¬ì¶”ì¶œ: 5-4-4ì°¸ê³ ,ëª…ì‚¬ì¶”ì¶œ í›„ ì‹¤í–‰í•˜ê¸° -> extract_nouns )
df_danawa['nouns'] = df_danawa['clean_review'].apply(extract_nouns)

# ê°ì„± ë¶„ì„ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ ì„ ì–¸ # ê°ì„± ë¶„ì„ í•¨ìˆ˜ ì •ì˜ -> ì´ê±¸ ì‹¤í–‰í•´ì„œ (sentiment_analysis)ê°€ ì‹¤í–‰ë¨.
positive_words = ["ì¢‹ì•„ìš”", "ë§Œì¡±", "ìµœê³ ", "í›Œë¥­", "ì¶”ì²œ", "ì™„ë²½"]
negative_words = ["ë³„ë¡œ", "ì‹¤ë§", "ë¶€ì¡±", "ìµœì•…", "ë¬¸ì œ", "ë¶ˆë§Œ"]

# ê°ì„± ë¶„ì„ í•¨ìˆ˜ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
def sentiment_analysis(nouns):
    pos_count = sum(1 for word in nouns if word in positive_words)
    neg_count = sum(1 for word in nouns if word in negative_words)

    if pos_count > neg_count:
        return "positive"
    elif pos_count < neg_count:
        return "negative"
    else:
        return "neutral"

# ê°ì„± ë¶„ì„ ì ìš©
df_danawa['sentiment'] = df_danawa['nouns'].apply(sentiment_analysis)

# í™•ì¸
df_danawa[['nouns', 'sentiment']].head(5)

# ê°ì„± ë¶„ì„ ì ìš© (clean_review ê¸°ë°˜) 
# 5-4-3ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œëœ 'nouns'ì‹¤í–‰í•˜ê¸° 
#df_danawa['sentiment'] = df_danawa['nouns'].apply(sentiment_analysis)

#nounsê°€ float í˜•ì‹ì´ë¼ astype(str)ì„ ì ìš©í•´ì„œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€ê²½í•´ì•¼í•¨. -> ê·¸ë˜ì„œ...ë°‘ì—ì„œ í•­ìƒ float ì˜¤ë¥˜ê°€..
df_danawa['sentiment'] = df_danawa['nouns'].astype(str).apply(sentiment_analysis)

# clean_review & item ì»¬ëŸ¼ ê°€ì ¸ì˜¤ê¸° 
df_selected = df_danawa[['clean_review', 'item']]

# ê²°ê³¼ í™•ì¸
df_selected.head(3)  

# #### **(1) ì•„ì´í°16 (ì•„ì´í°)**

# 'item'ì´ 'ì•„ì´í°16'ì¸ ë°ì´í„°ì—ì„œ 'clean_review'ë§Œ ì„ íƒ
iphone16_reviews = df_danawa[df_danawa['item'] == 'ì•„ì´í°16'][['clean_review']]

# ê²°ê³¼ í™•ì¸
iphone16_reviews.head(3)

# ë¶ˆìš©ì–´ ëª©ë¡ ì¶”ê°€ (ì œí’ˆê³¼ ê´€ë ¨ ì—†ëŠ” ë‹¨ì–´ë“¤)
stopwords = [
    "ë°°ì†¡", "í¬ì¥", "íƒë°°", "ì°¨ë¡œ", "ë„ì°©", "í˜œíƒ", "ì´ë²¤íŠ¸", "ê¸°ê¸°", "í°", "ê·¸ëƒ¥", "ì‡¼í•‘ëª°", 
    "ë§¤ìš°", "ìì£¼", "ì„ íƒ", "ì•„ì‰¬ì›€", "ë°”ë¡œ", "ì•„ë“¤","ë‹¤ë¥¸", "í•˜ì´ë§ˆíŠ¸", "ë°•ì‚´", "ì´ì", "ê²ƒ",
    "ì£¼ë¬¸", "ë²ˆê°€", "ì¼ë‹¨", "í™”ë©´", "ì•±", "ì‚¬ìš©", "í”„ë¡œê·¸ë¨", "ê¸°ëŠ¥", "ì„¸íŠ¸", "ì…€", "ë°ì €íŠ¸", 
    "ì˜ìƒ", "ì§„ì§œ", "í•˜ì´ë§ˆíŠ¸", "ì˜ˆì•½", "ì‚¬ì „ì˜ˆì•½", "ë¶€ëª¨ë‹˜", "í• ì¸", "ê°œí†µ", "ê³ ê°", "ìƒë‹´", 
    "ì„ ë¬¼", "ì†Œë¹„ì", "ìƒì¼", "ìƒíƒœ", "ì¼ì •", "ë°˜í’ˆ", "ì˜ˆì •", "ì•„ì´í°", "ê°¤ëŸ­ì‹œ", "êµ¬ë§¤",
    "êµ¬ë§¤ì", "ì‚¬ì „ì˜ˆì•½", "ë°°ì†¡ì¼", "ì •ì±…","ë¡¯ë°", 
]

# ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜ (ë¶ˆìš©ì–´ ì œê±°)
def extract_nouns_without_stopwords(text):
    nouns = okt.nouns(text)
    return [word for word in nouns if word not in stopwords]  # ë¶ˆìš©ì–´ ì œê±°

# itemì´ 'ì•„ì´í°16'ì¸ clean_reviewì—ì„œ ëª…ì‚¬ ì¶”ì¶œ
df_danawa['nouns'] = df_danawa[df_danawa['item'] == 'ì•„ì´í°16']['clean_review'].apply(extract_nouns_without_stopwords)

# 'nouns' ì—´ í™•ì¸
df_danawa['nouns'].head(3)

# 'nouns' ì—´ì—ì„œ NaN ê°’ì´ë‚˜ ìˆ«ì ê°’ ì œê±°
df_danawa['nouns'] = df_danawa['nouns'].apply(lambda x: [] if isinstance(x, float) or isinstance(x, int) else x)

# ê·¸ í›„ sentiment_analysis ì ìš©
df_danawa['sentiment'] = df_danawa['nouns'].apply(sentiment_analysis)

# ê°ì„± ë¶„ì„ í•¨ìˆ˜ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
def sentiment_analysis(nouns):
    pos_count = sum(1 for word in nouns if word in positive_words)
    neg_count = sum(1 for word in nouns if word in negative_words)
    
    if pos_count > neg_count:
        return "positive"
    elif pos_count < neg_count:
        return "negative"
    else:
        return "neutral"

# ê°ì„± ë¶„ì„ ì ìš©
df_danawa['sentiment'] = df_danawa['nouns'].apply(sentiment_analysis)

# ê¸ì •ì , ë¶€ì •ì  ë¦¬ë·° ë¶„ë¥˜
positive_reviews = df_danawa[df_danawa['sentiment'] == 'positive']['nouns']
negative_reviews = df_danawa[df_danawa['sentiment'] == 'negative']['nouns']

# ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±
positive_nouns = sum(positive_reviews.tolist(), [])
negative_nouns = sum(negative_reviews.tolist(), [])

# ê° ë¦¬ë·°ì˜ ê¸ì •, ë¶€ì • ë‹¨ì–´ ì¹´ìš´íŠ¸
positive_counts = Counter(positive_nouns)
negative_counts = Counter(negative_nouns)


# - 30ê°œ 

# ì œì™¸í•  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (# ì›Œë“œí´ë¼ìš°ë“œì—ì„œ ì œì™¸í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë“¤)
exclude_words = {
    "ë¬¸ì œ", "êµ¬ë§¤", "ìµœê³ ", "ë§Œì¡±", "ì‹¤ë§", "ë³„ë¡œ","êµ¬ì…","í¬ì¸íŠ¸", "ì •ë§","ì¶”ì²œ","ìƒê°", 
    "ìƒí’ˆ","ì¬ê³ ","ë°•ìŠ¤","í”„ë¡œ","ì—­ì‹œ", "ì²˜ìŒ", "ì‹¤ë¬¼", "ì €", "í•˜ë£¨", "ìŠˆíŒ…", "ë§ˆìŒ", "ì¢€", 
    "ì‚¬ëŒ", "ì´ê±´", "ì•„ì£¼", "ì—­ì‹œ", "ì™„ì „", "ëˆˆ", "ë”", "ì°¨", "ì²˜ìŒ", "ì €", "ê³ ë¯¼", "ì¢€", "ì•„ì£¼",
    "ë°˜", "ì œ", "í† ìš”ì¼", "ë‹¤ë§Œ", "ë‹¹ì¼", "ì£¼ë³€", "íŒŒì†", "ê±±ì •", "ì™œ", "ì¼ì°", "í–‡", "ì´", "ì œí’ˆ", "ë‹¤ìŒ",
     "í•‘ê¾¸í•‘ê¾¸", "ì–‘í’ˆ", "ìê¸‰","í•˜í•˜","ê°œë„", "ì•Œì§", "ë¡¯ë°", "ì˜ˆì •ì¼", "ê¸°ë„", "ì¶œì‹œì¼", "ë§¥ìŠ¤",
    "í• ë¶€", "ì›€", "ì ", "ê²Œ", "ì¤„", "ë¯¸ë‹ˆ", "í›„", "ì¿ íŒ¡", "ì˜¤ì „", "ìƒ‰ìƒ", "í™”ì´íŠ¸", "ë””ìì¸",
    "ë¦¬ë·°", "ì• í”Œ", "êµì²´", "êµ¬ì˜¤", "ë“­ë‹ˆ", "ë¶€ì‹œ", "ì·¨ì†Œ", "ë‚ ", "ë•Œë¬¸", "ì‹¤ìˆ˜", "í‹°íƒ€ëŠ„", "ì»¬ëŸ¬",
    "ìƒ‰", "ë³´ê³ ", "ìˆ˜", "ë•Œ", "ë¬´", "êµ¿", "ë¡œ", "ë¶ˆëŸ‰", "ì¼ë°˜", "ì˜ˆìƒ", "ì¼€ì´ìŠ¤", "í•˜ì", "ë‚˜", 
    "íœ´ëŒ€í°", "ë¶ˆë§Œ", "ë¸”ë™", "ë¡¯ëŒ€", "ì¼", "êµí™˜", 'ì§±ì§±', 'ì¬', 'ë°' 'ì´ë²ˆ', 'ë§Œ', 'ì‚¬ì„œ', 
    'ë‚˜ë¦„', 'ë¬¼ê±´', 'ë“¯', 'ì„¤ëª…',' í”¼íŠ¸', 'ìµœì•…', 'ì™¸ê´€', 'ë©°ì¹ ', 'ì œì¼', 'í•¨', 'ë“¤ë½ê±°ë¦¬', 
    'í•œì°¸', 'ì–¸ë°•ì‹±', 'ì¸ì§€', 'ë²ˆ', 'ìê¸‰', 'ëŠë‚Œ', 'ë°', 'ë§˜', 'ìƒ‰ê°', 'ê°€ê²©', 'í”¼íŠ¸', 
    'í…ŒìŠ¤íŠ¸', 'ì „í˜€', 'íœ´ëŒ€', 'ì—¬ê¸°', 'ì†Œë¹„', 'ê·¼ë³¸', 'ë§', 'ìš©', 'ì˜¤ëŠ˜', 'ì •í•´ì§„', 'ì´ë²ˆ', 
    'ìš”', 'ìœ„í•´', 'ê¸°ë¶„', 'ì¡°ê±´', 'ì•ˆì‹¬', 'ëª¨ë¡œ', 'ê°œë´‰', 'í™•ì¸', 'ì‚¬ì´íŠ¸', 'ì‹œê°„', 'ì ì‹¬', 
    'ì¶œì‹œ', 'ì§€ê¸‰', 'ì¶œê³ ', 'ì •ìƒ', 'ì†', 'ë°œì†¡', 'í•˜í•„', 'ì•„ì¥¬ì•„ì¥¬', 'ì•„ì´', 'ë°œê²¬', 'ì˜',
    'ì—­ëŒ€', 'ì´ìƒ', 'êµ¬í˜•', 'ë‚ ì§œ', 'ì¢…ì´', 'ê±´', 'ëŒ€ì²´ë¡œ', 'ì ê·¹', 'í’ˆì ˆ', 'ë¬´ì¡°ê±´', 'ë³´ê¸°',
    'ì—¬ë¦„', 'ì£¼ë¨¸ë‹ˆ', 'ìƒ‰ë„', 'í™”ì†Œ', 'ë„£ê¸°', 'ì‹œì „', 'ë¬´ë µ', 'í†µí•´', 'í™•', 'ë°˜ì‘', 'ê¸ˆ', 'ê²½í—˜', 
    'ì•½í’ˆ', 'ì´ˆê¸°', 'í•˜ë‚˜', 'ì ¤', 'ì•„ì´ë³´ë¦¬', 'ê±°ë‚˜', 'ê³ ë² ', 'ê¸°ì¡´', 'ì¸ì¹˜' 'ì˜†', 'ë°', 'í‚¤', 
    'ì§•', 'ì•', 'ì£¼ìœ„', 'ê²€ì€ìƒ‰', 'ì•ˆ', 'ì¡°ê¸ˆ', 'ì™„ë£Œ', 'ë¨¼ì§€', 'ì¼ì´', 'ê³„ì—´','ì‚¬ë¼', 'ì ë¦½', 
    'í•‘í¬', 'ì¸ì¹˜', 'ì‹­ì‚¼', 'íŒ¨í‚¹', 'ì¹´ë“œ', 'ê°“', 'ìŠ¤í™', 'ì˜†', 'ìƒ‰ê¹”', 'í¸ì˜', 'í•­ìƒ', 'ê°€ì§€', 
    'ì£¼ì–´', 'ë“±ë“±', 'ê¸€ë¼ìŠ¤', 'ê³„ì†', 'ì„¼í„°', 'ì“°ë µë‹ˆ', 'ì—ì–´ìº¡', 'ì§€ê¸ˆ', 'ì™“ìŠµë‹ˆë‹¿', 'ì‚ë¥´', 'ë­', 'ì°¨ì´ì ',
    'ë¼ìš´ë“œ', 'ë„¤ì¶”ëŸ´', 'í…Œë‘ë¦¬', 'ì¬ì§ˆ', 'ë‹¬ë¼', 'ë‚´ë…„', 'ìŠ¤íƒ€ì¼', 'ë¹„êµ', 'ì§ì ‘', 'ì¿ í°', 'ìµœëŒ€', 'í†µì‹ ì‚¬',
    'ë°”ë¡±', 'ìš”ìƒˆ', 'ì‚¿ê±°ë“ ', 'ë°ì ¸íŠ¸', 'ì—¬', 'ì•ˆí•´', 'ëŒ€ì²´', 'ìº¬', 'ë“±ë¡', 'ì‚¬ì‹¤', 'ì´í‹€', 'í™˜ë¶ˆ',
    'ë°”ê¿¨ëŠ˜í…œë°', 'ë°©ì§€', 'ë¼', 'ì–´ì œ','ì¿ ', 'ììœ„', 'ì˜¤í›„','ì¿ í†µ', 'ë„', 'íˆ¬í°', 'ë©”ì¸', 'ë«', 'ì‹ ì‚¬', 
    'ë§Œì „', 'í”Œë¦½', 'ì‹ ì˜í•œìˆ˜', 'ìš°ì„ ', 'ì™„ì£¤', 'ë“œ', 'ì§ˆì†Œ', 'ê¸°ì¼', 'ëª¨ë‘', 'ì„¸ëŒ€', 'ë°”ê¿¨ëŠ˜í…œë°', 
    'ê·¸', 'ë¶€ê°€', 'ëíŒ', 'ì •ë„', 'ì´í›„', 'ê±°', 'ì‹œ', 'ì‚´ì§', 'ê·¼ì ‘', 'í˜¹', 'ë‚´ë¶€', 'ìŠ¤íŠ¸ë ˆìŠ¤',
    'ë³´ìƒ', 'ë§Œí¼', 'ë²„ê¿¨ëŠ˜í…œë°', 'ëŠ¥ë°', 'íƒ€','ê¸°í•œ', 'ê´œ', 'ë³„', 'ê¸°ê°„', 'ì„¤ì •', 'ë”ë¼ë„', 'ëŒ€ìš©', 'ëŠ˜',
    'ê°•ë ¥', 'í•˜ë‹ˆ', 'íŠ¸', 'ì¸ê¸°', 'êµ³ì´', 'ëŒ€ë¹„', 'ê¸°ìŠ¤', 'ì˜¨ë¼ì¸', 'ì¡°ì¼ì´', 'íƒ€ì…', 'í™”ìš”ì¼', 
    'ì €ë…', 'ì›”ìš”ì¼', 'ê°‘ìê¸°', 'ì ë¦½ê¸ˆ', 'ìš”ì›', 'ì˜¬í•´', 'ì§±', 'ë°˜ë‚˜ì ˆ', 'ì´ìœ ', 'ê°œì›”', 'íšŒì‚¬',
    'ì—°ì¥', 'ê°€ëŠ¥', 'ì•„ë¬´', 'ìš”ì¦˜', 'ì—‰ë§', 'ê±´ê°€', 'ë¶€ì¡±', 'êµí†µì¹´ë“œ', 'ë³¼', 'ì°¨ë¼', 'ë¬´ê´‘', 
    'ìµœê³ ë´‰', 'ì–¼ë¥¸', 'íƒ‘ì œ', 'ë¶ˆë¡œ', 'ì¼ì‹œ', 'ì—¬ê¸°ì €ê¸°', 'ì–´ì´', 'í™•ì •', 'ìŠ¤', 'ë™ìƒ', 'ì§ˆ', 
    'ì—…ê¸€', 'ìƒì˜', 'ì œì¡°', 'íŒë§¤', 'ìƒ', 'ëŒ€ì²˜', 'ë§Œìš”', 'ì„±ë¹„', 'ë•Œê¹”', 'ì•½ê°„', 'ê·¸ëŒ€ë¡œ', 
    'ì¿ ìœ¼', 'ì£¼', 'ì¿ ì€', 'ì²˜ëŸ¼', 'ë‚˜ì€', 'ê³µì‹', 'ì–´ë³´', 'ë³¸ì¸', 'ì˜ì ‘', 'ìíŠ¸', 'í”„ë§¥', 'ëª¨í•´', 'ë³´ë“œ',
    'ì•ˆë‚˜', 'ë˜', 'ì°¸ê³ ', 'ì˜¤íƒ€', 'ìˆ˜ì¤€', 'ì‹œë„','ë“±', 

}  

# ìƒìœ„ 30ê°œ ë‹¨ì–´ë§Œ ë‚¨ê¸°ê¸°
top_n = 30

# ê¸ì •ì  ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± (íŠ¹ì • ë‹¨ì–´ ì œì™¸ í›„ ìƒìœ„ 30ê°œë§Œ ì„ íƒ)
positive_filtered_counts = {word: count for word, count in positive_counts.items() if word not in exclude_words}
positive_top30 = dict(Counter(positive_filtered_counts).most_common(top_n))  # ìƒìœ„ 30ê°œ ë‹¨ì–´ ì„ íƒ
wordcloud_positive = WordCloud(
    font_path="/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
    background_color="white",
    colormap="Blues",
    width=800,
    height=400
).generate_from_frequencies(positive_top30)

# ë¶€ì •ì  ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± (íŠ¹ì • ë‹¨ì–´ ì œì™¸ í›„ ìƒìœ„ 30ê°œë§Œ ì„ íƒ)
negative_filtered_counts = {word: count for word, count in negative_counts.items() if word not in exclude_words}
negative_top30 = dict(Counter(negative_filtered_counts).most_common(top_n))  # ìƒìœ„ 30ê°œ ë‹¨ì–´ ì„ íƒ
wordcloud_negative = WordCloud(
    font_path="/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
    background_color="white",
    colormap="Reds",
    width=800,
    height=400
).generate_from_frequencies(negative_top30)

# ì‹œê°í™” (ê¸ì •ì  ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ)
plt.figure(figsize=(4, 2))
plt.imshow(wordcloud_positive, interpolation="bilinear")
plt.axis("off")
plt.title("ì•„ì´í° ê¸ì •ì  ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ", fontsize=14)
plt.show()

# ì‹œê°í™” (ë¶€ì •ì  ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ)
plt.figure(figsize=(4, 2))
plt.imshow(wordcloud_negative, interpolation="bilinear")
plt.axis("off")
plt.title("ì•„ì´í° ë¶€ì •ì  ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ", fontsize=14)
plt.show()


# #### **(2) s24 (ê°¤ëŸ­ì‹œ)**



# 'item'ì´ 'S24'ì¸ ë°ì´í„°ì—ì„œ 'clean_review'ë§Œ ì„ íƒ
s24_reviews = df_danawa[df_danawa['item'] == 'S24'][['clean_review']]

# ê²°ê³¼ í™•ì¸
s24_reviews.head(3)

# s24ì— ëŒ€í•œ ë¦¬ë·°ë§Œ ì„ íƒ
s24_reviews = df_danawa[df_danawa['item'] == 'S24']['clean_review']

# ê° ë¦¬ë·°ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ ë° ë¶ˆìš©ì–´ ì œê±°
s24_reviews_nouns = s24_reviews.apply(extract_nouns_without_stopwords)


# ê°ì„± ë¶„ì„ ì ìš©
s24_reviews_sentiment = s24_reviews_nouns.apply(sentiment_analysis)

# ê¸ì •/ë¶€ì • ë¦¬ë·° ë¶„ë¦¬
positive_reviews_s24 = s24_reviews[s24_reviews_sentiment == "positive"]
negative_reviews_s24 = s24_reviews[s24_reviews_sentiment == "negative"]

# ê¸ì •, ë¶€ì • í‚¤ì›Œë“œ ì¶”ì¶œ
positive_nouns_s24 = sum(positive_reviews_s24.apply(extract_nouns_without_stopwords), [])
negative_nouns_s24 = sum(negative_reviews_s24.apply(extract_nouns_without_stopwords), [])

# í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜ ê³„ì‚°
positive_counts_s24 = Counter(positive_nouns_s24)
negative_counts_s24 = Counter(negative_nouns_s24)


# - 30ê°œ 

# ì œì™¸í•  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ì›Œë“œí´ë¼ìš°ë“œì—ì„œë§Œ ì œì™¸í•  ë‹¨ì–´ë“¤)
exclude_words = {"ë¬¸ì œ", "êµ¬ë§¤", "ì‚¼ì„±", "ìˆ˜", "ì œí’ˆ", "êµ¬ì…", "ì¶”ì²œ", "ë§Œì¡±", "ìµœê³ ", "í•¸ë“œí°", "ë³„ë¡œ", "ìš¸íŠ¸ë¼",
                 "ë…¸íŠ¸", "ì´ë²ˆ", "ë§ˆìŒ", "ì„±ëŠ¥", "ì¼€ì´ìŠ¤", "ìƒ‰ìƒ", "ë”", "ê³ ë¯¼", "ìƒê°", "ìê¸‰", "ì•ˆ", "ì¢€", "í•˜ë‚˜",
                 "ì—­ì‹œ", "ëª¨ë‘", "ë‹¤ìŒ", "ì¡°ê¸ˆ", "ì§€ê¸ˆ", "ì ê·¹", "ì •ë„", "ì´", "ê·¸", "ì¤‘", "ê¸°ì¡´", "í›„", "ë§", "ì •í’ˆ", 
                 "ì™„ì „", "êµì²´", "í¬ê¸°", "ë“­ë‹ˆ", "ëª¨ë¸", "ê±±ì •", "í¬ê¸°", "ë¶€ì¡±", "ì„¼í„°", "í•„ë¦„", "ì°¸ì—¬", "ì ", "ì‚¬ì´ì¦", 
                 "ë‚ ", "í”ŒëŸ¬ìŠ¤", "ì„œë¹„ìŠ¤", "ìœ ì‹¬", "ë¥¼", "ê±°", "ì €", "ì œ", "ë•Œ", "ë§˜", "ì •ë§", "ì „ì", "ë¹„", "ë¶€ë¶„", 
                 "ì»¤ì„œ", "ë˜í•œ", "ì‹œë¦¬ì¦ˆ", "ìµœì‹ ", "ì „ì²´", "ì†", "ì‚¬ì€", "ì»¬ëŸ¬", "í’ˆ", "ì•„ì£¼", "ë©´", "íœ´ëŒ€í°", "ì†ëª©",
                 "ì°¨ì´", "ì²˜ìŒ", "ë¬´ì—‡", "ì œì¼", "ìƒí’ˆ", "ë³´ê³ ", "ê°", "í•´", "ì„¤ì •", "ê±¸", "ì œê³µ", "ë„", "ë“¯", "ê³ ë ¤", 
                 "ê³„ì†", "ì–´ë¨¸ë‹ˆ", "ì „", "ì‚¬ìš©ì", "ë‹¤ì‹œ", "í™•ì¥", "í”½ì—…", "ë“±", "ëª»", "ìƒ‰", "ë¶ˆë§Œ", "ê°€ê²©", 
                 "í”Œë¦½", "ê¸°ë³¸", "ì‚¬ì´ì¦ˆ", "ì‚¬", "ê°€ì •", "ì¿ í°", "ì¥ì ", "ì—¬ê¸°", "ì´ì œ", "ìš”", "ë§ˆë¸”", "ë¶ˆëŸ‰", "ì¼€ì´ë¸”", 
                 "ì œë¡œ", "íŒë§¤", "ë§¤ì¥", "ë¶„", "ìƒí™©", "ê°€ì§€", "ìŠ¤í† ì–´","ì•¡ì •", "ì˜¤ë Œì§€", "ì•", "ëŠ¥ì´", "í•˜ë£¨", "ê¸°ê°„", "ë¸”ë™",
                 "ê·¸ë ˆì´", "ê°€ì„±", "ë¹„", "ìš©", "ë¸”ë£¨", "êµí™˜", "í…Œë‘ë¦¬", "í…ŒìŠ¤íŠ¸", "ì•ˆë‚´", "ë¬¸ì˜", "ë‹¨", "ê¸°ë‹¤ë¦¼", "ê°•ë‚¨", "ë•Œë¬¸", 
                 "ë­", "ìˆ˜ë ¹", "ëŠë‚Œ", "ê²°ê³¼", "ì‚¬ëŒ", "ì‹¤ë§", 'ì§œê¾¸', 'ì´ì „', 'ì›€', 'ì ë„', 'í´ë“œ', 'ì˜ë¡œìš°', 'ë¡œ', 'ì•„ë“¤', 'ì˜ˆì „',
                 'ì‚´ì§', 'ë‹¹ì¼', 'ê°€ì¥', 'ì‹ ì²­', 'ë³´í˜¸', 'ê±°ì˜', 'ì²´ê°', 'ê¸°ë¶„', 'ì‹œê°„', 'ë‹¤í–‰', 'ê°œë´‰', 'ì•„ë¬´', 'ì£¼ë§', 'ê³ ì¥', 
                 'ì¶œì‹œ', 'í‰ì†Œ', 'ë˜', 'ì¸ì‹', 'ë¶€ë‹´', 'ì´í‹€', 'ë‹¬', 'ëª¨ë“ ','ê°•ë ¥', 'ëŒ€ë¹„', 'í™•ì¸', 'ì¬ê³ ', 'ë°•ìŠ¤', 'í›„ê¸°', 'ëª¨ë“ ', 
                 'ë°”ì´ì˜¬ë ›', 'ì´ˆê¸°', 'ë…¸ë€ìƒ‰', 'ì´ìš©', 'ìê¾¸', 'ì‹¤ë¬¼', 'ê°œ', 'ì„ í˜¸', 'ë‚ ì§œ', 'ì™€ì´í”„', 'í•¨', 'í™œì„±í™”', 'ì§„', 'ì‚¬ì‹¤',
                 'ì „ìš©', 'ì§€ì¸', 'ìë…€', 'ìƒˆ', 'ë³´ì¡°', 'ì„œë¡œ', 'ë‹¹í™©', 'í•œë²ˆ', 'í•´ê²°', 'ë°±ë§Œì›', 'ì‹œ', 'í•­ìƒ','ë¬´ì²™', 'ìŠµ', 'ì• í”Œ', 
                 'ëª¨ì¹œ', 'ì§ì ‘', 'ë°œì „', 'ìœ„í•´', 'ì‹ ì„¸ê³„', 'ìš°ì„ ', 'ì°¸ê³ ', 'ê²Œ', 'ê¼­', 'ë•ë¶„', 'í•˜ë‹ˆ', 'ìƒ‰ê°', 'ì‹¬', 'ì§ì›', 
                 'ì°', 'ë°', 'ë¹›', 'ì„¤ëª…', 'ê²€ì€ìƒ‰', 'ë¶€ì°©', 'ë‚´ìš©', 'ì´ìƒ','ì²˜ë¦¬', 'ì¸í„°ë„·', 'ì¶”ê°€', 'ê³ ', 'ì•„ì´', 'ê³³', 
                 'ì‹¤ì‹œê°„', 'ì¡°ê±´', 'ë”ìš±','í• ë¶€', 'ì˜¤ëŠ˜', 'ë™ì•ˆ', 'ì—°ë½', 'ì§€ì—°', 'ë‹µ', 'í‚¤', 'ì „í™”', 'ë°©ë¬¸', 'ì–¸ì œ', 'ëŒ€ë¦¬ì ',
                 'ì“°ê¸°','ìƒ‰ê¹”', 'ì¤Œ', 'ì¹œêµ¬', 'í™œìš©', 'ë³´ê¸°', 'ì „ë°˜', 'ê°œì›”', 'ì“±', 'ê°€ì…', 'ì¶œê³ ', 'ê²°ì œ', 'ì•ˆì‹¬', 'ë¬¼ëŸ‰', 
                 'ì¼ì', 'ì•Œëœ°í°', 'ì»¤ë²„', 'ê²½ë¡œ', 'ì™œ', 'ê²½ìš°','ê±´', 'ëŒ€í•´', 'ì‹¤ê°', 'ë¹„êµ', 'í¬ê²Œ', 'ë²ˆ', 'ì£¼ë³€', 
                 'í˜ì´ì§€', 'ì´ê±´', 'ì¼', 'íŒ€', 'íƒœê·¸', 'í”„ë¡œëª¨ì…˜', 'ê·¸ë•Œ', 'í¸ì…', 'ê°', 'ë‹ˆ', 'íŒŒì†', 'í•´ì™¸', 
                 'ë¬´ê´‘', 'ê³ ê¸‰', 'ì ìš©', 'ê³µì§€', 'í†¤', 'ì‚¬ì„œ', 'ëŒ€í™”', 'ì²¨ë¶€', 'ì´í›„', 'ìƒë‹´ì‚¬', 'ëˆ','ê³µí™ˆ', 'ì°¨ë³„', 
                 'ì„¸ê³„', 'í•œêµ­ì¸', 'ê³ ê°€', 'ì·¨í–¥', 'ìŠ¬ë¦¼', 'ì ‘', 'ìˆ˜ì¤€', 'ë‚¨ì§€', 'ì–´ì œ', 'ì˜¤í›„','í•œì§€', 'ì˜¨ë¼ì¸', 
                 'ì‡¼ë§ˆì  ì‹œ', 'êµ¿', 'ê¸ˆì œ', 'ê°€í°', 'í–‰ì‹œ', 'ì·¨ê¸‰', 'ì¡°ì‘', 'ê·¸ê²ƒ', 'ì˜¬í•´', 'ê°ì‚¬', 'ìì²´', 'ë°', 
                 'ë¶€ê°€', 'ê·¸ëŒ€ë¡œ', 'ë¡œì  íƒë°°', 'ê³ ì§ˆ', 'ì£¼ì˜', 'ì—­ëŒ€', 'ê¸°ëŒ€', 'ê¸°ë³€', 'ì£¼ìœ„', 'ì—°ì†', 'í¬ë¦¼', 'ì„', 'ë°°ë‹¬', 
                 'ë…„ì „', 'ê°€ë”','ì‹ ë¢°', 'ì´ìŠˆ', 'ì•½ê°„', 'ì˜¤ë‹‰ìŠ¤', 'ë™ìƒ', 'ë²„ë²…ì„', 'ë§ˆì¹¨', 'ì°¨','ìƒí™œ', 'ì‘ë™', 'ì¼ì ˆ',
                 'í™”ì´íŠ¸', 'ë', 'ê°€ê¹Œì´',
                 }

# ìƒìœ„ 30ê°œ ë‹¨ì–´ë§Œ ì„ íƒí•˜ëŠ” í•¨ìˆ˜
def get_top_n_words(word_counts, n=30):
    return dict(Counter({word: count for word, count in word_counts.items() if word not in exclude_words}).most_common(n))

# ê¸ì •ì  & ë¶€ì •ì  ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
positive_top_30_s24 = get_top_n_words(positive_counts_s24, 30)
negative_top_30_s24 = get_top_n_words(negative_counts_s24, 30)

# ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜
def generate_wordcloud(word_freq, colormap, title):
    if word_freq:
        wordcloud = WordCloud(
            font_path="/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            background_color="white",
            colormap=colormap,
            width=800,
            height=400
        ).generate_from_frequencies(word_freq)

        plt.figure(figsize=(4, 2))
        plt.imshow(wordcloud, interpolation="bilinear")
        plt.axis("off")
        plt.title(title, fontsize=14)
        plt.show()
    else:
        print(f"âš ï¸ '{title}'ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ê¸ì •ì  ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ (ìƒìœ„ 30ê°œ)
generate_wordcloud(positive_top_30_s24, "Blues", "ê°¤ëŸ­ì‹œ S24 ê¸ì •ì  ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ")

# ë¶€ì •ì  ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ (ìƒìœ„ 30ê°œ)
generate_wordcloud(negative_top_30_s24, "Reds", "ê°¤ëŸ­ì‹œ S24 ë¶€ì •ì  ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ")


# # 7. ì•„ì´í°ê³¼ ê°¤ëŸ­ì‹œ í‰ê·  ê³„ì‚°

# 'scoring' ì»¬ëŸ¼ì˜ ë°ì´í„°ë¥¼ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜í•˜ê³ , 'item'ë³„ í‰ê·  ê³„ì‚°
df_danawa['scoring'] = pd.to_numeric(df_danawa['scoring'], errors='coerce')  # ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜

# itemë³„ í‰ê·  score ê³„ì‚°
average_scores = df_danawa.groupby('item')['scoring'].mean()

# ê²°ê³¼ ì¶œë ¥
print(average_scores)

# 'scoring'ê³¼ 'item' ì»¬ëŸ¼ë§Œ ì„ íƒí•´ì„œ ë¯¸ë¦¬ë³´ê¸°
df_danawa[['scoring', 'item']].head(3)  # ì²« 3ê°œ ë°ì´í„° í™•ì¸

# 'scoring' ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•œ í›„ 'ì ' ì œê±° ë° ìˆ«ìë§Œ ì¶”ì¶œ
df_danawa['scoring'] = df_danawa['scoring'].astype(str)  # ë¨¼ì € ë¬¸ìì—´ë¡œ ë³€í™˜
df_danawa['scoring'] = df_danawa['scoring'].str.replace('ì ', '', regex=False)  # 'ì ' ì œê±°
df_danawa['scoring'] = pd.to_numeric(df_danawa['scoring'], errors='coerce')  # ìˆ«ìë¡œ ë³€í™˜, ë³€í™˜ ë¶ˆê°€ì‹œ NaN ì²˜ë¦¬

# 'scoring'ê³¼ 'item' ì»¬ëŸ¼ë§Œ ì¶œë ¥í•˜ì—¬ í™•ì¸
print(df_danawa[['scoring', 'item']].head(3))

# 'scoring'ê³¼ 'item_id' ì»¬ëŸ¼ì„ ì„ íƒí•˜ê³ , NaN ê°’ì„ ê°€ì§„ í–‰ì„ ì œê±°
df_selected = df_danawa[['scoring', 'item']].dropna(subset=['scoring'])

# ê²°ê³¼ í™•ì¸
print(df_selected.head(3))  # ìƒìœ„ 10ê°œ ì¶œë ¥

# 'scoring' ì»¬ëŸ¼ì—ì„œ 'ì 'ì„ ì œê±°í•˜ê³ , ìˆ«ìë§Œ ì¶”ì¶œ í›„ í‰ê· ê³¼ ìµœëŒ€ê°’ ê³„ì‚°
df_danawa['scoring'] = df_danawa['scoring'].str.replace('ì ', '', regex=False)  # 'ì ' ì œê±°
df_danawa['scoring'] = pd.to_numeric(df_danawa['scoring'], errors='coerce')  # ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜

# ìµœëŒ€ ì ìˆ˜ì™€ í‰ê·  ì ìˆ˜ ê³„ì‚°
max_score = df_danawa['scoring'].max()
mean_score = df_danawa['scoring'].mean()

# ê²°ê³¼ ì¶œë ¥
print(f"ìµœëŒ€ ì ìˆ˜: {max_score}")
print(f"í‰ê·  ì ìˆ˜: {mean_score:.2f}")


# 'item'ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚° (S24ì™€ ì•„ì´í°)
average_score_by_item = df_danawa.groupby('item')['scoring'].mean()

# ê²°ê³¼ ì¶œë ¥
print(average_score_by_item)


# 1. ìµœëŒ€ì ìˆ˜(Max Score): 100ì (scoring columns max)   
# 2. í‰ê· ì ìˆ˜(Mean Score): ì „ì²´ í‰ê· ì ìˆ˜ 97.32(scoring cloumns all number mean)
#     - ì „ì²´ì ìœ¼ë¡œ ë†’ì€ ì ìˆ˜ê°€ ë¶€ì—¬ë˜ì–´ì ¸ ìˆìŒ.  
# 3. itemë³„ í‰ê·  ì ìˆ˜
#    - s249(ì‚¼ì„±): 96.86ì   
#    - ì•„ì´í°15(ì•„ì´í°): 98.18ì   
#    -> ì•„ì´í°16ì´ s24ë³´ë‹¤ í‰ê· ì´ ì•½ê°„ ë†’ë‹¤ëŠ”ê±¸ ì•Œ ìˆ˜ ìˆìŒ.  

